{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Keras\n",
    "\n",
    "In this module we will introduce [Keras](https://keras.io/), a high level API for Neural Networks.\n",
    "\n",
    "\n",
    "To be specific\n",
    "- we will mostly restrict ourselves to the Keras Sequential model\n",
    "- this will greatly simplify your learning and coding\n",
    "- it will restrict the type of Deep Learning programs that you can write\n",
    "    - but not a meaningful restriction for the simple programs that you will write in this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note**: \n",
    "\n",
    "The code snippets in this notebook are *fragments* of a larger [notebook](DNN_Keras_example.ipynb)\n",
    "- are illustrative: will not actually execute in this notebook but will in the complete notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Keras Sequential Model\n",
    "\n",
    "**Reference**: [Getting started with the Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/)\n",
    "\n",
    "Keras has two programming models\n",
    "- Sequential\n",
    "- Functional\n",
    "\n",
    "We will start with the Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Sequential model allows you to build Neural Networks (NN) that are composed of a *sequence* of layers\n",
    "- just like our cartoon\n",
    "- a very prevalent paradigm\n",
    "\n",
    "This will likely be sufficient in your initial studies\n",
    "- but it restricts the architecture of the Neural Networks that  you can build\n",
    "- use the Functional API for full generality\n",
    "    - but it might appear more complicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea is quite simple.\n",
    "\n",
    "Keras Sequential implements an `sklearn`-like API\n",
    "- define a model\n",
    "- fit the model\n",
    "- predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining a model\n",
    "\n",
    "Let's jump into some code.\n",
    "\n",
    "We start with some preliminaries \n",
    "- imports\n",
    "- determining size of an example\n",
    "\n",
    "        import keras\n",
    "\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Input, Dense\n",
    "\n",
    "        input_size = X[0].shape\n",
    "        output_size = np.unique(y).shape[0]\n",
    " \n",
    "Next: some old friends, in new clothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    \n",
    "    # Regression\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_size,)),\n",
    "        Dense(1, activation=None)\n",
    "        )    \n",
    "\n",
    "    model.compile(loss='mse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A model uses the `Sequential` architecture\n",
    "- A sequence (implemented as an array) of layers\n",
    "    - `Input` layer\n",
    "        - defines the shape of a single example\n",
    "    - `Dense` (Fully connected) layer\n",
    "        - with $1$ output\n",
    "        - No activation\n",
    "        - Implements Regression\n",
    "- Loss is `mse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    # Binary Classification\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_size,)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "        )\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A model uses the `Sequential` architecture\n",
    "- A sequence (implemented as an array) of layers\n",
    "    - `Input` layer\n",
    "    - `Dense` (Fully connected) layer\n",
    "        - with $1$ output: binary classification\n",
    "        - sigmoid activation\n",
    "        - Implements Classification\n",
    "- Loss is `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Both examples are single non-Input layer\n",
    "            <ul>\n",
    "                <li>Dense, with 1 unit (\"neuron\")</li>\n",
    "            </ul>\n",
    "        <li>Regression example\n",
    "             <ul>\n",
    "                <li>No activation</li>\n",
    "                <li>MSE loss</li>\n",
    "                 </ul>\n",
    "        <li>Binary classification example\n",
    "         <ul>\n",
    "            <li>Sigmoid activation</li>\n",
    "            <li>Binary cross entropy loss</li>\n",
    "             </ul>\n",
    "    </ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hopefully you get the idea.\n",
    "\n",
    "Let's explore a slightly more complicated model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    # Multinomial Classification\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_size,)),\n",
    "        Dense(n_hidden_1, activation='relu', name=\"hidden_1\"),\n",
    "        Dense(n_hidden_2, activation='relu', name=\"hidden_2\"),\n",
    "        Dense(output_size, activation='softmax', name=\"outputs\")\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy')\n",
    "                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A model uses the `Sequential` architecture\n",
    "- A sequence (implemented as an array) of layers\n",
    "    - `Input` layer\n",
    "    - 2 `Dense` layers \n",
    "        - with varying number of outputs: `n_hidden_1`,`n_hidden_2`\n",
    "        - `relu` activation\n",
    "    - A `Dense` layer implementing Multinomial Classification\n",
    "        - number of outputs equal to number of classes: `output_size`\n",
    "        - `softmax` activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above example illustrates a common architecture\n",
    "- a final *head* layer, specific to the task type\n",
    "    - regression\n",
    "    - classification\n",
    "- pre-head layers \n",
    "    - transform raw features\n",
    "    - into synthetic features \n",
    "    - that are best suited for the head layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compiling a model\n",
    "\n",
    "A primary purpose of the `compile` statement\n",
    "- associating a Loss Function with the model\n",
    "\n",
    "As we will see later, we can also associate\n",
    "- an optimizer to use in fitting\n",
    "- metrics to report during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   ## Fitting a model\n",
    "   \n",
    "Next, just as in `sklearn`: you \"fit\" the model to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=5,\n",
    "                        batch_size=128,\n",
    "                        shuffle=True,\n",
    "                        validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction \n",
    "\n",
    "The fitted model can be used to make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    # Evaluation\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'\\nTest accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Keras Functional Model\n",
    "\n",
    "- More verbose than `Sequential`\n",
    "- Also more flexible\n",
    "    - you can define more complex computation graphs (multiple inputs/outputs, shared layers)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "   \n",
    "    from keras.models import Model\n",
    "\n",
    "    # This returns a tensor\n",
    "    inputs = Input(shape=(784,))\n",
    "\n",
    "    # a layer instance is callable on a tensor, and returns a tensor\n",
    "    x = Dense(32, activation='relu')(inputs)\n",
    "    predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    # This creates a model that includes\n",
    "    # the Input layer and  Dense layers\n",
    "    model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Highlights:\n",
    "- Manually invoke a single layer at a time\n",
    "    - Passing as input the output of the prior layer.\n",
    "\n",
    "- You must define an `Input` layer (placeholder for the input/define its shape)\n",
    "    - `Sequential` uses the `input_shape=` parameter to the first layer\n",
    "- You \"wrap\" the graph into a \"model\" by a `Model` statement\n",
    "    - looks like a function definition\n",
    "        - names the input and output formal parameters\n",
    "    - a `Model` acts just like a layer (but with internals that you create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a beginner, you will probably exclusively use the Sequential model.\n",
    "\n",
    "Keep the Functional API in the back of your mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's code\n",
    "\n",
    "Let's see some [actual code](DNN_Keras_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some programming details\n",
    "\n",
    "## Keras: backend agnostic\n",
    "\n",
    "Keras is  one-level higher than the \"backend\" APIs for Deep Learning\n",
    "- TensorFlow, PyTorch, JAX\n",
    "\n",
    "If our code uses *only* methods within the Keras module\n",
    "- with no uses of backend-specific functions\n",
    "- the code can run on multiple backends\n",
    "    - TensorFlow, PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "<table>\n",
    "    <center><strong>Keras and the backends</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/Keras_hierarchy.png\" width=50%>\n",
    "    </tr>\n",
    "    \n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to achieve this *backend agnostic* coding\n",
    "- we must avoid explicit direct calls to the other Deep Learning toolkits\n",
    "- DO NOT import the backend\n",
    "\n",
    "        import tensorflow as tf\n",
    "\n",
    "        import torch\n",
    "\n",
    "- DO NOT use functions (e.g., `func`) in the backend namespace\n",
    "\n",
    "        tf.func\n",
    "        tensorflow.func\n",
    "        \n",
    "        torch.func\n",
    "- DO use [equivalent functions](https://keras.io/guides/migrating_to_keras_3/#transitioning-to-backendagnostic-keras-3) implemented in Keras\n",
    "\n",
    "\n",
    "        keras.func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We specify which backend to use via an environment variable that is set before importing Keras\n",
    "\n",
    "    import os\n",
    "    os.environ[\"KERAS_BACKEND\"] =  \"tensorflow\" # torch\n",
    "\n",
    "    import keras\n",
    "    \n",
    "We will *try very hard* to use only backend agnostic code\n",
    "- However: some older notebooks may not have been fully converted to be backend -agnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Input layer specification: explicit versus implicit\n",
    "\n",
    "In a `Sequential` model: the use of an *explicit* `Input` layer is optional\n",
    "- if present: it specifies the shape of an example, e.g. the tuple `INPUT_SHAPE`\n",
    "\n",
    "    `Input(shape=INPUT_SHAPE)`\n",
    "    \n",
    "- if absent:  there are two *implicit* ways to for the model `model` to obtain the shape of an example\n",
    "    - infer it from the first example (in the first batch) presented to the model\n",
    "    - via a `build` method that passes in the example shape *with an extra leading element (with value `None`)*\n",
    "    \n",
    "        `model.build(input_shape=(None,)+INPUT_SHAPE)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, suppose our examples are MNIST images in gray scale (one channel)\n",
    "\n",
    "    INPUT_SHAPE=(28,28,1)\n",
    "    \n",
    "The explicit `Input` layer would be\n",
    "\n",
    "    Input(shape=(28,28,1))\n",
    "    \n",
    "Using build on the object `model`\n",
    "\n",
    "    model.build(input_shape=(None,28,28,1)\n",
    "\n",
    "Note the leading batch dimension with value `None` in the tuple passed to`build`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is the import of knowing the shape of an example ?\n",
    "- it determines the shape of the weights for the first non-Input layer !\n",
    "- allowing us to allocate memory and initialize the weights\n",
    "\n",
    "For example\n",
    "- if the first non-`Input` layer is `Dense(10)`\n",
    "- and shape of an example is `(784,)`\n",
    "- then the `Dense(10)` layer\n",
    "    - has 784*10 weights (+ 10 bias weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is:\n",
    "- the number of weights for a layer is a function of\n",
    "    - the shape of the layer input\n",
    "    - the shape of the layer output\n",
    "    \n",
    "The first non-`Input` layer is the only layer for which we don't know the shape of the layer input.\n",
    "- For all other layers: the shape of the input is the shape of the output of the preceding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The advantage of the implicit methods of specifying the model input shape\n",
    "- it is dynamic\n",
    "- we can change the image dimension from `(28,28,1)`  to `(100,100,1)`\n",
    "- without any change in the model code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras implementations\n",
    "\n",
    "Confusion warning:\n",
    "\n",
    "There are two similar *but different* packages that implement Keras\n",
    "- the one from the Keras project, imported/used as\n",
    "\n",
    "        import keras\n",
    "    \n",
    "        # Use a Dense layer\n",
    "        keras.layers.Dense(...)\n",
    "    \n",
    "- one built into TensorFlow, imported/used as\n",
    "\n",
    "    `from tensorflow import keras`\n",
    "    \n",
    "    or via the `tf.keras` namespace\n",
    "    \n",
    "       import tensorflow as tf\n",
    "            \n",
    "    \n",
    "        # Use a Dense layer\n",
    "        tf.keras.layers.Dense(...)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**We will be using the first, so always**\n",
    "\n",
    "    import keras\n",
    "    \n",
    "This will be Keras 3, from the Keras Project.\n",
    "\n",
    "The advantage is that Keras 3 works **unchanged** across several Deep Learning frameworks \n",
    "- TensorFlow\n",
    "- PyTorch\n",
    "- JAX\n",
    "\n",
    "Keras (under the covers) will use code for the chosen framework (called the *back-end*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is highly desirable to write code using *only* the Keras API\n",
    "- *back-end agnostic*\n",
    "- *not* directly to the a particular framework's back-end\n",
    "\n",
    "We will try to do as much as possible\n",
    "- however\n",
    "    - older notebooks used Keras 2 (and the `tf.keras` namespace)\n",
    "    - some residual \"old code\" may still remain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras 2 (old version)\n",
    "\n",
    "There is a lot of code written in Keras 2\n",
    "- including some of the older notebooks in this repo\n",
    "\n",
    "FYI, here is information on using older code\n",
    "\n",
    "- [Keras 2 backward compatibility](https://keras.io/getting_started/#tensorflow--keras-2-backwards-compatibility)\n",
    "\n",
    "The key points:\n",
    ">>\n",
    "Meanwhile, the legacy Keras 2 package is still being released regularly and is available on PyPI as tf_keras (or equivalently tf-keras â€“ note that - and _ are equivalent in PyPI package names). To use it, you can install it via pip install tf_keras then import it via import tf_keras as keras.\n",
    ">>\n",
    "Should you want tf.keras to stay on Keras 2 after upgrading to TensorFlow 2.16+, you can configure your TensorFlow installation so that tf.keras points to tf_keras. To achieve this:\n",
    ">>\n",
    "Make sure to install tf_keras. Note that TensorFlow does not install it by default.\n",
    "Export the environment variable TF_USE_LEGACY_KERAS=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
