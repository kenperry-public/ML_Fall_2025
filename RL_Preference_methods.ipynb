{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\rewmod}{\\mathbb{r}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$\n",
    "\\newcommand{\\rewmod}{\\mathbb{r}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classic approach: PPO with Preferences instead of Rewards\n",
    "\n",
    "Before preference-oriented methods were introduced\n",
    "- PPO was the main method for RL with preferences\n",
    "- It still is \n",
    "\n",
    "But PPO is based on rewards (scalars) and we only have preferences (rankings).\n",
    "\n",
    "So we first have to translate preferences into rewards\n",
    "- and then use PPO as usual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Reward Model\n",
    "\n",
    "It is difficult and manually-intensive for a human to translate preferences to rewards\n",
    "- Need to be consistent across examples\n",
    "- No guiding principles\n",
    "\n",
    "Example\n",
    "\n",
    "        Task 1: reward scale 0-100\n",
    "        Task 2: reward scale 0-10\n",
    "\n",
    "Are we saying that the perfect example of Task 1 is 10 times more important than that of Task 2 ?\n",
    "- or is the human using a different scale ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead, we train a *Reward Model* $\\rewmod_\\phi$\n",
    "- Neural Network parameterized by $\\phi$\n",
    "- to translate rankings to rewards\n",
    "\n",
    "The model must satisfy\n",
    "\n",
    "$$\n",
    "\\rewmod_\\phi(x, y^+) > \\rewmod_\\phi(x, y^-)\n",
    "$$\n",
    "\n",
    "for all preferences\n",
    "$$\n",
    "(x, y^+, y^-)\n",
    "$$\n",
    "\n",
    "That is: the reward of the preferred choice is higher than the less preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The per-example Loss function is even stricter than this condition\n",
    "\n",
    "$$\n",
    "\\loss^\\ip_\\mathrm{reward}(\\phi, x, y^+, y^-) = - \\log \\sigma \\big(\n",
    "\\rewmod_\\phi(x, y^+) - \\rewmod_\\phi(x, y^-)\n",
    "\\big)\n",
    "$$\n",
    "\n",
    "It tries to *maximize the spread* between the two choices.\n",
    "\n",
    "In the Discussion: we will provide an interesting interpretation of this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Where does the training data for the Reward model come from ?\n",
    "\n",
    "- human preferences\n",
    "    - costly\n",
    "- LLM \"Judge\" preferences\n",
    "    - synthetic data in practice\n",
    "\n",
    "The training causes the reward model to\n",
    "imitate the preferences inherent in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reward model: training\n",
    "\n",
    "We illustrate how to train the reward model, using human preferences.\n",
    "\n",
    "- A prompt (context) is fed to the both a human (offline) and the model\n",
    "- The model creates *multiple* responses (continuation)\n",
    "- The Reward Model and the Human both rank the responses (calculate a reward)\n",
    "- The Loss function penalizes the model for model rewards that deviate from the human reward\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <img src=\"images/instructgpt_reward.png\" width=75%>\n",
    "    </tr> \n",
    "    <br>\n",
    "     <tr> \n",
    "         <center>context = prompt; continuation = response</center>\n",
    "    </tr>\n",
    "    <br><br>\n",
    "    <tr>\n",
    "    Source: https://arxiv.org/pdf/1909.08593.pdf#page=2\n",
    "    </tr>\n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Reward model is thus trained to mimic Human Preferences.\n",
    "\n",
    "Similarly: we could replace the human with an LLM Judge \n",
    "- synthetic data type approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\loss^\\ip_\\mathrm{reward}(\\phi, x, y^+, y^-)$ interpretation: Preference Classifier\n",
    "\n",
    "Consider the form of the per-example Loss\n",
    "$$\n",
    "\\loss^\\ip_\\mathrm{reward}(\\phi, x, y^+, y^-) = - \\log \\sigma \\big(\n",
    "\\rewmod_\\phi(x, y^+) - \\rewmod_\\phi(x, y^-)\n",
    "\\big)\n",
    "$$\n",
    "\n",
    "It is similar to the Binary Cross-Entropy Loss term for Positive examples\n",
    "\n",
    "$$\n",
    "\\log \\pr{\\x}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\pr{\\x} = \\sigma(\\z)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, in Logistic Regression\n",
    "\n",
    "- we compute a score $z$\n",
    "- convert $z$ to a probability $\\pr{\\x} = \\sigma(z)$ \n",
    "    - probability of \"example being Positive\" via the sigmoid function\n",
    "- use Binary Cross Entropy as the Loss\n",
    "    - sum of terms for \n",
    "        - Positive examples: $ - \\log( \\sigma(z) )$ \n",
    "        - Negative examples: $ \\log (1 - \\sigma(z))$\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our case: the triples are all \"Positive\" examples so cross entropy collapses to $ - \\log( \\sigma(z) )$ \n",
    "\n",
    "So we can view \n",
    "$$\n",
    "\\loss^\\ip_\\mathrm{reward}(\\phi, x, y^+, y^-) = - \\log \\sigma \\big(\n",
    "\\rewmod_\\phi(x, y^+) - \\rewmod_\\phi(x, y^-)\n",
    "\\big)\n",
    "$$\n",
    "\n",
    "as the Cross Entropy Loss of predicting the probability\n",
    "$$\n",
    "\\prc{y^+ \\text{ preferred to } y^-}{x}\n",
    "$$\n",
    "where the score $z$ is\n",
    "$$\n",
    "z = \\rewmod_\\phi(x, y^+) - \\rewmod_\\phi(x, y^-)\n",
    "$$\n",
    "\n",
    "Thus, the Loss is identical for a Classification task\n",
    "- probability that $y^+$ is preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "The usual critique of PPO is that involves multiple models\n",
    "- usual instances of the same model for Policy\n",
    "\n",
    "They are\n",
    "- Policy Model\n",
    "- Reference Model\n",
    "    - recall: PPO Surrogate Loss constrains Policy Model updates to not deviate too far from the Reference Model\n",
    "- Value/Critic: for advantage computation\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "PPO for Preferences involves an additional model\n",
    "- Reward Model\n",
    "\n",
    "This has motivated the search for alternate methods for Preference Data.\n",
    "\n",
    "| Model Type   | Typical Size                           | Role                          |\n",
    "|:--------------|:---------------------------------------|:-------------------------------|\n",
    "| Policy Model | Large-scale pretrained LM (billions) | Generate responses            |\n",
    "| Reward Model | Smaller/fine-tuned LM or distilled (hundreds of millions) | Score outputs based on preferences |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Direct Preference Optimization (DPO)\n",
    "\n",
    "The key idea of PPO was the translation of preferences into rewards.\n",
    "\n",
    "We did this by training a Reward model to maximize the\n",
    "- *spread in rewards*\n",
    "- between preferred $y^+$ and non-preferred $y^-$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Direct Preference Optimization (DPO)* takes this core idea but simplifies the process\n",
    "\n",
    "It directly maximizes the\n",
    "- *spread in probability*\n",
    "- between preferred $y^+$ and non-preferred $y^-$\n",
    "- being generated by the policy $\\pi$\n",
    "\n",
    "No need to convert preferences to rewards !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This method is\n",
    "- Supervised Fine Tuning\n",
    "- rather than Reinforcement Learning\n",
    "\n",
    "and can be formulated as a Binary Classification problem\n",
    "- maximize the spread in probability of the Preferred (\"Positive\" label) vs Non-Preferred (\"Negative\" label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Direct Preference Optimization (DPO)** is thus a policy optimization method \n",
    "where supervision\n",
    "- comes from *Preference Data*\n",
    "$$\n",
    "(x, y^+, y^-)\n",
    "$$\n",
    "- rather than rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relation to the Unified Gradient Formulation\n",
    "\n",
    "DPO also uses a surrogate loss to guide the derivation of the policy.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "L_{\\mathrm{DPO}}(\\theta) = -\\log \\sigma \\left( \\log \\frac{\\pi_\\theta(y^+|x)}{\\pi_\\theta(y^-|x)} \\right)\n",
    "= -\\log \\sigma \\left( \\Delta \\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\pi_\\theta (y^+ | x )$\n",
    "- $\\pi_\\theta (y^- | x )$\n",
    "\n",
    "\n",
    "are the probabilities of the *trajectories* resulting in outputs $y^+$ and $y^-$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The probability ratio\n",
    "$$\n",
    "\\frac{\\pi_\\theta(y^+|x)}{\\pi_\\theta(y^-|x)}\n",
    "$$\n",
    "\n",
    "is the relative probability of the preferred output, compared to the non-preferred output._\n",
    "\n",
    "- We take the log of the ratio\n",
    "    - resulting in log-probabilities, as in the Universal Formulation\n",
    "\n",
    "- The sigmoid $\\sigma$ converts the relative probability to the range $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can compute the gradient of $L_{\\mathrm{DPO}}(\\theta)$\n",
    "\n",
    "We first simplify $L_{\\mathrm{DPO}}(\\theta)$ by defining\n",
    "\n",
    "$$\n",
    "\\Delta = \\log \\frac{\\pi_\\theta(y^+|x)}{\\pi_\\theta(y^-|x)}  = \\log \\pi_\\theta(y^+|x) - \\log \\pi_\\theta(y^-|x)\n",
    "$$\n",
    "\n",
    "Substituting into  $L_{\\mathrm{DPO}}(\\theta)$\n",
    "\n",
    "$$\n",
    "L_{\\mathrm{DPO}}(\\theta) = -\\log \\sigma \\left( \\Delta \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The gradient of this simplified $L_{\\mathrm{DPO}}(\\theta)$ is\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\theta L_{\\mathrm{DPO}}(\\theta) & = & -(1 - \\sigma(\\Delta)) \\cdot \\nabla_\\theta \\Delta \\\\\n",
    "& = & -(1 - \\sigma(\\Delta)) \\cdot \n",
    "\\big( \n",
    "\\nabla_\\theta \\log \\pi_\\theta(y^+|x)\n",
    "-\n",
    "\\nabla_\\theta \\log \\pi_\\theta(y^-|x)\n",
    "\\big)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This follows from basic rules of calculus\n",
    "- $\\nabla_\\theta \\log  \\sigma(\\Delta ) = \\frac{1}{\\sigma(\\Delta ) } \\nabla_\\theta \\sigma(\\Delta ) $\n",
    "- $\\nabla_\\theta \\sigma(\\Delta ) = \\sigma(\\Delta) \\big( 1 - \\sigma(\\Delta) \\big) * \\nabla_\\Theta \\Delta$\n",
    "    - since $\\sigma(\\Delta) = \\frac{1}{1 + e^{-\\Delta}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The term \n",
    "$$\n",
    "(1 - \\sigma(\\Delta))\n",
    "$$\n",
    "\n",
    "is interpreted as the *Advantage* of $y^+$ over $y^-$.\n",
    "\n",
    "This advantage is small\n",
    "- when $\\sigma(\\Delta) \\approx 1$\n",
    "    - i.e., the model is confident: assigning high probability to the preferred output\n",
    "\n",
    "Conversely, it is large when the model is uncertain.\n",
    "\n",
    "So the advantage term adjusts the Gradient update step size depending on how far the\n",
    "probability of the preferred output is from 100%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The gradient can be interpreted as adjusting the policy\n",
    "- so as to increase the (log) likelihood\n",
    "- adjusted by the advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $L_{\\mathrm{DPO}}(\\theta)$ interpretation: Preference Classifier\n",
    "\n",
    "Consider the form of the per-example Loss\n",
    "\n",
    "$$\n",
    "L_{\\mathrm{DPO}}(\\theta) = -\\log \\sigma \\left( \\log \\frac{\\pi_\\theta(y^+|x)}{\\pi_\\theta(y^-|x)} \\right)\n",
    "= -\\log \\sigma \\left( \\Delta \\right)\n",
    "$$\n",
    "\n",
    "Just as we observed for \n",
    "$L_{\\mathrm{PPO}}(\\theta)$ in the section on PPO\n",
    "- It is similar to the Binary Cross-Entropy Loss term for Positive examples\n",
    "\n",
    "$$\n",
    "\\log \\pr{\\x}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\pr{\\x} = \\sigma(\\Delta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But, for DPO\n",
    "- $\\Delta$ is not the score $z$ (the logit)\n",
    "- it is the *spread* in log probabilities of the Preferred and Non-Preferred choices\n",
    "\n",
    "$$\n",
    "\\Delta =  \\log \\frac{\\pi_\\theta(y^+|x)}{\\pi_\\theta(y^-|x)} = \\log \\pi_\\theta(y^+|x) - \\log \\pi_\\theta(y^-|x)\n",
    "$$\n",
    "\n",
    "Thus\n",
    "$$L_{\\mathrm{DPO}}(\\theta)$$\n",
    "\n",
    "is equivalent to the Binary Cross Entropy loss for the problem \n",
    "predicting the probability\n",
    "$$\n",
    "\\prc{y^+ \\text{ preferred to } y^-}{x}\n",
    "$$\n",
    "\n",
    "So the Loss is identical to that for the Classification task\n",
    "- probability that $y^+$ is preferred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPO vs PPO\n",
    "\n",
    "DPO avoids the main critique of PPO\n",
    "- the need for multiple models\n",
    "- each potentially consuming many parameters\n",
    "\n",
    "It avoids the artificial creation of rewards and operates directly on probabilities.\n",
    "\n",
    "Supervised Fine Tuning of a Classifier-like Loss is used in DPO\n",
    "- rather than Reinforcement Learning as in PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Approach         | Data Used                  | Training Steps                            | Model Copies Required          | Key Challenge                          |\n",
    "|:------------------|:----------------------------|:-----------------------------------------|:-------------------------------|:--------------------------------------|\n",
    "| PPO + Reward Model| Preference â†’ Scalar rewards | Train reward model, then PPO optimization | Policy, reference, value, reward | Reward modeling complexity, instability |\n",
    "| DPO              | Preference pairs directly  | Single-stage policy gradient optimization | Only policy                   | Requires careful pairing, but simpler overall |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Pseudo code for DPO\n",
    " \n",
    "    # DPO training for LLM\n",
    "    for prompt in training_prompts:\n",
    "        outputs = [llm.generate(prompt) for _ in range(2)]\n",
    "        \n",
    "        # outputs: [output_0, output_1]\n",
    "        # preference: 0 if output_0 is preferred, 1 if output_1 is preferred\n",
    "        preferred_idx = compare_outputs(outputs) # Human or synthetic comparison\n",
    "\n",
    "        logit_pref = llm.score(outputs[preferred_idx], prompt)\n",
    "        logit_nonpref = llm.score(outputs[1 - preferred_idx], prompt)\n",
    "\n",
    "        # DPO loss: maximize difference so preferred > non-preferred\n",
    "        loss = -logsigmoid(logit_pref - logit_nonpref)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Group Relative Policy Optimization (GRPO)\n",
    "\n",
    "*Group Relative Policy Optimization (GRPO)* is an innovative, policy-based method for learning from Preference Data.\n",
    "\n",
    "It cleverly circumvents several issues common to RL for Preferences\n",
    "- Scarcity of training examples\n",
    "- Difficulty in constructing a Reward model and calibrating rewards\n",
    "\n",
    "It essentially \n",
    "- generates its own synthetic preference dataset as part of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "Given training dataset $D$\n",
    "- traditional RL  learns from each questions/answer pair $(q,a) \\in D$\n",
    "\n",
    "So learning is limited by the size of $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "GRPO creates $G$ synthetic examples from $q$\n",
    "- prompting a model multiple ($G$) times with the same question $q$\n",
    "    - with non-zero temperature: will get $G$ different answers with high probability\n",
    "- resulting in a *group* of $G$ responses to the same $q$\n",
    "\n",
    "$$\n",
    "\\{ (q, o_i) \\, | \\, 1 \\le i \\le G \\}\n",
    "$$\n",
    "\n",
    "which are then ranked (to be described).\n",
    "\n",
    "This is the *Group* part of the GRPO name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As in most Policy-gradient method\n",
    "- an advantage is computed for each\n",
    "- the policy is updated to favor positive advantage responses/disfavor negative advantage responses\n",
    "\n",
    "In other methods for Preference Data\n",
    "- each question $q$ provides a *single* opportunity to learn a better policy\n",
    "- by learning from a *pair* $(y^+, y^-)$\n",
    "\n",
    "GRPO *synthetically* \n",
    "- creates $G$ opportunities\n",
    "\n",
    "to learn from each $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reward model construction an calibration\n",
    "\n",
    "\n",
    "### Construction\n",
    "\n",
    "In GRPO the user creates a simple Reward Model\n",
    "- by **writing a function** that measures *qualitative* aspects of the response\n",
    "- typically\n",
    "    - a sum of several component qualitative measures\n",
    "    \n",
    "This reward model is *hard-coded*\n",
    "- not learned by training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate, suppose we want to post-train a model to \"think before answering\".\n",
    "\n",
    "One quality metric is whether the response $o_i$ *obeys the format* of a \"thinking\" response\n",
    "\n",
    "    <think> Thought_1, Thought_2, ... </think> <answer> Answer </answer>\n",
    "\n",
    "The function for this component of the reward can assign higher rewards the closer $o_i$ is to the ideal format\n",
    "- presence and ordering of tags `<think>, </think>, <answer>, </answer>`\n",
    "- number of thinking steps\n",
    "    - not too few or too many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A second very important quality metric: is $o_i$ a correct response to $q$ ?\n",
    "- an easily evaluated binary measure for problems with verifiable responses (e.g. math problems)\n",
    "- AI Judge to measure quality of problems with non-verifiable responses\n",
    "\n",
    "The final Reward sums up the component rewards.\n",
    "\n",
    "The user can define the relative importance of the components\n",
    "- weighted sum or different reward scales for each component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key insight: Calibration of Rewards is not necessary\n",
    "\n",
    "A key issue with human constructed rewards is *calibration*.\n",
    "\n",
    "For \n",
    "- two different classes of problem instance\n",
    "- or two different humans assigning rewards to instances of the same class\n",
    "    - or even the same instance\n",
    "    \n",
    "are the two rewards on a comparable scale ?\n",
    "\n",
    "For example\n",
    "- Human 1 uses a scale of 0 to 10\n",
    "- Human 2 uses a scale of 0 to 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "GRPO uses rewards \n",
    "- *indirectly*\n",
    "- only to \n",
    " *pseudo-rank* each response $o_i$ in the group of responses for questions $q$.\n",
    "\n",
    "*Ranks* don't have magnitude (just order)\n",
    "- this bypasses the need to calibrate rewards\n",
    "\n",
    "Moreover\n",
    "- ranks are comparable across different questions $q, q'$\n",
    "    - both are in the range $[1,G]$\n",
    "- similarly for two humans\n",
    "Human 1 and Human 2, even though they may use different scales\n",
    "-\n",
    "\n",
    "This ranking is the source of the *Group Relative* parts of the GRPO name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantage definition\n",
    "\n",
    "The Group Relative ranking is implemented via the definition of the Advantage function.\n",
    "\n",
    "In GRPO, \n",
    "the Advantage of a response is defined\n",
    "- **relative** to its group\n",
    "- rather than an *absolute* advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a prompt/input $x$\n",
    "- a group $$g = \\rm{group}(x)$$ \n",
    "\n",
    "of size $G$ sample responses are collected\n",
    "\n",
    "\n",
    "For each sample response $y_i \\in g$ \n",
    "- there is a corresponding reward $\\rewseq(g,  y_i)$\n",
    "- via the hard-coded reward function\n",
    "\n",
    "The rewards within group $g$ are normalized, giving the advantage for $y_i$ as\n",
    "$$\n",
    "A_{g,y_i} = \\frac{\\rewseq(g,  y_i) - \\mu_g}{\\sigma_g}\n",
    "$$\n",
    "\n",
    "where $(\\mu_g, \\sigma_g)$ are the mean and standard deviation of the rewards withing group $g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By subtracting the mean\n",
    "- responses with below average rewards are penalized\n",
    "- responses with above average rewards are favored\n",
    "\n",
    "By normalizing via the standard deviation\n",
    "- the Advantages of the responses to different queries\n",
    "- are on a **similar scale**\n",
    "    - z-score relative to its group\n",
    "\n",
    "So the *pseudo-rank* we alluded to is really a z-score (number of standard deviations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Surrogate Loss for GRPO\n",
    "\n",
    "The Surrogate Loss for GRPO is a more complicated version of the Surrogate Loss for PPO.\n",
    "\n",
    "Recall:\n",
    "\n",
    "The Surrogate Loss for PPO is\n",
    "$$\n",
    "J_{\\mathrm{PPO}}(\\theta) = \\mathbb{E}_\\tt \\left[\n",
    "\\min{} \\left(\n",
    "r_\\tt(\\theta) \\hat{A}_\\tt,\\;\n",
    "\\mathrm{clip}\\left(r_\\tt(\\theta), 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{\\advseq}_\\tt\n",
    "\\right)\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Surrogate Loss for GRPO is\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{GRPO}}(\\theta) = \\mathbb{E}_{(q,a) \\sim D, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\mathrm{old}}(\\cdot|q)} }\\left[\n",
    "\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{\\tt=1}^{|o_i|} \n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "- \\beta \\,\\mathrm{D}_{\\mathrm{KL}}(\\pi_\\theta || \\pi_{\\mathrm{ref}})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Where: \n",
    "- $D$ is the set of training examples: $(q,a)$ query/answer pairs\n",
    "- $G$ is the number of sampled outputs per prompt  \n",
    "- $|o_i|$ is the token length of output $o_i$  \n",
    "- $r_{i,\\tt}(\\theta) = \\frac{\\pi_\\theta(o_{i,\\tt} | q, o_{i,<\\tt})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,\\tt} | q, o_{i,<\\tt})}$ is the per-token importance sampling ratio  \n",
    "- $\\hat{A}_{i,\\tt}$ is the group-relative normalized advantage for token $\\tt$ in sample $i$  \n",
    "- $\\beta$ is the KL penalty coefficient controlling divergence from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We see the familiar\n",
    "- probability ratio\n",
    "- clipping\n",
    "\n",
    "and the addition of a KL divergence term\n",
    "- in order to further constrain policy changes between epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main difference is in the Expectation\n",
    "- which is adapted for all members of the group\n",
    "\n",
    "$$\n",
    " \\mathbb{E}_{(q,a) \\sim D, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\mathrm{old}}(\\cdot|q)} }\n",
    "$$\n",
    "\n",
    "which results in the averaging expressions\n",
    "\n",
    "$$\n",
    "\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{\\tt=1}^{|o_i|} \n",
    "\\min{} \n",
    "$$\n",
    "\n",
    "- normalization across responses of different length\n",
    "- averaged over the group\n",
    "\n",
    "We will have a more detailed discussion of the averaging methodology choice in\n",
    "the section of DAPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relation to the Unified Gradient Formulation\n",
    "\n",
    "The simplified Policy Gradient Formulation is\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) =  \\mathbb{E}_{x} \\left[  \\sum_{y \\in \\rm{group}(x)} \\nabla_\\theta \\log \\pi_\\theta(y | x) A_{\\rm{group}(x),y} \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "where\n",
    "- $x$ is an input to the LLM\n",
    "- $\\rm{group}(x)$ is a set of LLM outputs, given $x$ as input\n",
    "    - since the LLM actions are probabilistic\n",
    "- the advantage $A_{\\rm{group}(x),y}$\n",
    "    - of a particular response $y \\in  \\rm{group}(x)$  \n",
    "    - is *relative* to other members of $\\rm{group}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "The Surrogate Loss for GRPO and PPO\n",
    "- are similar in form\n",
    "\n",
    "But GRPO is a major simplification over PPO\n",
    "- *eliminates* the need for a Value function\n",
    "    - using trajectory-level rewards\n",
    "    - compared to token-level rewards for PPO (derived from the Value function)\n",
    "\n",
    "GRPO differs from PPO \n",
    "- adds a KL-constraint to limit the policy update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Group-relative, normalized Advantage**\n",
    "\n",
    "- **Normalization**\n",
    "\n",
    "The advantage of a response $y$ given input $x$\n",
    "- is  relative to alternate responses to the same inputs $x$\n",
    "    - in units of \"number of standard deviations of the group\"\n",
    "- across groups:\n",
    "    - there is a different standard deviation per group\n",
    "    - but the response to two different inputs $x, x'$ (and hence groups $g, g'$) are in similar units\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, normalization to mean $0$ and  unit standard deviation\n",
    "- reduces variance of gradients\n",
    "- smoother parameter update\n",
    "\n",
    "By subtracting a baseline (e.g., the mean)\n",
    "- policy updates favor/disfavor responses with above/below average rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Groups**\n",
    "\n",
    "Having multiple responses per prompt $x$\n",
    "- provides multiple updates per prompt, rather than just a single response $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pseudo code for GRPO\n",
    "\n",
    "**Detailed Surrogate Loss for GRPO**\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{GRPO}}(\\theta) = \\mathbb{E}_{(q,a) \\sim D, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\mathrm{old}}}(\\cdot|q)} \\left[\n",
    "\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{\\tt=1}^{|o_i|} \n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "- \\beta \\mathrm{D}_{\\mathrm{KL}}(\\pi_\\theta || \\pi_{\\mathrm{ref}})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $G$ is the number of sampled outputs per prompt  \n",
    "- $|o_i|$ is the token length of output $o_i$  \n",
    "- $r_{i,\\tt}(\\theta) = \\frac{\\pi_\\theta(o_{i,\\tt} | q, o_{i,<\\tt})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,\\tt} | q, o_{i,<\\tt})}$ is the per-token importance sampling ratio  \n",
    "- $\\hat{A}_{i,\\tt}$ is the group-relative normalized advantage for token $\\tt$ in sample $i$  \n",
    "- $\\beta$ is the KL penalty coefficient controlling divergence from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    # GRPO training for LLM\n",
    "    for prompt in training_prompts:\n",
    "        outputs = [llm.generate(prompt) for _ in range(group_size)]\n",
    "        advantages = compute_group_relative_advantages(outputs, prompt) # e.g., using human rank or scoring function\n",
    "\n",
    "        # Compute loss for all outputs (favor those with high relative advantage)\n",
    "        losses = []\n",
    "        for output, advantage in zip(outputs, advantages):\n",
    "            logprob = llm.logprob(output, prompt)\n",
    "            losses.append(-logprob * advantage)\n",
    "\n",
    "        loss = sum(losses) / group_size\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "where\n",
    "\n",
    "    llm.generate(prompt)\n",
    "    \n",
    "is a call to the model, using a prompt `prompt` (e.g., $q$), to generate a responses (e.g., $o_i$)\n",
    "\n",
    "Key Points: \n",
    "    \n",
    "- Multiple candidates sampled per prompt; \n",
    "- each gets an advantage score\n",
    "- updates increase likelihood of better completions in the group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAPO: An Improved GRPO; Case Study of Real-World\n",
    "\n",
    "\n",
    "\n",
    "The GRPO model was introduced by DeepSeek, which published the algorithm pseudo-code.\n",
    "\n",
    "However, as is often the case, the \"reference\" model (i.e., pseudo-code) \n",
    "- is *not sufficient* to replicate the performance results quoted in the paper.\n",
    "\n",
    "There are substantial \"engineering details\" that are not disclosed.\n",
    "- The Performance Metrics quoted by DeepSeek\n",
    "- are 50% higher than those obtained by others trying to replicated the algorithm from the pseudo-code\n",
    "\n",
    "\n",
    "Details matter !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Responding to this, ByteDance introduced a new model:\n",
    "\n",
    "- *Decoupled\n",
    "Clip and Dynamic sAmpling Policy Optimization (DAPO)*\n",
    "\n",
    "in the paper:\n",
    "[DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/pdf/2503.14476).\n",
    "\n",
    "DAPO is\n",
    "\n",
    "- fully Open Source\n",
    "- that provides refinements to the \"reference\" GRPO pseudo-code\n",
    "- sufficient to achieve similar results to the published DeepSeek paper\n",
    "\n",
    "This was the result of\n",
    "- experimentation\n",
    "- error analysis\n",
    "\n",
    "to diagnose the failures of baseline GRPO and devise remedies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DAPO introduces 4 key modifications on baseline GRPO\n",
    "- Changing the threshold for clipping\n",
    "- *Eliminating* non-informative samples \n",
    "- Careful redefinition of Token-level Gradient Loss\n",
    "- Penalizing overly long responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evolution of $J_{\\mathrm{DAPO}}(\\theta)$\n",
    "\n",
    "DAPO uses a Surrogate Loss that differs from that of GRPO\n",
    "- in *subtle* ways\n",
    "\n",
    "To highlight the differences in Surrogate Loss\n",
    "- we incrementally modify $J_{\\mathrm{GRPO}}(\\theta)$\n",
    "- to obtain $J_{\\mathrm{DAPO}}(\\theta)$\n",
    "\n",
    "**We defer the justification for the modifications** until after the final $J_{\\mathrm{DAPO}}(\\theta)$\n",
    "has been derived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's start by recalling  the Surrogate Loss for GRPO:\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{GRPO}}(\\theta) = \\mathbb{E}_{(q,a) \\sim D, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\mathrm{old}}}(\\cdot|q)} \\left[\n",
    "\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{\\tt=1}^{|o_i|} \n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "- \\beta \\, \\mathrm{D}_{\\mathrm{KL}}(\\pi_\\theta || \\pi_{\\mathrm{ref}})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\mathcal{D}$ is the training dataset\n",
    "    - consisting of examples consisting of question/answer pairs\n",
    "    $$(q, a)$$\n",
    "- $\\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\mathrm{old}}}(\\cdot|q)$\n",
    "    - are the $G$ answers in the group for a questons $q$\n",
    "\n",
    "Recall that GRPO\n",
    "- creates a group of size $G$ answers to $q$\n",
    "- by sampling from the LLM with non-zero temperature\n",
    "\n",
    "We modify the  GRPO Surrogate Loss in increments to obtain the Surrogate Loss for DAPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Remove KL constraint\n",
    "\n",
    "First, observe that, relative to GRPO\n",
    "- DAPO removes the KL-divergence constraint\n",
    "$$\n",
    "\\mathrm{D}_{\\mathrm{KL}}(\\pi_\\theta || \\pi_{\\mathrm{ref}})\n",
    "$$\n",
    "\n",
    "that limits how far\n",
    "- new policy $\\pi_\\theta$\n",
    "- can diverge from the *reference* policy $\\pi_\\text{ref}$.\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{DAPO^1}}(\\theta) = \\mathbb{E}_{(q,a) \\sim D, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\mathrm{old}}}(\\cdot|q)} \\left[\n",
    "\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{\\tt=1}^{|o_i|} \n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "giving us the intermediate $J_{\\mathrm{DAPO^1}}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Changing the token-level average\n",
    "\n",
    "\n",
    "GRPO\n",
    "- First normalizes response advantages to make them independent of response length\n",
    "    - By dividing by length $ |o_i| $ of each response $o_i$\n",
    "    - Recall: there is a measure per token of the response\n",
    "- Then averages sample losses uniformly over group $G$.\n",
    "\n",
    "DAPO\n",
    "- Sums all token losses in the group before normalizing by total tokens in all samples.\n",
    "\n",
    "giving us the intermediate \n",
    "\n",
    "$$\n",
    "J_{\\mathrm{DAPO^2}}(\\theta) =\n",
    "\\mathbb{E}_{(q,a) \\sim \\mathcal{D}, \\{ o_i \\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot | q)}\n",
    "\\left[\n",
    "\\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{\\tt=1}^{|o_i|}\n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This is referred to as the *Token-level Loss* technique in the paper.\n",
    "\n",
    "We will explain the purpose of this subtle change in the subsequent Discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Asymmetric Clipping \n",
    "\n",
    "GRPO's clipping is symmetric with range\n",
    "$$\n",
    "[ 1 - \\epsilon, 1 + \\epsilon ]\n",
    "$$\n",
    "\n",
    "\n",
    "DAPO adds different clipping values on either side\n",
    "$$\n",
    "[ 1 - \\epsilon_{\\mathrm{low}}, 1 + \\epsilon_{\\mathrm{high}} ]\n",
    "$$\n",
    "\n",
    "giving us the intermediate \n",
    "\n",
    "$$\n",
    "J_{\\mathrm{DAPO^3}}(\\theta) =\n",
    "\\mathbb{E}_{(q,a) \\sim \\mathcal{D}, \\{ o_i \\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot | q)}\n",
    "\\left[\n",
    "\\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{\\tt=1}^{|o_i|}\n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon_{\\mathrm{low}}, 1 + \\epsilon_{\\mathrm{high}} \\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This is referred to in the paper as the *Clip Higher* technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dynamic sampling\n",
    "\n",
    "GRPO's expectation is over\n",
    "- **all** responses within the group for a given question $q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DAPO limits the Group composition\n",
    "- to ensure that **all** intra-group rewards **are not identical**\n",
    "- expressed as the constraint\n",
    "\n",
    "$$\n",
    "0 < \\big| \\{ o_i \\mid \\text{is_equivalent}(a, o_i) \\} \\big| < G\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    " $$\\text{is_equivalent}(a, o_i)$$\n",
    " \n",
    "is true if response $o_i$ is equivalent to target response $a$\n",
    "\n",
    "Thus the expression is a *count* of equivalent responses\n",
    "- and the group is kept *only if* count is strictly less than $G$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This gives us the final\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{DAPO}}(\\theta) =\n",
    "\\mathbb{E}_{(q,a) \\sim \\mathcal{D}, \\{ o_i \\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot | q)}\n",
    "\\left[\n",
    "\\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{\\tt=1}^{|o_i|}\n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon_{\\mathrm{low}}, 1 + \\epsilon_{\\mathrm{high}} \\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "\\right],\n",
    "$$\n",
    "subject to:\n",
    "$$\n",
    "0 < \\big| \\{ o_i \\mid \\text{is_equivalent}(a, o_i) \\} \\big| < G\n",
    "$$\n",
    "\n",
    "where $a$ is the (single) response in the training dataset $\\mathcal{D}$\n",
    "- before sampling to generate $G$ sample responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRPO vs DAPO Comparison\n",
    "\n",
    "\n",
    "| Feature                    | GRPO                                           | DAPO                                              |\n",
    "|:----------------------------|:------------------------------------------------|:---------------------------------------------------|\n",
    "| Loss Aggregation           | $ \\frac{1}{G}\\sum_i \\frac{1}{|o_i|} \\sum_t $ | $ \\frac{1}{\\sum_i |o_i|} \\sum_i \\sum_t $        |                            |\n",
    "| Effect on Long Responses   | Weighed less per token (gradient dilution)     | Maintains per-token gradient magnitude            |\n",
    "| Clipping                  | Symmetric $(1-\\epsilon, 1+\\epsilon)$            | Asymmetric $(1-\\epsilon_{\\mathrm{low}}, 1+\\epsilon_{\\mathrm{high}})$ |\n",
    "| Impact                    | Slower learning on longer tokens                | Finer control, better learning on complex outputs |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Aspect                | GRPO                                               | DAPO                                                        |\n",
    "|:----------------------|:---------------------------------------------------|:------------------------------------------------------------|\n",
    "| Value Model Dependency| Removes PPO value model, uses relative group rewards| Same, but more robust and stable optimization               |\n",
    "| Clipping Mechanism    | Basic clip bounds on importance ratios              | Asymmetric Clip-Higher for rare, valuable tokens            |\n",
    "| Sampling              | Multi-response batch, potentially redundant         | Dynamic Sampling ensures diverse, effective samples         |\n",
    "| Gradient Weighting    | Token-level, gradient diluted in long outputs       | Token-Level Gradient Loss corrects signal dilution          |\n",
    "| Reward for Length     | None or truncation-based                            | Overlong Reward Shaping with soft penalties                 |\n",
    "| Efficiency/Stability  | Improved over PPO, but unstable for some scenarios | Superior stability and efficiency; state-of-the-art results |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Feature             | PPO                                                      | GRPO                                                     | DAPO                                                           |\n",
    "|:--------------------|:---------------------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------------|\n",
    "| Advantage Source    | Value model estimate                                     | Group-relative, reward normalized in batch                | Same as GRPO                                                   |\n",
    "| Clipping            | Symmetric ($1-\\epsilon$, $1+\\epsilon$)                   | Symmetric ($1-\\epsilon$, $1+\\epsilon$)                    | Asymmetric ($1-\\epsilon_\\text{low}$, $1+\\epsilon_\\text{high}$) (Clip-Higher) |\n",
    "| Loss Aggregation    | Per-sample/token                                         | Per-sample, then averaged over tokens                     | Per-token, avoids dilution in long outputs                     |\n",
    "| Sampling            | Independent samples                                      | Groups of samples per prompt                              | Dynamic: only informative samples (not all-correct/incorrect)   |\n",
    "| Reward Shaping      | None by default                                          | None by default                                           | Overlong shaping (discourages excessive length)                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KL-constraint elimination: why ?\n",
    "\n",
    "For many RL objectives (e.g., induce long CoT reasoning in the response)\n",
    "- the *new distribution*\n",
    "- is necessarily *far* from the reference distribution\n",
    "\n",
    "So the KL constraint is too limiting for some tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Speculation**\n",
    "\n",
    "DeepSeek initially tried to induce reasoning \n",
    "- using **only** RL\n",
    "- **no** SFT step before the RL\n",
    "\n",
    "As we have seen\n",
    "- SFT \"primes the pump\" to make RL more likely to succeed\n",
    "- by ensuring the initial RL model is able to produce\n",
    "    - correctly formatted responses\n",
    "    \n",
    "Perhaps the KL-constraint is a vestige of the RL-only attempt\n",
    "- since the long CoT desired model was far different from the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Token-level averaging change: why ?\n",
    "\n",
    "The GRPO loss aggregation\n",
    "$$\n",
    "\\frac{1}{G}\\sum_i \\frac{1}{|o_i|} \\sum_\\tt \\ldots\n",
    "$$\n",
    "\n",
    "creates a loss for sample $i$ in group $g$\n",
    "- that is the *average over the tokens* of response $o_i$\n",
    "\n",
    "before averaging the sample loss over the $G$ elements of the group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This introduces a *bias*\n",
    "- the gradient update of an \"important\" token in a **long** response $o_i$\n",
    "    - important: high gradient\n",
    "- has less weight in the Gradient of $J_{\\mathrm{DAPO}}(\\theta)$\n",
    "- than a token of *equal importance* in a **shorter** response $o_{i'}$\n",
    "\n",
    "even when the advantages $\\hat A_{i}$ and $\\hat A_{i'}$ are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reward hacking\n",
    "\n",
    "This can lead to the undesirable phenomenon known as *Reward Hacking*.\n",
    "- where the response length is manipulated by the RL feedback\n",
    "- to change the relative contribution to policy update\n",
    "- of **important** tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RL learns to produce *short **correct** responses* \n",
    "- with high trajectory (and thus, high per-token) reward\n",
    "- to amplify the contribution of important tokens\n",
    "\n",
    "and *long **incorrect** responses*\n",
    "- with low trajectory (and thus, low per-token) reward\n",
    "    - Especially: *negative* rewards\n",
    "- to dilute the contribution of important tokens\n",
    "    - adding gibberish or repetitive content in order to increase length\n",
    "\n",
    "The result is that\n",
    "- the impact of important tokens\n",
    "- on updating the policy \n",
    "- is distorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DAPO eliminates this bias by changing the aggregation\n",
    "\n",
    " $ \\frac{1}{\\sum_i |o_i|} \\sum_i \\sum_\\tt \\ldots $\n",
    " \n",
    "so that\n",
    "- *all tokens in all responses in a group*\n",
    "- have the same contribution to the Gradient of $J_{\\mathrm{DAPO}}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reward shaping: Overlong reward shaping length penalty\n",
    "\n",
    "DAPO also adds a *length penalty* to directly discourage excessively long responses\n",
    "\n",
    "$$\n",
    "\\tilde{R}_i = R_i - \\alpha \\cdot \\text{LengthPenalty}(o_i)\n",
    "$$ \n",
    "\n",
    "where $\\text{LengthPenalty}(o_i)$ measures the  undesirable properties of response $o_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The penalty used in the paper is called *Soft Overlong Punishment*\n",
    "- two length thresholds\n",
    "- length penalty is phased in between the thresholds\n",
    "\n",
    "In addition\n",
    "- long responses are truncated to a maximum length\n",
    "- the truncated elements of the response are masked in the loss calculation\n",
    "\n",
    "This is referred to as *Overlong Filtering* in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Asymmetric Clipping: why ?\n",
    "\n",
    "\n",
    "Recall the *probability ratio*\n",
    "\n",
    "$$\n",
    "r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\n",
    "$$ \n",
    "\n",
    "expresses how much \n",
    "- the *new probability* $\\pi_\\theta(a_t|s_t)$ of an action (given a state)\n",
    "- can differ  from\n",
    "- the *old probability* $\\pi_{\\theta_{\\text{old}}}(a_t|s_t)$\n",
    "- in *proportional terms*\n",
    "\n",
    "This ratio is *clipped* in GRPO and DAPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "In the GRPO loss\n",
    "- the clipping range is \n",
    "$$\n",
    "[ 1-\\epsilon, 1+\\epsilon ]\n",
    "$$\n",
    "\n",
    "Typically:\n",
    "$$\n",
    "\\epsilon = .20\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In DAPO\n",
    "- the clipping range is adjusted to\n",
    "$$1-\\epsilon_{\\mathrm{low}}, 1+\\epsilon_{\\mathrm{high}}\n",
    "$$\n",
    "\n",
    "Typically\n",
    "- $\\epsilon_{\\mathrm{low}} = \\epsilon = .2$\n",
    "- $\\epsilon_{\\mathrm{high}} = .28$\n",
    "\n",
    "Thus, the probability in DAPO is allowed to increase\n",
    "- more (proportionally) than it is allowed to decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The reason for doing so is a simple consequence of \n",
    "- translating proportional increase to absolute increase\n",
    "\n",
    "To illustrate, suppose that , at step $\\tt$ of the trajectory, there are\n",
    "- an important action/token $a_{t, \\text{low}}$\n",
    "    - with *low* probability of being chosen by the policy\n",
    "- a less important action/token $a_{t, \\text{high}}$\n",
    "    - with *high* probability of being chosen by the policy\n",
    "\n",
    "$$\n",
    "\\pi_{\\theta_{\\text{old}}}(a_{t, \\text{low}} |s_t) \\lt \n",
    "\\pi_{\\theta_{\\text{old}}}(a_{t, \\text{high}} |s_t)\n",
    "$$\n",
    "\n",
    "**Note**\n",
    "\n",
    "*Important* is an informal term that refers to the ability to move the policy closer to the optimal one\n",
    "\n",
    "Here: \"low\" and \"high\" \n",
    "- refer to the policy *probability* and **not** the *importance*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to adjust the policy $\\pi$ to\n",
    "- increase $\\pi_{\\theta_{\\text{old}}}(a_{t, \\text{low}} |s_t)$\n",
    "- decrease $\\pi_{\\theta_{\\text{old}}}(a_{t, \\text{high}} |s_t)$\n",
    "\n",
    "But, because\n",
    "\n",
    "$$\n",
    "\\pi_{\\theta_{\\text{old}}}(a_{t, \\text{low}} |s_t) \\lt \n",
    "\\pi_{\\theta_{\\text{old}}}(a_{t, \\text{high}} |s_t)\n",
    "$$\n",
    "\n",
    "the **absolute increase** in probability \n",
    "- by multiplying by $( 1 + \\epsilon)$ in the clipping ratio\n",
    "- for the *low probability* but **more important** token $a_{t, \\text{low}}$\n",
    "\n",
    "is less than the absolute increase \n",
    "- for the *high probability* but **less important** token $a_{t, \\text{high}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So a proportional increase\n",
    "- to an initial low probability but important action\n",
    "\n",
    "has less effect on the Loss (optimization objective)\n",
    "- than a same proportion increase\n",
    "- to the initial high probability (but less important) action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Early in learning\n",
    "- when Exploration (vs Exploitation)\n",
    "-  *could* be high value in moving the policy in the optimal direction\n",
    "- there is a potential high benefit\n",
    "- from choosing the *most important* (but low action probability) action\n",
    "\n",
    "Raising the upper clipping bound has the effect of increasing the entropy of the policy\n",
    "- more exploration vs exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dynamic sampling: why ? \n",
    "\n",
    "If all samples in a group have the same trajectory reward\n",
    "- the Advantage of each sample in the group\n",
    "- is mathematically equal to $0$\n",
    "\n",
    "Groups with samples having identical reward are typically\n",
    "- all samples are correct\n",
    "- all samples are incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because the advatange of each sample is $0$, these groups\n",
    "- do not contribute to the Gradient of $J_{\\mathrm{DAPO}}(\\theta)$\n",
    "\n",
    "When computing the Gradient of a batch of $N$ questions\n",
    "- each with $G$ sample responses\n",
    "\n",
    "the groups with $0$ advantage reduce the effective batch size.\n",
    "\n",
    "This results in such batches having *noisier* gradient updates than unaffected batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover the learning signal\n",
    "- is stronger\n",
    "- when there is a *contrast* between samples\n",
    "    - high reward vs. low reward\n",
    "- whether within a group or within a batch\n",
    "\n",
    "Dynamic sampling promotes contrastive groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On the topic of *contrastive examples*\n",
    "- without the initial SFT of RFT\n",
    "- when the behavior to learn via RL is very different than the behavior of the base LLM\n",
    "- all examples and samples are likely to have the same low reward\n",
    "    - because of incorrect formatting or logic\n",
    "    \n",
    "So the initial SFT will hopefully create some high reward examples\n",
    "- by \"moving the distribution\" of RL training examples\n",
    "- closer to the ultimate RL goal\n",
    "- than the distribution of examples from the base (non-SFT tuned) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contribution of each technique to the improvement of DAPO vs GRPO\n",
    "\n",
    "| Technique              | Description                                                                                  | Commentary                                                                                  | Accuracy Improvement (AIME 2024 avg@32) |\n",
    "|:-----------------------|:---------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|:----------------------------------------|\n",
    "| Naive GRPO             | Baseline group relative policy optimization without enhancements                            | Starting point with relatively low accuracy                                                 | 30                                       |\n",
    "| Overlong Filtering     | Filters out truncated (overlong) samples from training loss                                | Reduces reward noise caused by forced truncation, stabilizes training                       | 36                                       |\n",
    "| Clip-Higher            | Decouples lower and upper clipping range to allow higher increase for low-probability tokens | Enhances policy entropy and exploration, avoids early collapse of exploration              | 38                                       |\n",
    "| Soft Overlong Punishment| Length-aware penalty on excessively long responses                                        | Prevents reward noise from overly penalizing valid but long reasoning chains                | 41                                       |\n",
    "| Token-Level Loss       | Aggregates loss over tokens normalized by total token count (not per sample average)       | Improves training stability and healthier growth in output length                           | 42                                       |\n",
    "| Dynamic Sampling       | Oversamples and filters batches to keep effective gradient signals by excluding zero-advantage samples | Significantly improves training stability and performance speed                            | 50                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pseudo code for DAPO\n",
    "\n",
    "**Detailed Surrogate Loss for DAPO**\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{DAPO}}(\\theta) =\n",
    "\\mathbb{E}_{(q,a), \\{o_i\\} \\sim \\pi_{\\theta_{\\text{old}}}} \\left[\n",
    "\\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{\\tt=1}^{|o_i|} \n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}(r_{i,\\tt}(\\theta), 1-\\epsilon_{\\mathrm{low}}, 1+\\epsilon_{\\mathrm{high}}) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    # Given:\n",
    "    # batch_size = N\n",
    "    # num_samples = K\n",
    "    # sequence_lengths = [T_gi for each response i in group g]\n",
    "    # importance_scores = array of same shape as tokens, default = 1\n",
    "\n",
    "    for group_id in range(N):\n",
    "        for sample_id in range(K):\n",
    "            T = sequence_lengths[group_id][sample_id]\n",
    "            importance_sum = 0\n",
    "            # Compute sum of importance scores in sample\n",
    "            for t in range(T):\n",
    "                importance_sum += importance_scores[group_id][sample_id][t]\n",
    "            # Calculate alpha for each token\n",
    "            for t in range(T):\n",
    "                alpha = importance_scores[group_id][sample_id][t] / importance_sum\n",
    "                # Compute token-wise policy gradient component:\n",
    "                grad = alpha * w[group_id][sample_id] \\\n",
    "                       * clip(r[group_id][sample_id][t], 1-eps_low, 1+eps_high) \\\n",
    "                       * advantage[group_id][sample_id][t]\n",
    "                # Accumulate grad, update model params, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References for GRPO to DAPO**\n",
    "\n",
    "- [DAPO: An Open-Source LLM Reinforcement Learning System at Scale (arXiv)](https://arxiv.org/abs/2503.14476)\n",
    "- [Mathematics of DAPO, PPO, and GRPO (SSRN)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5205449)\n",
    "- [From GRPO to DAPO and GSPO: What, Why, and How (Hugging Face blog)](https://huggingface.co/blog/NormalUhr/grpo-to-dapo-and-gspo)\n",
    "- [The Evolution of GRPO: DAPO (Towards AI)](https://towardsai.net/p/l/the-evolution-of-grpo-dapo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison of methods for Preference Data\n",
    "\n",
    "| Aspect              | PPO (Proximal Policy Optimization)                  | DPO (Direct Preference Optimization)                      | GRPO (Group Relative Policy Optimization)                    |\n",
    "|:---------------------|:------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------|\n",
    "| **Stability**       | Moderate stability, uses clipped objective to limit policy updates and prevent divergence. Can still be sensitive to reward noise and hyperparameters. | High stability due to supervised-learning style objective on preference pairs. Does not rely on policy gradient RL steps. | Higher stability than PPO due to normalized group rewards reducing gradient noise; does not require a value function critic which reduces instability. |\n",
    "| **Variance**        | High variance in gradient estimates caused by sparse rewards and stochastic policy sampling. Requires variance reduction techniques (e.g., baseline/critic). | Low variance because gradients come from direct supervised preference comparisons without sampling or policy gradients. | Moderate varianceâ€”variance is reduced by reward normalization within groups but still involves sampling multiple outputs, so more variance than DPO but less than PPO. |\n",
    "| **Sample Efficiency** | Moderate to lowâ€”needs many environment interactions/samples due to sparse reward signal and on-policy updates. Sampling multiple sequences per prompt increases cost. | Very highâ€”trains directly on labeled preference pairs with no complex sampling or reward modeling. | Higher than PPOâ€”requires multiple samples per prompt for group comparison but gains efficiency from relative advantage normalization and critic-free updates. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9 (new)",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
