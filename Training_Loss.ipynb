{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import training_models_helper as tmh\n",
    "%aimport training_models_helper\n",
    "\n",
    "tm = tmh.TrainingModelsHelper()\n",
    "\n",
    "import svm_helper\n",
    "%aimport svm_helper\n",
    "svmh = svm_helper.SVM_Helper()\n",
    "\n",
    "kn = tmh.KNN_Helper()\n",
    "\n",
    "import transform_helper\n",
    "%aimport transform_helper\n",
    "\n",
    "th = transform_helper.Transformation_Helper()\n",
    "\n",
    "iph = transform_helper.InfluentialPoints_Helper()\n",
    "\n",
    "import svm_helper\n",
    "%aimport svm_helper\n",
    "svmh = svm_helper.SVM_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Improving prediction: Understanding the Loss function\n",
    "\n",
    "In performing Error Analysis \n",
    "- we *identified* **test** examples where our model failed to generalize correctly\n",
    "- but we didn't propose a **solution** to improve generalization\n",
    "\n",
    "That is the topic of this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we perform Error Analysis\n",
    "- we focus on the Performance Metric (e.g., Accuracy)\n",
    "- on a per-example basis\n",
    "- for **out of sample** examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But we can't *directly* influence the Performance on an out of sample example.\n",
    "\n",
    "Instead, we will perform a *Loss Analysis*\n",
    "- we focus on the Loss\n",
    "- on a per-example basis\n",
    "- for **in sample** examples\n",
    "\n",
    "This is the analog of Error Analysis, but performed on Training rather than Test examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reminder\n",
    "- the Loss Function and Performance metrics are **not necessarily** the same\n",
    "- the Loss Function is computed **in-sample**\n",
    "- the Performance Metric is computed **out of sample**\n",
    "\n",
    "The hope is\n",
    "- that improving in-sample Loss\n",
    "- will *indirectly* lead to a better out of sample Performance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate, recall how Logistic Regression creates a prediction\n",
    "- the model computes a score/logit for an example\n",
    "$$\n",
    "\\hat{\\mathbf{p}}^\\ip = \\sigma( \\Theta^T \\cdot \\x^\\ip )\n",
    "$$\n",
    "- the score/logit is converted into a prediction by comparison with a threshold\n",
    "$$\n",
    "\\hat{\\y}^\\ip = \n",
    "\\left\\{\n",
    "    {\n",
    "    \\begin{array}{lll}\n",
    "     \\text{Negative} & \\textrm{if } \\hat{\\mathbf{p}}^\\ip   < 0.5  \\\\\n",
    "     \\text{Positive}& \\textrm{if } \\hat{\\mathbf{p}}^\\ip \\ge 0.5 \n",
    "    \\end{array}\n",
    "    }\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "The Loss is Binary Cross Entropy evaluated on the probabilities $\\hat{\\mathbf{p}}^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we observed [before](Classification_Loss_Function.ipynb#Classification:-Loss-function)\n",
    "- a small change in $\\hat{\\mathbf{p}}^\\ip$\n",
    "- does **not necessarily** change prediction $\\y^\\ip$\n",
    "- unless $\\hat{\\mathbf{p}}^\\ip$ crosses the threshold\n",
    "\n",
    "Thus\n",
    "- the Loss varies continuously with changes in parameters $\\Theta$\n",
    "- but the Performance **may not**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Loss Analysis\n",
    "- examines the per-example Loss using in-sample examples\n",
    "- in order to finding common attributes of problematic (mis-predicted) in-sample examples\n",
    "\n",
    "Once we *diagnose* the problem with Loss\n",
    "- we can explore remedies\n",
    "    - feature engineering\n",
    "    - pre-processing\n",
    "- with the goal of causing the optimizer to change $\\Theta$\n",
    "- in order to push $\\hat{\\mathbf{p}}^\\ip$ in the right direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the basics of minimizing Loss Functions\n",
    "- Predictions $h(\\x; \\Theta)$  are a function of both inputs and parameters $\\Theta$\n",
    "- A given $\\Theta$ induces a per-example loss $\\loss_\\Theta^\\ip$\n",
    "- Average Loss is the average of the per-examples losses $\\loss_\\Theta^\\ip, i=1, \\ldots, m$\n",
    "- We seek the optimal $\\Theta^*$: $$\n",
    "\\Theta^* = \\argmin{\\Theta} { \\loss_\\Theta }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In pictures:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L4_s55_Intro_training.png\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis.png\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional loss\n",
    "\n",
    "In Error Analysis we partition test examples into groups with some common property, such as\n",
    "- Commonality of result: TP, FN, TN, FP\n",
    "- Commonality of features\n",
    "in order to compute a *conditional* out of sample metric.\n",
    "\n",
    "In Loss Analysis we partition training examples into groups to\n",
    "in order to compute a *conditional* in sample metric.\n",
    "\n",
    "The following picture uses colors to identify which group a training example belongs to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Loss analysis: conditional loss</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis_1.png\"</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The real advantage of performing Conditional analysis in sample\n",
    "- In sample examples (training/validation) can be re-used, unlike Test examples\n",
    "- Added features based on in sample analysis is likely to affect the Loss\n",
    "    - Unknown whether it will affect Performance Metric (when it is different than Loss, e.g., Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What can we do to reduce loss ?\n",
    "\n",
    "Understanding the per example loss can help you \"push\" the optimizer toward find a \"better\" $\\Theta$.\n",
    "\n",
    "We will outline some simple strategies via examples that identify a probelm and propose a solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Increase number of \"problem\" training example\n",
    "\n",
    "For MNIST digit classification\n",
    "- We hypothesize a commonality that causes images of the digit 8 to be mis-classified\n",
    "    - 8's that are slanted in the \"opposite\" direction of normal\n",
    "    - We will refer to this as the *problematic* class\n",
    "\n",
    "One reason our classifier may fail on this sub-class of 8's\n",
    "- There are many fewer of them than the more prevalent images of 8's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mathematically, the Average Loss is equally weighted\n",
    "$$\n",
    "\\loss_\\Theta  = { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta\n",
    "$$\n",
    "\n",
    "but the cumulative weight of the problematic class (mis-shaped 8's) is very small.\n",
    "\n",
    "So even if all examples in the problematic class were mis-classified\n",
    "- The impact on Average Loss may be sufficiently small.\n",
    "- That $\\Theta$ doesn't get updated in the direction that will improve these examples\n",
    "    - Especially if we end optimization before absolute convergence occurs, as is common\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One strategy for pushing the model to better fit the problematic examples is\n",
    "- Increase their cumulative weight in the Loss\n",
    "- By increasing their number !\n",
    "\n",
    "The strategy known as *Data Augmentation* adds examples to the Training examples\n",
    "- Here we try to find/synthesize more instances of the problematic type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can augment examples by repeating them (as above).\n",
    "- re-sampling the Training data\n",
    "- covered in the module on Imbalanced Data\n",
    "\n",
    "This is a simple method that works well for most data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For some types of data (e.g., Image), other means of augmentation are available.\n",
    "- Create a new training example\n",
    "- By perturbing the features of an existing training example\n",
    "- In such a way as to preserve the label\n",
    "\n",
    "For instance, give a training example we can\n",
    "- Add a small quantity of noise to the feature vector\n",
    "- Perform data-type specific transformations\n",
    "    - Images: [shift, rotate, transpose](DataAugmentation/Data_augmentation.ipynb#Original-image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Influential points\n",
    "\n",
    "We have described the case where the issue is mis-classification of an important but small sub-class.\n",
    "- Which results in a small cumulative contribution to the Loss \n",
    "\n",
    "Sometimes the problem is a small sub-class with an *out-sized* contribution to the Loss\n",
    "- Having a few problem examples\n",
    "- Whose contribution to Loss is so large\n",
    "- That it pushes $\\Theta$ in the wrong direction for the more numerous non-problem examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is: $\\loss_\\Theta^\\ip$ is so large (for some example $i$) that \n",
    "- $\\Theta$ is changed to reduce $\\loss_\\Theta^\\ip$ \n",
    "- Resulting in an increase in $\\loss_\\Theta^{(i')}$ for each non-problematic example $i'$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The phenomenon we just described is sometimes called *Influential Points*.\n",
    "\n",
    "These have been particularly well-studied in the context of Linear Regression.\n",
    "\n",
    "We will use Linear Regression as an illustration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Loosely speaking, an example is **influential** if \n",
    "- the parameter estimate $\\Theta$ changes greatly depending on whether the example is included/excluded\n",
    "\n",
    "Feature values on the extreme ends of the range have greater potential\n",
    "for being influential.\n",
    "\n",
    "This is one argument for constraining the range of the feature (MinMax, Standardization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's an interactive tool to get a feel for influential points in Linear Regression.\n",
    "\n",
    "It allows you to change the value of a single data point and see the effect on the fitted line.\n",
    "\n",
    "Observe how the slope changes (displayed in the title)\n",
    "- 10 labeled examples $\\{ [\\x^\\ip, \\y^\\ip] \\, | \\, 0 \\le i \\lt 10  \\}$\n",
    "- The top slider chooses the index $i \\in  \\{ 0 \\ldots 9 \\}$ of one data point to change\n",
    "- The bottom slider is the new value $\\y^\\ip$ for the point at the chosen index $i$\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import FloatSlider, Button, HBox, VBox, Output\n",
    "\n",
    "# Generate base data: many points along a line\n",
    "np.random.seed(42)\n",
    "n_points = 15\n",
    "x = np.linspace(-3, 3, n_points)\n",
    "y_orig = 2 + 1.5 * x + np.random.normal(0, 0.3, size=n_points)\n",
    "\n",
    "# Indices for the three controllable points\n",
    "idx_left = 0\n",
    "idx_mid = n_points // 2\n",
    "idx_right = -1\n",
    "\n",
    "# Output widget for plot\n",
    "out = Output()\n",
    "\n",
    "# Sliders for the three points\n",
    "slider_left = FloatSlider(\n",
    "    value=y_orig[idx_left],\n",
    "    min=y_orig[idx_left] - 8,\n",
    "    max=y_orig[idx_left] + 8,\n",
    "    step=0.1,\n",
    "    description='Left y'\n",
    ")\n",
    "slider_mid = FloatSlider(\n",
    "    value=y_orig[idx_mid],\n",
    "    min=y_orig[idx_mid] - 8,\n",
    "    max=y_orig[idx_mid] + 8,\n",
    "    step=0.1,\n",
    "    description='Mid y'\n",
    ")\n",
    "slider_right = FloatSlider(\n",
    "    value=y_orig[idx_right],\n",
    "    min=y_orig[idx_right] - 8,\n",
    "    max=y_orig[idx_right] + 8,\n",
    "    step=0.1,\n",
    "    description='Right y'\n",
    ")\n",
    "\n",
    "# Reset button\n",
    "reset_btn = Button(description=\"Reset\", button_style='warning')\n",
    "\n",
    "def plot_regression(left_y, mid_y, right_y):\n",
    "    y_mod = y_orig.copy()\n",
    "    y_mod[idx_left] = left_y\n",
    "    y_mod[idx_mid] = mid_y\n",
    "    y_mod[idx_right] = right_y\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1, 1), y_mod)\n",
    "    y_pred = model.predict(x.reshape(-1, 1))\n",
    "    r2 = model.score(x.reshape(-1, 1), y_mod)\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # Plot fixed points\n",
    "    mask = np.ones(n_points, dtype=bool)\n",
    "    mask[[idx_left, idx_mid, idx_right]] = False\n",
    "    plt.scatter(x[mask], y_mod[mask], color='gray', label='Fixed points')\n",
    "    # Plot controllable points\n",
    "    plt.scatter(x[idx_left], y_mod[idx_left], color='red', s=120, label='Left')\n",
    "    plt.scatter(x[idx_mid], y_mod[idx_mid], color='green', s=120, label='Mid')\n",
    "    plt.scatter(x[idx_right], y_mod[idx_right], color='blue', s=120, label='Right')\n",
    "    plt.plot(x, y_pred, color='black', lw=2, label='OLS fit')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Effect of Influential Points on Linear Regression')\n",
    "    plt.legend()\n",
    "    eqn = f\"$y = {intercept:.2f} + {slope:.2f}x$\\n$R^2 = {r2:.3f}$\"\n",
    "    plt.text(0.05, 0.95, eqn, transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "    plt.ylim(min(y_mod)-2, max(y_mod)+2)\n",
    "    plt.show()\n",
    "\n",
    "def update_plot(*args):\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        plot_regression(slider_left.value, slider_mid.value, slider_right.value)\n",
    "\n",
    "def reset_sliders(b):\n",
    "    slider_left.value = y_orig[idx_left]\n",
    "    slider_mid.value = y_orig[idx_mid]\n",
    "    slider_right.value = y_orig[idx_right]\n",
    "\n",
    "# Attach callbacks\n",
    "slider_left.observe(update_plot, names='value')\n",
    "slider_mid.observe(update_plot, names='value')\n",
    "slider_right.observe(update_plot, names='value')\n",
    "reset_btn.on_click(reset_sliders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4df347359ce4813a476125ac06f5612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=-2.35098575409663, description='Left y', max=5.64901424590337,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial plot\n",
    "update_plot()\n",
    "\n",
    "# Display controls and plot\n",
    "ui = VBox([HBox([slider_left, slider_mid, slider_right, reset_btn]), out])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Play around with the tool\n",
    "- Move a point close to either end of the range\n",
    "- Move a point close to the middle of the range\n",
    "\n",
    "Observe how the slope changes as you move the point.\n",
    "\n",
    "You will see that points closer to either end have greater influence on the slope.\n",
    "anging $\\y^\\ip$ for $i$ near either end ($0$ or $9$) has a large effect on the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The solution is to somehow reduce example $i$'s contribution to Average Loss\n",
    "- Removing the example: possible data error or outlier\n",
    "- Down-weighting\n",
    "- Clipping the values of the features/target to some upper bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Further background \n",
    "\n",
    "Consider feature $j$.\n",
    "\n",
    "The **leverage** of example $i$ is related to\n",
    "- How far $\\x_j^\\ip$ is from $\\bar{\\x}$, the average of $\\x_j$ across all examples\n",
    "\n",
    "It is not always the case, but high leverage sometimes makes the point influential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reference:\n",
    "[Influence from leverage and distance](http://onlinestatbook.com/2/regression/influential.html)\n",
    ">An observation's influence is a function of two factors: (1) how much the observation's value on the predictor variable differs from the mean of the predictor variable and (2) the difference between the predicted score for the observation and its actual score. The former factor is called the observation's leverage. The latter factor is called the observation's distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Calculation of Leverage (h) of example $i$, feature $j$\n",
    "\n",
    "[formula](https://learnche.org/pid/least-squares-modelling/outliers-discrepancy-leverage-and-influence-of-the-observations#leverage)\n",
    "\n",
    "$$ \n",
    "\\begin{array}{lll}\n",
    "h^\\ip_j & = & { 1 \\over n }+ \\frac{ (\\x^\\ip_j - \\bar{\\x_j})^2}{ \\sum_i { (\\x^\\ip_j - \\bar{\\x_j})^2} } \\\\\n",
    "    & = & \\frac{ 1 + \\left( \\frac{\\x^\\ip_j - \\bar{\\x_j}}{\\sigma_{\\x_j} } \\right) ^2}{n}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "You can see that the leverage of $\\x^\\ip_j$ depends on the (standardized) distance of $\\x^\\ip_j$ from the mean (over all $i$) of $\\x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9 (new)",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
