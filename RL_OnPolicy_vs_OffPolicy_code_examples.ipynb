{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Python file contains three key reinforcement learning examples highlighting on-policy vs off-policy learning and experience replay usage:\n",
      "1) Q-learning (Off-policy):\n",
      "   - Behavior policy is epsilon-greedy (explores)\n",
      "   - Update uses greedy max Q (target policy)\n",
      "   - Shows the separation of behavior and target policy in Q-learning\n",
      "2) SARSA (On-policy):\n",
      "   - Behavior and target policy both epsilon-greedy\n",
      "   - Update uses Q-value of the next action taken (following behavior policy)\n",
      "   - Demonstrates on-policy learning where sample and update policy are the same\n",
      "3) Q-learning with Experience Replay Buffer:\n",
      "   - Stores exploratory transitions in a replay buffer\n",
      "   - Samples batches randomly for training updates\n",
      "   - Illustrates off-policy training with replay and improved sample efficiency\n",
      "Run each example independently (uncomment the corresponding block) to observe behavior and Q-table evolution.\n"
     ]
    }
   ],
   "source": [
    "# Reinforcement Learning Code Examples\n",
    "\n",
    "\n",
    "print('This Python file contains three key reinforcement learning examples highlighting on-policy vs off-policy learning and experience replay usage:')\n",
    "\n",
    "print('\\\n",
    "1) Q-learning (Off-policy):')\n",
    "print('   - Behavior policy is epsilon-greedy (explores)')\n",
    "print('   - Update uses greedy max Q (target policy)')\n",
    "print('   - Shows the separation of behavior and target policy in Q-learning')\n",
    "\n",
    "print('\\\n",
    "2) SARSA (On-policy):')\n",
    "print('   - Behavior and target policy both epsilon-greedy')\n",
    "print('   - Update uses Q-value of the next action taken (following behavior policy)')\n",
    "print('   - Demonstrates on-policy learning where sample and update policy are the same')\n",
    "\n",
    "print('\\\n",
    "3) Q-learning with Experience Replay Buffer:')\n",
    "print('   - Stores exploratory transitions in a replay buffer')\n",
    "print('   - Samples batches randomly for training updates')\n",
    "print('   - Illustrates off-policy training with replay and improved sample efficiency')\n",
    "\n",
    "print('\\\n",
    "Run each example independently (uncomment the corresponding block) to observe behavior and Q-table evolution.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      " Behavior policy action (epsilon-greedy): 0\n",
      " Next state: 1, Reward: 0\n",
      " Updated Q[0,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 2:\n",
      " Behavior policy action (epsilon-greedy): 0\n",
      " Next state: 0, Reward: 0\n",
      " Updated Q[1,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 3:\n",
      " Behavior policy action (epsilon-greedy): 0\n",
      " Next state: 1, Reward: 0\n",
      " Updated Q[0,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 4:\n",
      " Behavior policy action (epsilon-greedy): 0\n",
      " Next state: 0, Reward: 0\n",
      " Updated Q[1,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 5:\n",
      " Behavior policy action (epsilon-greedy): 0\n",
      " Next state: 1, Reward: 0\n",
      " Updated Q[0,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 6:\n",
      " Behavior policy action (epsilon-greedy): 0\n",
      " Next state: 0, Reward: 0\n",
      " Updated Q[1,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 7:\n",
      " Behavior policy action (epsilon-greedy): 0\n",
      " Next state: 1, Reward: 0\n",
      " Updated Q[0,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 8:\n",
      " Behavior policy action (epsilon-greedy): 0\n",
      " Next state: 0, Reward: 0\n",
      " Updated Q[1,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 9:\n",
      " Behavior policy action (epsilon-greedy): 0\n",
      " Next state: 1, Reward: 0\n",
      " Updated Q[0,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 10:\n",
      " Behavior policy action (epsilon-greedy): 1\n",
      " Next state: 0, Reward: 1\n",
      " Updated Q[1,1]: 0.100\n",
      " Current Q table:\n",
      "[[0.  0. ]\n",
      " [0.  0.1]]\n",
      "Final Q-table:\n",
      "[[0.  0. ]\n",
      " [0.  0.1]]\n",
      "Behavior policy actions are epsilon-greedy while Q updates bootstrapped from greedy target policy (max Q).\n"
     ]
    }
   ],
   "source": [
    "# === Example: Q Learning Simple ===\n",
    "# Simple Q-learning example demonstrating the off-policy nature: behavior policy is epsilon-greedy, update uses greedy max Q.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Simple 2-state, 2-action environment setup\n",
    "states = [0, 1]\n",
    "actions = [0, 1]\n",
    "\n",
    "# Initialize Q-table: Q[state, action]\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1    # Learning rate\n",
    "gamma = 0.9    # Discount factor\n",
    "epsilon = 0.3  # Exploration rate for behavior policy\n",
    "\n",
    "# Behavior policy: Epsilon-greedy\n",
    "def epsilon_greedy_policy(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explore: choose random action\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        # Exploit: choose greedy action\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "# Target policy: Greedy\n",
    "def greedy_policy(state):\n",
    "    return np.argmax(Q[state])\n",
    "\n",
    "# Environment step: Deterministic transition and reward for simplicity\n",
    "def env_step(state, action):\n",
    "    next_state = 1 - state       # Flip between states 0 and 1\n",
    "    reward = 1 if action == 1 else 0  # Reward only for action 1\n",
    "    return next_state, reward\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "state = 0\n",
    "\n",
    "for step in range(10):\n",
    "    # Behavior policy selects an action (epsilon-greedy)\n",
    "    action = epsilon_greedy_policy(state)\n",
    "    next_state, reward = env_step(state, action)\n",
    "\n",
    "    # Q-learning update uses target policy (greedy) for bootstrapping:\n",
    "    target = reward + gamma * np.max(Q[next_state])\n",
    "    Q[state, action] += alpha * (target - Q[state, action])\n",
    "\n",
    "    print(f\"Step {step + 1}:\")\n",
    "    print(f\" Behavior policy action (epsilon-greedy): {action}\")\n",
    "    print(f\" Next state: {next_state}, Reward: {reward}\")\n",
    "    print(f\" Updated Q[{state},{action}]: {Q[state, action]:.3f}\")\n",
    "    print(f\" Current Q table:\\n{Q}\")\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "print(\"Final Q-table:\")\n",
    "print(Q)\n",
    "print(\"\\\n",
    "Behavior policy actions are epsilon-greedy while Q updates bootstrapped from greedy target policy (max Q).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      " Behavior (and target) policy action (epsilon-greedy): 0\n",
      " Next state: 1, Reward: 0\n",
      " Next action chosen: 0\n",
      " Updated Q[0,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 2:\n",
      " Behavior (and target) policy action (epsilon-greedy): 0\n",
      " Next state: 0, Reward: 0\n",
      " Next action chosen: 0\n",
      " Updated Q[1,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 3:\n",
      " Behavior (and target) policy action (epsilon-greedy): 0\n",
      " Next state: 1, Reward: 0\n",
      " Next action chosen: 0\n",
      " Updated Q[0,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 4:\n",
      " Behavior (and target) policy action (epsilon-greedy): 0\n",
      " Next state: 0, Reward: 0\n",
      " Next action chosen: 0\n",
      " Updated Q[1,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 5:\n",
      " Behavior (and target) policy action (epsilon-greedy): 0\n",
      " Next state: 1, Reward: 0\n",
      " Next action chosen: 0\n",
      " Updated Q[0,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 6:\n",
      " Behavior (and target) policy action (epsilon-greedy): 0\n",
      " Next state: 0, Reward: 0\n",
      " Next action chosen: 0\n",
      " Updated Q[1,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 7:\n",
      " Behavior (and target) policy action (epsilon-greedy): 0\n",
      " Next state: 1, Reward: 0\n",
      " Next action chosen: 0\n",
      " Updated Q[0,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 8:\n",
      " Behavior (and target) policy action (epsilon-greedy): 0\n",
      " Next state: 0, Reward: 0\n",
      " Next action chosen: 0\n",
      " Updated Q[1,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 9:\n",
      " Behavior (and target) policy action (epsilon-greedy): 0\n",
      " Next state: 1, Reward: 0\n",
      " Next action chosen: 1\n",
      " Updated Q[0,0]: 0.000\n",
      " Current Q table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 10:\n",
      " Behavior (and target) policy action (epsilon-greedy): 1\n",
      " Next state: 0, Reward: 1\n",
      " Next action chosen: 0\n",
      " Updated Q[1,1]: 0.100\n",
      " Current Q table:\n",
      "[[0.  0. ]\n",
      " [0.  0.1]]\n",
      "Final Q-table:\n",
      "[[0.  0. ]\n",
      " [0.  0.1]]\n",
      "In SARSA, the behavior policy and target policy are the same (epsilon-greedy), demonstrating on-policy learning.\n"
     ]
    }
   ],
   "source": [
    "# === Example: Sarsa Simple ===\n",
    "# Simple SARSA example demonstrating on-policy nature: behavior and target policy are the same (epsilon-greedy).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example environment: 2 states, 2 actions\n",
    "states = [0, 1]\n",
    "actions = [0, 1]\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1    # learning rate\n",
    "gamma = 0.9    # discount factor\n",
    "epsilon = 0.3  # exploration rate\n",
    "\n",
    "def epsilon_greedy_policy(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)  # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[state])        # Exploit\n",
    "\n",
    "# Simple environment step: switches states, reward for action 1\n",
    "def env_step(state, action):\n",
    "    next_state = 1 - state\n",
    "    reward = 1 if action == 1 else 0\n",
    "    return next_state, reward\n",
    "\n",
    "np.random.seed(42)\n",
    "state = 0\n",
    "action = epsilon_greedy_policy(state)\n",
    "\n",
    "for step in range(10):\n",
    "    next_state, reward = env_step(state, action)\n",
    "    next_action = epsilon_greedy_policy(next_state)\n",
    "\n",
    "    # SARSA update uses Q-value of next action actually taken (on-policy):\n",
    "    target = reward + gamma * Q[next_state, next_action]\n",
    "    Q[state, action] += alpha * (target - Q[state, action])\n",
    "\n",
    "    print(f\"Step {step + 1}:\")\n",
    "    print(f\" Behavior (and target) policy action (epsilon-greedy): {action}\")\n",
    "    print(f\" Next state: {next_state}, Reward: {reward}\")\n",
    "    print(f\" Next action chosen: {next_action}\")\n",
    "    print(f\" Updated Q[{state},{action}]: {Q[state, action]:.3f}\")\n",
    "    print(f\" Current Q table:\\n{Q}\")\n",
    "\n",
    "    state = next_state\n",
    "    action = next_action\n",
    "\n",
    "print(\"Final Q-table:\")\n",
    "print(Q)\n",
    "print(\"\\\n",
    "In SARSA, the behavior policy and target policy are the same (epsilon-greedy), demonstrating on-policy learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5, Q-table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 10, Q-table:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Step 15, Q-table:\n",
      "[[0.    0.1  ]\n",
      " [0.009 0.109]]\n",
      "Step 20, Q-table:\n",
      "[[0.06965733 0.289639  ]\n",
      " [0.15582401 0.22416751]]\n",
      "Step 25, Q-table:\n",
      "[[0.24651764 0.56210171]\n",
      " [0.29521963 0.55165219]]\n",
      "Step 30, Q-table:\n",
      "[[0.50691331 0.97076309]\n",
      " [0.40317376 1.01129603]]\n",
      "Final Q-table after training with replay buffer:\n",
      "[[0.50691331 0.97076309]\n",
      " [0.40317376 1.01129603]]\n"
     ]
    }
   ],
   "source": [
    "# === Example: Q Learning With Replay ===\n",
    "# Q-learning example with experience replay buffer showing how stored exploratory experience supports off-policy updates and improves sample efficiency.\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 2 states, 2 actions\n",
    "states = [0, 1]\n",
    "actions = [0, 1]\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1    # learning rate\n",
    "gamma = 0.9    # discount factor\n",
    "epsilon = 0.3  # exploration rate\n",
    "\n",
    "# Replay buffer settings\n",
    "replay_buffer = []\n",
    "buffer_capacity = 100\n",
    "batch_size = 4\n",
    "\n",
    "def epsilon_greedy_policy(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)  # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[state])        # Exploit\n",
    "\n",
    "def env_step(state, action):\n",
    "    next_state = 1 - state\n",
    "    reward = 1 if action == 1 else 0\n",
    "    return next_state, reward\n",
    "\n",
    "np.random.seed(42)\n",
    "state = 0\n",
    "\n",
    "def replay_update():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return  # Not enough samples yet\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    for (s, a, r, s_next) in batch:\n",
    "        target = r + gamma * np.max(Q[s_next])\n",
    "        Q[s, a] += alpha * (target - Q[s, a])\n",
    "\n",
    "for step in range(30):\n",
    "    # 1. Use behavior policy (epsilon-greedy) for action selection\n",
    "    action = epsilon_greedy_policy(state)\n",
    "    next_state, reward = env_step(state, action)\n",
    "\n",
    "    # 2. Store transition in replay buffer\n",
    "    if len(replay_buffer) == buffer_capacity:\n",
    "        replay_buffer.pop(0)  # Remove oldest if buffer full\n",
    "    replay_buffer.append((state, action, reward, next_state))\n",
    "\n",
    "    # 3. Sample a batch from replay buffer to update Q\n",
    "    replay_update()\n",
    "\n",
    "    if (step + 1) % 5 == 0:\n",
    "        print(f\"Step {step + 1}, Q-table:\\n{Q}\")\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "print(\"Final Q-table after training with replay buffer:\")\n",
    "print(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
