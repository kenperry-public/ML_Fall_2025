{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy-based methods/Policy gradient methods\n",
    "\n",
    "Value-based methods\n",
    "- assign a value to a state (value function) OR action given a state (action value function)\n",
    "- policy is *derived* from these values\n",
    "    - chose the action leading to the greatest return\n",
    "    - $\\argmax{\\act}$ to implement the policy\n",
    "\n",
    "Policy-based methods\n",
    "- by contrast, construct the policy $\\pi$ directly\n",
    "- as a parameterized (by parameters $\\Theta$) function\n",
    "\n",
    "    $$\n",
    "    \\pi_\\theta( \\actseq | \\stateseq ) = \\prc{\\actseq_\\tt = \\act}{\\stateseq_\\tt=\\stateseq}\n",
    "    $$\n",
    "    \n",
    "i.e., the policy is a probability distribution of actions $\\act$ , conditional on the state $\\state$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a brief comparison of Value-based and Policy-based methods.\n",
    "\n",
    "| Aspect                     | Value-Based Methods                         | Policy-Based Methods                           |\n",
    "|:-------------------------- |:------------------------------------------- |:---------------------------------------------- |\n",
    "| Output                     | State/action value functions                | Directly parameterized policy                  |\n",
    "| Policy Representation      | Implicit (via greedy/exploratory actions)   | Explicit (probability/distribution mapping)    |\n",
    "| Learning Objective         | Value prediction loss minimization          | Expected return maximization (gradient ascent) |\n",
    "| Typical Example Algorithms | DQN, Q-learning, SARSA                      | REINFORCE, PPO, vanilla policy gradient        |\n",
    "| Action Space               | Discrete (practical)                        | Handles continuous and discrete                |\n",
    "| Stochastic Policies        | Limited                                     | Natural/efficient                              |\n",
    "| Exploration Strategies     | Decoupled from policy (e.g. epsilon-greedy) | Inherent (stochastic policy outputs)           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Policy-based methods are *necessary* it those cases in which Value-based methods are not possible:\n",
    "- Continuous (versus discrete) action\n",
    "    - the  $\\argmax{\\act}$ that implements policy in Value based methods is not possible\n",
    "- Stochastic policy necessary\n",
    "    - games against an adversary: when an adversary can take advantage of Agent predictability\n",
    "    \n",
    "Policy-based methods are *desirable/preferable* when Value-based methods are impractical\n",
    "- Large number of possible actions\n",
    "- High dimensional state spaces\n",
    "    - state is characterized by a (long) vector of characteristics\n",
    "\n",
    "In both these cases:\n",
    "- tables are impractical representations of Value function or Action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Scenario                               | Policy-Based Required | Policy-Based Desirable |\n",
    "|:-------------------------------------- |:--------------------- |:---------------------- |\n",
    "| Continuous action spaces               | Yes                   | Yes                    |\n",
    "| Stochastic strategies needed           | Yes                   | Yes                    |\n",
    "| Aliased or partially observable states | Yes                   | Yes                    |\n",
    "| High-dimensional spaces                | Sometimes             | Yes                    |\n",
    "| Discrete/simple environments           | No                    | Sometimes              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Policy Gradient methods\n",
    "\n",
    "The predominant class of Policy based methods are those based on the Policy Gradient method.\n",
    "\n",
    "Policy Gradient methods \n",
    "create a sequence of improving policies\n",
    "$$\n",
    "\\pi_0, \\ldots, \\pi_p, \\ldots\n",
    "$$\n",
    "by creating a sequence of improved parameter estimates\n",
    "$$\n",
    "\\theta_0, \\ldots, \\theta_p, \\ldots\n",
    "$$\n",
    "using Gradient Ascent on some objective function $J(\\theta)$ to improve $\\theta_p$\n",
    "$$\n",
    "\\theta_{p+1} = \\theta_p + \\alpha * \\nabla_\\theta J(\\theta_p)\n",
    "$$\n",
    "- gradient of a Performance Measure $J(\\theta)$\n",
    "- with respect to parameters $\\Theta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There a a few Policy base methods that *don't* use Policy Gradient\n",
    "- in the module on Value based methods, we learned about Policy Iteration\n",
    "- Policy iteration alternates\n",
    "    - Policy Evaluation: improving the estimate of a Value function\n",
    "    - Policy Improvement: improving the policy\n",
    "        - use  $\\argmax{\\act}$ to implement the current policy \n",
    "\n",
    "Thus, Policy Iteration is both Value based and Policy based\n",
    "- but does not evolve policy via gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since we are trying to maximize objective function $J(\\theta)$ rather than minimize a loss objective\n",
    "- we use Gradient Ascent rather than Gradient Descent\n",
    "- hence we add the gradient rather than subtract it, in the update\n",
    "\n",
    "[RL Book Chapt 12](http://incompleteideas.net/book/RLbook2020.pdf#page=343)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic policy and environment\n",
    "\n",
    "With Value based methods\n",
    "- the Environment can be stochastic\n",
    "- but the Policy must be deterministic\n",
    "    -  $\\argmax{\\act}$ to implement the policy\n",
    "    \n",
    "With Policy Gradient methods\n",
    "- the policy can be stochastic (action is a probability distribution)\n",
    "    $$\n",
    "    \\pi( \\act | \\state; \\theta ) = \\pr{ \\actseq_\\tt = \\act | \\stateseq_\\tt = \\state, \\theta_\\tt = \\theta }\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The environment can *also* be stochastic\n",
    "$$\n",
    "\\transp({ \\state', \\rew | \\state, \\act }) \n",
    " = \\transp({\\stateseq_\\tt = \\state', \\rewseq_\\tt = \\rew | \\stateseq_{\\tt-1} = \\state, \\actseq_{\\tt-1} = \\act })\n",
    "$$\n",
    "- the response $(\\state', \\rew)$ by the environment is not deterministic\n",
    "\n",
    "This poses a challenge to Value-based methods\n",
    "- a single observation of $(\\state', \\rew)$ is a *high variance* estimate of $\\transp({ \\state', \\rew | \\state, \\act }) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objective function\n",
    "\n",
    "Recall that the return $G_\\tt$ of a single episode is the expected value of rewards accumulated starting in the state of step $\\tt$ of the episode\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "G_\\tt & = & \\sum_{k=0}^\\tt {  \\gamma^k * \\rewseq_{\\tt+k+1} } \\\\\n",
    "      & = & \\rew_{\\tt+1}  + \\gamma * G_{\\tt+1} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "The performance measure $J(\\theta)$ that we define will be the\n",
    "*expected value* (across all possible episodes) of the return $G_0$ from initial state $\\stateseq_0$ of the episode\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\Exp{\\tau} ( G_{0, \\tau} )\n",
    "$$\n",
    "- using the notation\n",
    "$$\n",
    "G_{\\tt, \\tau}\n",
    "$$\n",
    "to denote the return within episode $\\tau$ of step $\\tt$ of the episode.\n",
    "\n",
    "Note that $G_{\\tt, \\tau}$ is equivalent to $\\statevalfun_\\pi(\\stateseq_\\tt)$ (relative to episode $\\tau$)\n",
    "- the value function evaluated on the initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taking the gradient of the Objective\n",
    "\n",
    "This objective function presents some challenges in computing $\\nabla_\\theta J(\\theta_p)$\n",
    "\n",
    "The first is: how to take gradient of an expectation ?\n",
    "\n",
    "We can do away with the expectation by replacing it with the sum\n",
    "\n",
    "$$\n",
    "\\Exp{\\tau} ( G_{\\tt, \\tau} ) = \\sum_\\tau {  \\pr{\\tau ; \\theta} * G_{\\tt, \\tau} }\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\pr{\\tau ; \\theta}\n",
    "$$\n",
    "is the probability of episode $\\tau$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In practical terms\n",
    "- we don't sum over every possible episode\n",
    "- we can approximate the Expectation through *trajectory sampling*\n",
    "    - accumulate a batch of episodes\n",
    "    - approximate the expectation as the average across the episodes in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the gradient of a sum is equal to the sum of the gradients\n",
    "- so being able to compute the gradient of $J(\\theta)$ depends on being able to compute the terms in the sum.\n",
    "\n",
    "But this too presents a challenge\n",
    "\n",
    "The probability of episode $\\tau$ occurring is thus the product of each step occurring\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\pr{\\tau; \\theta} & = & \\prod_{t=0} { \n",
    "    \\transp({ \\state', \\rew | \\state = \\stateseq_\\tt, \\act = \\actseq_\\tt })\n",
    "    * \\pi( \\actseq_\\tt | \\stateseq_\\tt ) \n",
    "     } \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem is the Transition Probability term in the product\n",
    "$$\n",
    " \\transp({ \\state', \\rew | \\state = \\stateseq_\\tt, \\act = \\actseq_\\tt })\n",
    "$$\n",
    "- the reaction of the Environment to the agent choosing action $\\actseq_\\tt$ in state $\\stateseq_\\tt$\n",
    "- is controlled by the environment\n",
    "- generally: unknown\n",
    "- *Model free* method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Gradient Theorem\n",
    "\n",
    "Our objective is to maximize the *expected return*\n",
    "- where the expectation is over *all* possible trajectories $\\tau$ generated by $\\theta$:\n",
    "$$\n",
    "\\tau \\sim \\pr{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\Exp{\\tau \\sim \\pr{\\theta} } \\,\\, { G_{\\tau,0} } = \\Exp{\\tau \\sim \\pr{\\theta} } \\,\\,{ \\rewseq(\\tau) }\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\rewseq(\\tau) = G_{\\tau,0}$\n",
    "\n",
    "is the return from the initial state $0$ of a trajectory $\\tau$\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- We assume the discount factor $\\gamma = 1$ only to simplify notation\n",
    "- To clarify that states, actions, rewards, returns, etc. depend on the specific trajectory $\\tau$\n",
    "    - we add an extra subscript when necessary for clarification\n",
    "$$\n",
    "\\rewseq_{\\tau,\\tt}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using Gradient Ascent to solve the maximization, we need to compute\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "The *Policy Gradient Theorem* shows us how to compute this gradient.\n",
    "\n",
    "We can state it in two forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the first formulation\n",
    "- the reward is stated as a reward $\\rewseq(\\tau)$ for the entire *trajectory* $\\tau$\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \n",
    "\\Exp{\\tau \\sim \\pr{\\theta}} { \n",
    "\\sum_{\\tt=0}^{|\\tau|} {\n",
    "\\nabla_\\theta   \\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) ) \\, \\,\\rewseq(\\tau)\n",
    "}\n",
    "} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each step $\\tt$ receives the *trajectory reward* $\\rewseq(\\tau)$.\n",
    "\n",
    "This works out well\n",
    "- in the case where there is a single reward *received at the end* of the trajectory\n",
    "    - all steps assigns equal \"credit\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The second formulation credits a reward for each step of trajectory $\\tau$\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{\\tt=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,t}) G_{\\tau,\\tt} \\right]\n",
    "$$\n",
    "\n",
    "where $G_{\\tau,\\tt}$ is the return-to-go of the trajectory $\\tau$ from step $\\tt$ (state $\\stateseq_{\\tau, \\tt}$).\n",
    "\n",
    "In the second formulation (which is algebraically equivalent to the first)\n",
    "step $\\tt$ receives the *return to go* $G_{\\tau,\\tt}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This works out well \n",
    "- when there are intermediate rewards\n",
    "    - each step is rewarded in proportion to the future return attributed to the step's action\n",
    "    - reduces variance of the gradient estimate by assigning precise credit\n",
    "    \n",
    "**Note**\n",
    "\n",
    "You may sometimes see \n",
    "$$\n",
    "G_{\\tau,\\tt}\n",
    "$$\n",
    "written as a Q-function\n",
    "$$\n",
    "Q(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Whichever way we write it\n",
    "- the Policy Gradient Theorem is the foundation for all policy-based methods\n",
    "- it tells us how to change the parameters $\\theta$ of the Policy NN in a direction leading to optimality\n",
    "\n",
    "We will study a few of these methods in a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing $\\nabla_\\theta J(\\theta)$\n",
    "\n",
    "Our first step will be to turn the expectation \n",
    "$$\n",
    "\\Exp{\\tau \\sim \\pr{\\theta}} \\,\\, { \\rewseq({\\tau}) }\n",
    "$$\n",
    "into a sum\n",
    "$$\n",
    "\\sum_{\\tau \\sim \\pr{\\theta}} \\pr{\\tau} *  { \\rewseq({\\tau}) }\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\pr{\\tau}\n",
    "$$\n",
    "is the probability of trajectory $\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With stochastic policy and Environment and trajectory $\\tau$\n",
    "$$\n",
    "\\tau = \\stateseq_{\\tau,0}, \\actseq_{\\tau,0}, \\rewseq_{\\tau,1}, \\stateseq_{\\tau,1} \\dots \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt}, \\rewseq_{\\tau, \\tt+1}, \\stateseq_{\\tau, \\tt+1}, \\ldots\n",
    "$$\n",
    "we can compute $\\pr{\\tau}$\n",
    "$$\n",
    "\\pr{\\tau} = \n",
    "\\pr{\\stateseq_{\\tau,0}}\n",
    "*\n",
    "\\prod_{\\tt=0}^{|\\tau|} { \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) * \\transp(\\stateseq_{\\tau, \\tt+1}, \\rewseq_{\\tau,\\tt+1} | \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt})} \n",
    "$$\n",
    "- as the chained (multiplicative) probability of each step $\\tt$ \n",
    "- where the probability of each step $\\tt$ is a product of\n",
    "    - the probability \n",
    "    $$\\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt})$$ \n",
    "    that the agent choses $\\actseq_{\\tau,\\tt}$ as the action\n",
    "    - the probability $$\\transp(\\stateseq_{\\tau, \\tt+1}, \\rewseq_{\\tau,\\tt+1} | \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt})$$\n",
    "    responds by changing the state to $\\stateseq_{\\tau, \\tt+1}$ and giving reward $\\rewseq_{\\tau, \\tt+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the gradient\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\theta\n",
    "\\sum_{\\tau \\sim \\pr{\\theta}} \\pr{\\tau} *  { \\rewseq({\\tau}) } \n",
    "& = & \\sum_{\\tau \\sim \\pr{\\theta}}{ \\nabla_\\theta  \\, \\,\\pr{\\tau} *  { \\rewseq({\\tau}) }}  & \\text{grad of a sum is sum of the grads} \\\\\n",
    "& = & \\sum_{\\tau \\sim \\pr{\\theta}}{ (\\pr{\\tau} \\nabla_\\theta  \\log  \\pr{\\tau}) \\, \\,\\rewseq(\\tau) } & \\text{Likelihood ratio trick:} \\\\\n",
    "& & & \\nabla_\\theta \\pr{\\tau} = \\pr{\\tau} * \\log \\pr{\\tau} \\\\\n",
    "& = & \\Exp{\\tau \\sim \\pr{\\theta}} { \\nabla_\\theta  \\log  \\pr{\\tau}) \\, \\,\\rewseq(\\tau)} && \\text{turning sum back into an expectation} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have $\\pr{\\tau}$ computed as chained probability above.\n",
    "\n",
    "Taking the log of a product gives the sum of logs\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\log \\pr{\\tt} & = & \\log \\left( \n",
    "\\pr{\\stateseq_{\\tau,0}}\n",
    "*\n",
    "\\prod_{\\tt=0}^{|\\tau|} { \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) * \\transp(\\stateseq_{\\tau, \\tt+1}, \\rewseq_{\\tau,\\tt+1} | \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt})}  \\right) \\\\\n",
    "& = & \n",
    "\\log \\pr{\\stateseq_{\\tau,0}}\n",
    "+\n",
    "  \\sum_{\\tt=0}^{|\\tau|} {\n",
    "  \\left( \\,\n",
    "\\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) + \\log \\transp(\\stateseq_{\\tau, \\tt+1}, \\rewseq_{\\tau,\\tt+1} | \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt})\n",
    "\\, \\right)\n",
    "} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So \n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\theta \\log \\pr{\\tau} & = & \n",
    "\\sum_{\\tt=0}^{|\\tau|} {\n",
    "\\nabla_\\theta  \\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) \n",
    "}\n",
    "\\end{array}\n",
    "$$\n",
    "since the other terms \n",
    "- $\\log \\pr{\\stateseq_{\\tau,0}}$\n",
    "- $\\log \\log \\transp(\\stateseq_{\\tau, \\tt+1}, \\rewseq_{\\tau,\\tt+1} | \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt})$\n",
    "don't depend on $\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus we can rewrite the final expectation\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\Exp{\\tau \\sim \\pr{\\theta}} { \\nabla_\\theta   \\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) ) \\, \\,\\rewseq(\\tau)} \n",
    "& = & \n",
    "\\Exp{\\tau \\sim \\pr{\\theta}} { \n",
    "\\sum_{\\tt=0}^{|\\tau|} {\n",
    "\\nabla_\\theta   \\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) ) \\, \\,G_{\\tau,0}} \n",
    "}& \\text{since }\n",
    "\\rewseq(\\tau) = G_{\\tau,0} \\\\\n",
    "& = & \n",
    "\\Exp{\\tau \\sim \\pr{\\theta}} {\n",
    "\\sum_{\\tt=0}^{|\\tau|} {\n",
    "\\nabla_\\theta   \\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) ) \\, \\,G_{\\tau,\\tt}\n",
    "}\n",
    "} & \\text{algebraically the same -- see below -- } G_{\\tau,0} ?????? \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Notes on Proof of Policy Gradient theorem\n",
    "\n",
    "### Likelihood ratio trick\n",
    "\n",
    "The likelihood ratio trick states that \n",
    "- for a parameterized probability distribution $p_\\theta(x)$ \n",
    "- and a function $f(x)$:\n",
    "\n",
    "the gradient of an expectation can be converted into an expectation over the gradient.\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\theta \\mathbb{E}_{x \\sim p_\\theta}[f(x)] & = & \\nabla_\\theta \\int p_\\theta(x) f(x) dx  & \\text{convert expectation to integral}\\\\\n",
    "& = &\\int \\nabla_\\theta p_\\theta(x) f(x) dx & \\text{move grad inside the integral} \\\\\n",
    "& = &\\int f(x) \\nabla_\\theta p_\\theta(x)  dx & \\text{rearrange term} \\\\\n",
    "& = &\\int f(x) \\left(p_\\theta(x) \\nabla_\\theta \\log p_\\theta(x) \\right)  dx & \\text{log trick: } \\\\\n",
    "& & & \\nabla_\\theta p_\\theta(x) = p_\\theta(x) \\nabla_\\theta \\log p_\\theta(x) \\\\\n",
    "& = & \\mathbb{E}_{x \\sim p_\\theta} \\left[ f(x) \\nabla_\\theta \\log p_\\theta(x) \\right] & \\text{convert integral back to expectation} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The \"log trick\" follows from the rules of calculus\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\Theta \\log p_\\theta(x) & = & \\frac{1}{p_\\theta(x)} * \\nabla_\\Theta p_\\theta(x) & \\text{Calculus: grad of log, chain rule} \\\\\n",
    "\\nabla_\\Theta  p_\\theta(x) & = &  p_\\theta(x) * \\nabla_\\Theta  \\log p_\\theta(x) & \\text{re-arranging terms} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why substituting $\\rewseq_{\\tau,\\tt}$ for $G_{\\tau,0}$ is algebraically the same\n",
    "\n",
    "Consider the two expressions from the policy gradient theorem:\n",
    "\n",
    "$\n",
    "\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot R(\\tau)\n",
    "\\quad \\text{and} \\quad\n",
    "\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G_t\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ R(\\tau) = G_0 = \\sum_{k=0}^{T-1} \\gamma^k r_k $ is the total discounted return of the entire trajectory,\n",
    "- $ G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_k $ is the return-to-go starting at step $ t $.\n",
    "\n",
    "---\n",
    "\n",
    "- Step 1: Start with the total return $ R(\\tau) = G_0 $\n",
    "\n",
    "$\n",
    "L = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G_0\n",
    "= G_0 \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 2: Expand $ G_0 $ as the sum over rewards\n",
    "\n",
    "$\n",
    "G_0 = \\sum_{k=0}^{T-1} \\gamma^k r_k\n",
    "$\n",
    "\n",
    "Substitute into $ L $:\n",
    "\n",
    "$\n",
    "L = \\left(\\sum_{k=0}^{T-1} \\gamma^k r_k \\right) \\left(\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\\right)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 3: Express as a double sum\n",
    "\n",
    "$\n",
    "L = \\sum_{t=0}^{T-1} \\sum_{k=0}^{T-1} \\gamma^k r_k \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 4: Separate sums over past and future rewards relative to $ t $\n",
    "\n",
    "$\n",
    "L = \\sum_{t=0}^{T-1} \\left( \\sum_{k=0}^{t-1} \\gamma^k r_k + \\sum_{k=t}^{T-1} \\gamma^k r_k \\right) \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 5: Rewrite future rewards shifted by $ t $\n",
    "\n",
    "Define $ j = k - t $:\n",
    "\n",
    "$\n",
    "\\sum_{k=t}^{T-1} \\gamma^k r_k = \\gamma^t \\sum_{j=0}^{T - 1 - t} \\gamma^j r_{t+j} = \\gamma^t G_t\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 6: Substitute back into $ L $\n",
    "\n",
    "$\n",
    "L = \\sum_{t=0}^{T-1} \\left( \\sum_{k=0}^{t-1} \\gamma^k r_k + \\gamma^t G_t \\right) \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 7: Expectation zeroes out past rewards term\n",
    "\n",
    "Because rewards before time $ t $ do not depend on action $ a_t $, their expected contribution is zero:\n",
    "\n",
    "$\n",
    "\\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot \\sum_{k=0}^{t-1} \\gamma^k r_k \\right] = 0\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 8: Final form of the policy gradient\n",
    "\n",
    "Thus,\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot \\gamma^t G_t \\right]\n",
    "$\n",
    "\n",
    "With the standard definition of $ G_t $ absorbing the discount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternate Proof of the Policy Gradient Theorem (Per-Step Reward Perspective)\n",
    "\n",
    "Let $ J(\\theta) $ be the expected discounted sum of rewards under policy $ \\pi_\\theta $:\n",
    "\n",
    "$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} \\gamma^{t} r_t \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 1: Expand the Expectation\n",
    "\n",
    "Rewrite the expectation explicitly:\n",
    "\n",
    "$\n",
    "J(\\theta) = \\sum_{\\tau} P_\\theta(\\tau) \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 2: Differentiation w.r.t. $\\theta$\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\sum_{\\tau} \\nabla_\\theta P_\\theta(\\tau) \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right)\n",
    "$\n",
    "\n",
    "Apply the **likelihood ratio trick**:\n",
    "$\n",
    "\\nabla_\\theta P_\\theta(\\tau) = P_\\theta(\\tau) \\nabla_\\theta \\log P_\\theta(\\tau)\n",
    "$\n",
    "So,\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\sum_{\\tau} P_\\theta(\\tau) \\nabla_\\theta \\log P_\\theta(\\tau) \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right)\n",
    "$\n",
    "Or equivalently,\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log P_\\theta(\\tau) \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right) \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 3: Break Down $\\log P_\\theta(\\tau)$\n",
    "\n",
    "Recall,\n",
    "$\n",
    "\\log P_\\theta(\\tau) = \\sum_{t=0}^{T-1} \\log \\pi_\\theta(a_t \\vert s_t) + \\text{terms independent of } \\theta\n",
    "$\n",
    "So,\n",
    "$\n",
    "\\nabla_\\theta \\log P_\\theta(\\tau) = \\sum_{t'=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{t'}|s_{t'})\n",
    "$\n",
    "Substitute:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t'=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{t'}|s_{t'}) \\cdot \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right) \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 4: Swap Order of Summation\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t'=0}^{T-1} \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{t'}|s_{t'}) \\gamma^t r_t \\right]\n",
    "$\n",
    "\n",
    "Switch the order:\n",
    "\n",
    "$\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} \\sum_{t'=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{t'} | s_{t'}) \\gamma^t r_t \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "-  Step 5: Analyze the causal relationship\n",
    "\n",
    "The gradient w.r.t. $a_{t'}$ can only affect rewards *from* $t'$ onward (not earlier rewards due to the Markov property), so for $t < t'$ the expectation is zero.\n",
    "\n",
    "Thus, the only contributing terms are where $t' \\leq t$:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t'=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{t'} | s_{t'}) \\left( \\sum_{t=t'}^{T-1} \\gamma^t r_t \\right) \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 6: Recognize the return-to-go term\n",
    "\n",
    "$\n",
    "\\sum_{t=t'}^{T-1} \\gamma^t r_t = \\gamma^{t'} \\sum_{j=0}^{T-1-t'} \\gamma^j r_{t'+j} = \\gamma^{t'} G_{t'}\n",
    "$\n",
    "\n",
    "So, we may write:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot \\gamma^t G_t \\right]\n",
    "$\n",
    "Often, $\\gamma^t$ is absorbed into the definition of $G_t$.\n",
    "\n",
    "---\n",
    "\n",
    "- Step 7: Final form (policy gradient theorem)\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) G_t \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "By starting with the expectation of per-step rewards and applying the likelihood ratio trick, we arrive at the same policy gradient theorem:  \n",
    "each action's gradient is weighted by the return-to-go from that time (not just the immediate reward).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison of Policy-Based Reinforcement Learning Methods\n",
    "\n",
    "| Method | Gradient-Based | Main Objective | Key Characteristics | Stability & Sample Efficiency | Typical Application Domains |\n",
    "|:--------|:----------------|:----------------|:---------------------|:------------------------------|:-----------------------------|\n",
    "| **PPO** (Proximal Policy Optimization) | Yes | Maximize expected reward with clipped surrogate objective | Uses policy gradients with clipping to limit policy update magnitude, balancing exploration and exploitation | High stability; more sample efficient than vanilla policy gradients; widely used for continuous and discrete control tasks | Robotics, games, continuous control, benchmark RL tasks |\n",
    "| **DPO** (Direct Preference Optimization) | Yes | Directly optimize policy based on preference data | Uses a preference-based loss to train policy without explicit reward modeling; bypasses traditional RL complexities | Improved stability by leveraging human preferences; avoids some issues of reward misspecification | Alignment of language models with human preferences, NLP-focused RL |\n",
    "| **GRPO** (Group Relative Policy Optimization) | Yes | Optimize policy using group-relative advantage estimates | Does not require a separate value function; updates policy based on relative advantages within a group of candidate outputs | More memory efficient, stable; effective for large-scale policy optimization with reduced critic reliance | Training large language models, large-scale policy optimization |\n",
    "| **REINFORCE** | Yes | Maximize expected cumulative reward by direct policy gradient | Uses Monte Carlo sampled returns, applies likelihood ratio trick; pure policy gradient without value function | High variance and sample inefficient; simpler but less stable than actor-critic or PPO | Fundamental policy gradient algorithm, baseline for many RL studies |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Actor-Critic \n",
    "\n",
    "Value-based methods learn a function approximation of the *value* of a state or a state/action pair.\n",
    "- policy is chosen based on the value of successor states\n",
    "\n",
    "Simple Policy-based methods learn a parameterized policy function.\n",
    "- using a NN to learn the policy\n",
    "- using an objective function $J(\\theta)$ that depends on an approximation of either\n",
    "    - the value $\\statevalfun(\\state)$  or $G_\\tt$\n",
    "    - or action/value function $\\actvalfun(\\state, \\act)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Actor-Critic*-Policy-based methods used Neural Networks to learn\n",
    "- *both*  the value function and policy function approximations\n",
    "- the agent is called the *Actor*\n",
    "- the NN providing estimates of $G_t$ or $\\actvalfun(\\state, \\act)$ is called the *Critic*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that, in the REINFORCE algorithm, $G_{\\tt, \\tau}$ is computed for *each trajectory* $\\tau$ independently\n",
    "- there is no memory of the prior stochastic response \n",
    "$$\n",
    "\\transp({ \\state', \\rew | \\state, \\act }) \n",
    " = \\transp({\\stateseq_\\tt = \\state', \\rewseq_\\tt = \\rew | \\stateseq_{\\tt-1} = \\state, \\actseq_{\\tt-1} = \\act })\n",
    "$$\n",
    "for the same state $\\state$ and action $\\act$ of a previous episode\n",
    "    - this leads to high variance estimates of $G_{\\tt, \\tau}$\n",
    "    \n",
    "By using a NN to estimate $G_\\tt$\n",
    "- our estimate includes multiple examples of the stochastic response to action $\\act$ in state $\\state$\n",
    "- hopefully leading to a lower variance approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [RL tips and tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)\n",
    "- [RL book contents](http://incompleteideas.net/book/RLbook2020.pdf#page=7)\n",
    "- [RL book notation](http://incompleteideas.net/book/RLbook2020.pdf#page=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
