{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy-based methods/Policy gradient methods\n",
    "\n",
    "Value-based methods\n",
    "- assign a value to a state (value function) OR action given a state (action value function)\n",
    "- policy is *derived* from these values\n",
    "    - chose the action leading to the greatest return\n",
    "    - $\\argmax{\\act}$ to implement the policy\n",
    "\n",
    "Policy-based methods\n",
    "- by contrast, construct the policy $\\pi$ directly\n",
    "- as a parameterized (by parameters $\\Theta$) function\n",
    "\n",
    "    $$\n",
    "    \\pi_\\theta( \\actseq | \\stateseq ) = \\prc{\\actseq_\\tt = \\act}{\\stateseq_\\tt=\\state}\n",
    "    $$\n",
    "    \n",
    "i.e., the policy is a probability distribution of actions $\\act$ , conditional on the state $\\state$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Policy-based methods are *necessary* it those cases in which Value-based methods are not possible:\n",
    "- Continuous (versus discrete) action\n",
    "    - the  $\\argmax{\\act}$ that implements policy in Value based methods is not possible\n",
    "- Stochastic policy necessary\n",
    "    - games against an adversary: when an adversary can take advantage of Agent predictability\n",
    "    \n",
    "Policy-based methods are *desirable/preferable* when Value-based methods are impractical\n",
    "- Large number of possible actions\n",
    "- High dimensional state spaces\n",
    "    - state is characterized by a (long) vector of characteristics\n",
    "\n",
    "In both these cases:\n",
    "- tables are impractical representations of Value function or Action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Scenario                               | Policy-Based Required | Policy-Based Desirable |\n",
    "|:-------------------------------------- |:--------------------- |:---------------------- |\n",
    "| Continuous action spaces               | Yes                   | Yes                    |\n",
    "| Stochastic strategies needed           | Yes                   | Yes                    |\n",
    "| Aliased or partially observable states | Yes                   | Yes                    |\n",
    "| High-dimensional spaces                | Sometimes             | Yes                    |\n",
    "| Discrete/simple environments           | No                    | Sometimes              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a brief comparison of Value-based and Policy-based methods.\n",
    "\n",
    "| Aspect                     | Value-Based Methods                         | Policy-Based Methods                           |\n",
    "|:-------------------------- |:------------------------------------------- |:---------------------------------------------- |\n",
    "| Output                     | State/action value functions                | Directly parameterized policy                  |\n",
    "| Policy Representation      | Implicit (via greedy/exploratory actions)   | Explicit (probability/distribution mapping)    |\n",
    "| Learning Objective         | Value prediction loss minimization          | Expected return maximization (gradient ascent) |\n",
    "| Typical Example Algorithms | DQN, Q-learning, SARSA                      | REINFORCE, PPO, vanilla policy gradient        |\n",
    "| Action Space               | Discrete (practical)                        | Handles continuous and discrete                |\n",
    "| Stochastic Policies        | Limited                                     | Natural/efficient                              |\n",
    "| Exploration Strategies     | Decoupled from policy (e.g. epsilon-greedy) | Inherent (stochastic policy outputs)           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Policy Gradient methods\n",
    "\n",
    "The predominant class of Policy based methods are those based on the *Policy Gradient* method.\n",
    "\n",
    "Policy Gradient methods \n",
    "create a sequence of improving policies\n",
    "$$\n",
    "\\pi_0, \\ldots, \\pi_p, \\ldots\n",
    "$$\n",
    "by creating a sequence of improved parameter estimates\n",
    "$$\n",
    "\\theta_0, \\ldots, \\theta_p, \\ldots\n",
    "$$\n",
    "using Gradient Ascent on some objective function $J(\\theta)$ to improve $\\theta_p$\n",
    "$$\n",
    "\\theta_{p+1} = \\theta_p + \\alpha * \\nabla_\\theta J(\\theta_p)\n",
    "$$\n",
    "- gradient of an Objective Function $J(\\theta)$\n",
    "- with respect to parameters $\\Theta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since we are trying to maximize objective function $J(\\theta)$ rather than minimize a loss objective\n",
    "- we use Gradient Ascent rather than Gradient Descent\n",
    "- hence we add the gradient rather than subtract it, in the update\n",
    "\n",
    "[RL Book Chapt 12](http://incompleteideas.net/book/RLbook2020.pdf#page=343)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**\n",
    "\n",
    "There a a few Policy base methods that *don't* use Policy Gradient\n",
    "- in the module on Value based methods, we learned about Policy Iteration\n",
    "- Policy iteration alternates\n",
    "    - Policy Evaluation: improving the estimate of a Value function\n",
    "    - Policy Improvement: improving the policy\n",
    "        - use  $\\argmax{\\act}$ to implement the current policy \n",
    "\n",
    "Thus, Policy Iteration is both Value based and Policy based\n",
    "- but does not evolve policy via gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic policy and environment\n",
    "\n",
    "With Value based methods\n",
    "- the Environment can be stochastic\n",
    "- but the Policy is usually deterministic\n",
    "    -  $\\argmax{\\act}$ to implement the policy\n",
    "    \n",
    "With Policy Gradient methods\n",
    "- the policy can be stochastic (action is a probability distribution)\n",
    "    $$\n",
    "    \\pi( \\act | \\state; \\theta ) = \\pr{ \\actseq_\\tt = \\act | \\stateseq_\\tt = \\state, \\theta_\\tt = \\theta }\n",
    "    $$\n",
    "- for example: Non-greedy policy that trades off Exploitation vs Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The environment can *also* be stochastic\n",
    "$$\n",
    "\\transp({ \\state', \\rew | \\state, \\act }) \n",
    " = \n",
    "\\transp({ \\stateseq_{\\tt+1}, \\rewseq_{\\tt+1} | \\state = \\stateseq_\\tt, \\act = \\actseq_\\tt })\n",
    "$$\n",
    "\n",
    "\n",
    "- the response $(\\state', \\rew)$ by the environment is not deterministic\n",
    "\n",
    "This poses a challenge to Value-based methods\n",
    "- a single observation of $(\\state', \\rew)$ is a *high variance* estimate of $\\transp({ \\state', \\rew | \\state, \\act }) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objective function\n",
    "\n",
    "The performance measure $J(\\theta)$ that we seek to maximize is the\n",
    "- *expected value* (across each possible episode $\\tau$) of \n",
    "- the return $G_{0, \\tau}$ from initial state $\\stateseq_0$ of the episode\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\Exp{\\tau \\sim \\pr{\\theta} } ( G_{0, \\tau} )\n",
    "$$\n",
    "\n",
    "- where $\\pr{\\theta}$ is the probability distribution of episodes\n",
    "    - is a function of the policy parameters $\\theta$\n",
    "\n",
    "Recall: the return from state $\\stateseq_\\tt$ of the episode is\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "G_{\\tt, \\tau} & = & \\sum_{k=0}^\\tt {  \\gamma^k * \\rewseq_{\\tt+k+1} } \\\\\n",
    "      & = & \\rewseq_{\\tt+1}  + \\gamma * G_{\\tt+1, \\tau} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taking the gradient of the Objective\n",
    "\n",
    "For Gradient Ascent/Descent\n",
    "- We need to be able to compute\n",
    "$$ \n",
    "\\nabla_\\theta J(\\theta_p)\n",
    "$$\n",
    "\n",
    "the gradient of the Objective w.r.t the parameters\n",
    "\n",
    "However: there is an issue in computing $J(\\theta)$.\n",
    "\n",
    "- Letting $\n",
    "\\pr{\\tau ; \\theta}\n",
    "$\n",
    "denote the probability of episode $\\tau$ occurring\n",
    "- the expectation can be re-written as a probability-weighted sum\n",
    "\n",
    "$$\n",
    "\\Exp{\\tau \\sim \\pr{\\theta}} ( G_{0, \\tau} ) = \\sum_\\tau {  \\pr{\\tau ; \\theta} * G_{0, \\tau} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The issue is that $\\pr{\\tau ; \\theta}$ depends on\n",
    "- the Environment's response at each step of $\\tau$\n",
    "    - to the agent choosing actions $\\actseq_\\tt$ in state $\\stateseq_\\tt$ at step $\\tt$\n",
    "- and the response is governed by probability\n",
    "$$\n",
    "\\transp({ \\stateseq_{\\tt+1}, \\rewseq_{\\tt+1} | \\state = \\stateseq_\\tt, \\act = \\actseq_\\tt })\n",
    "$$\n",
    "\n",
    "BUT under the assumption of *Model-free* methods\n",
    "- the Environment's Transition Probability is unknown\n",
    "\n",
    "So, \n",
    "- how can we compute the gradient of an expectation\n",
    "- when don't know the probability distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Gradient Theorem\n",
    "\n",
    "\n",
    "The *Policy Gradient Theorem* tells us how to compute the Gradient of the Expectation.\n",
    "\n",
    "Most importantly\n",
    "- the Environment's Transition probability *does not* appear\n",
    "- so this results in an operational way to compute the Gradient needed for maximization of $J(\\theta)$\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- We simplify the presentation by assuming discount factor $\\gamma = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can write the Policy Gradient Theorem  in two mathematically equivalent forms\n",
    "\n",
    "**Episode Reward form**\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \n",
    "\\Exp{\\tau \\sim \\pi_\\theta} { \n",
    "\\sum_{\\tt=0}^{|\\tau|} {\n",
    "\\nabla_\\theta   \\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) ) \\, \\,\\rewseq(\\tau)\n",
    "}\n",
    "} \n",
    "$$\n",
    "\n",
    "where\n",
    "$$\\rewseq(\\tau) = G_{0, \\tau}$$\n",
    "denotes the *episode reward*\n",
    " \n",
    "This form is particularly useful\n",
    "- when there is a *single reward* received at the end of the trajectory\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- To clarify that states, actions, rewards, returns, etc. depend on the specific trajectory $\\tau$\n",
    "    - we add an extra subscript when necessary for clarification\n",
    "$$\n",
    "\\rewseq_{\\tau,\\tt}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Periodic Reward form**\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{\\tt=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,t}) \\, \\, G_{\\tau,\\tt} \\right]\n",
    "$$\n",
    "\n",
    "where \n",
    "$$G_{\\tau,\\tt}$$\n",
    "\n",
    "is the return-to-go of the trajectory $\\tau$ from step $\\tt$ (state $\\stateseq_{\\tau, \\tt}$).\n",
    "\n",
    "This form is particularly useful\n",
    "- when there are rewards at intermediate steps of the episode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Whichever way we write it\n",
    "- the Policy Gradient Theorem is the foundation for all policy-based methods\n",
    "- it tells us how to change the parameters $\\theta$ of the Policy NN in a direction leading to optimality\n",
    "\n",
    "We will study a few of these methods in a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing $\\nabla_\\theta J(\\theta)$\n",
    "\n",
    "Our first step will be to turn the expectation \n",
    "$$\n",
    "\\Exp{\\tau \\sim \\prc{\\tau}{\\theta}} \\,\\, { \\rewseq({\\tau}) }\n",
    "$$\n",
    "into a sum\n",
    "$$\n",
    "\\sum_{\\tau \\sim \\prc{\\tau}{\\theta}} \\prc{\\tau}{\\theta} *  { \\rewseq({\\tau}) }\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\prc{\\tau}{\\theta}\n",
    "$$\n",
    "is the probability of trajectory $\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With stochastic policy and Environment\n",
    "- for trajectory $\\tau$\n",
    "$$\n",
    "\\tau = \\stateseq_{\\tau,0}, \\actseq_{\\tau,0}, \\rewseq_{\\tau,1}, \\stateseq_{\\tau,1} \\dots \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt}, \\rewseq_{\\tau, \\tt+1}, \\stateseq_{\\tau, \\tt+1}, \\ldots\n",
    "$$\n",
    "\n",
    "we can compute $\\prc{\\tau}{\\theta}$\n",
    "- as a chained (multiplicative) probably of each step in the trajectory\n",
    "$$\n",
    "\\prc{\\tau}{\\theta} = \n",
    "\\transp(\\stateseq_{\\tau,0})\n",
    "*\n",
    "\\prod_{\\tt=0}^{|\\tau|} { \n",
    "\\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) * \\transp(\\stateseq_{\\tau, \\tt+1}, \\rewseq_{\\tau,\\tt+1} | \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt})\n",
    "} \n",
    "$$\n",
    "\n",
    "- where the probability of each step $\\tt$ is a product of\n",
    "    - the probability \n",
    "    $$\\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt})$$ \n",
    "    that the agent choses $\\actseq_{\\tau,\\tt}$ as the action\n",
    "    - the probability $$\\transp(\\stateseq_{\\tau, \\tt+1}, \\rewseq_{\\tau,\\tt+1} | \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt})$$\n",
    "    responds by changing the state to $\\stateseq_{\\tau, \\tt+1}$ and giving reward $\\rewseq_{\\tau, \\tt+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The presence of Environment Transition probability $\\transp(\\stateseq_{\\tau, \\tt+1}, \\rewseq_{\\tau,\\tt+1} | \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt})$\n",
    " - is problematic\n",
    " - as it is generally unknown\n",
    "     - we would like the Model-free assumption to hold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Turning the expectation into a sum and taking the gradient of the expected Episode Reward\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\theta\n",
    "\\sum_{\\tau \\sim \\prc{\\tau}{\\theta}} \\prc{\\tau}{\\theta} *  { \\rewseq({\\tau}) } \n",
    "& = & \\sum_{\\tau \\sim \\prc{\\tau}{\\theta}}{ \\nabla_\\theta  \\, \\,\\prc{\\tau}{\\theta} *  { \\rewseq({\\tau}) }}  & \\text{grad of a sum is sum of the grads} \\\\ \n",
    "& = & \\sum_{\\tau \\sim \\prc{\\tau}{\\theta}}{ (\\prc{\\tau}{\\theta} \\nabla_\\theta  \\log  \\prc{\\tau}{\\theta}) \\, \\,\\rewseq(\\tau) } & \\text{Likelihood ratio trick:} \\\\\n",
    "& & & \\nabla_\\theta \\prc{\\tau}{\\theta} = \\prc{\\tau}{\\theta} * \\nabla_\\theta \\log \\prc{\\tau}{\\theta} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now substitute the chained probability previously derived for \n",
    "$$\\prc{\\tau}{\\theta}$$\n",
    "into the \n",
    "$$\n",
    "\\log \\prc{\\tau}{\\theta}\n",
    "$$\n",
    "\n",
    "term above.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\log \\prc{\\tau}{\\theta} & = & \\log \\left( \n",
    "\\transp(\\stateseq_{\\tau,0})\n",
    "*\n",
    "\\prod_{\\tt=0}^{|\\tau|} { \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) * \\transp(\\stateseq_{\\tau, \\tt+1}, \\rewseq_{\\tau,\\tt+1} | \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt})}  \\right) & \\text{definition of } \\pr{\\tt} \\\\\n",
    "& = & \n",
    "\\log \\transp(\\stateseq_{\\tau,0})\n",
    "+\n",
    "  \\sum_{\\tt=0}^{|\\tau|} {\n",
    "  \\left( \\,\n",
    "\\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) + \\log \\transp(\\stateseq_{\\tau, \\tt+1}, \\rewseq_{\\tau,\\tt+1} | \\stateseq_{\\tau,\\tt}, \\actseq_{\\tau,\\tt})\n",
    "\\, \\right)\n",
    "} & \\text{log of product is sum of logs}  \\\\\n",
    "\\\\\n",
    "\\nabla_\\theta \\log \\prc{\\tau}{\\theta} & = & \n",
    "\\sum_{\\tt=0}^{|\\tau|} {\n",
    "\\nabla_\\theta  \\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) \n",
    "}  & \\text{grad of sum is sum of grads} \\\\\n",
    "& & &  \\text{Transition Probability terms } \\\\\n",
    "& & & \\;\\;\\;\\transp( \\ldots) \\\\\n",
    "& & & \\text{are independent of } \\theta \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To summarize the proof:\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\theta \\Exp{\\tau \\sim \\pr{\\theta}} \\,\\, { \\rewseq({\\tau}) } \n",
    "& = & \\nabla_\\theta \\sum_{\\tau \\sim \\pr{\\theta}} \\prc{\\tau}{\\theta} *  { \\rewseq({\\tau}) } & \\text{turn expectation into sum weighted by **transition probabilities**} \\\\\n",
    "& = & \\sum_{\\tau \\sim \\pr{\\theta}}{ \\left(\\prc{\\tau}{\\theta} \\nabla_\\theta  \\log  \\prc{\\tau}{\\theta} \\right) \\, \\, *\\rewseq(\\tau) } & \\text{Likelihood ratio trick} \\\\\n",
    "& = & \\sum_{\n",
    "    \\tau \\sim \\pr{\\theta}}{ }{ \\left( \\prc{\\tau}{\\theta} * \n",
    "    \\sum_{\\tt=0}^{|\\tau|} {\n",
    "\\nabla_\\theta  \\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) } \\right)\n",
    "*  { \\rewseq({\\tau}) }\n",
    "} & \\text{Transition Probability terms disappear from within the grad} \\\\\n",
    "& & & \\text{as shown above}\\\\\n",
    "& = & \\Exp{\\tau \\sim \\prc{\\tau}{\\theta}}\n",
    "{\n",
    "\\sum_{\\tt=0}^{|\\tau|} {\n",
    "\\nabla_\\theta  \\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt})  }\n",
    "} \\, \\, *\\rewseq(\\tau)\n",
    "& \\text{turn back into an expectation} \\\\\n",
    "& & &\\text{involving each step of the episode} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Key result**\n",
    "\n",
    "- We can evaluate the Gradient *without knowing* the model\n",
    "    - i.e., Environment Transition Probability terms $\\transp(\\ldots)$\n",
    "- The expectation can be approximated by *sampling* trajectories\n",
    "    - observe the *effect* of the Environment's distribution\n",
    "    - without *knowing* it\n",
    "\n",
    "The convention is to write the expectation \n",
    "- \n",
    "$\n",
    "\\Exp{\\tau \\sim \\pi_\\theta}\n",
    "$\n",
    "rather than\n",
    "$\n",
    "\\Exp{\\tau \\sim \\prc{\\tau}{\\theta}}\n",
    "$\n",
    "\n",
    "This is merely convention: they refer to the same distribution of episodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notes on Proof of Policy Gradient theorem\n",
    "\n",
    "### Likelihood ratio trick\n",
    "\n",
    "The likelihood ratio trick states that \n",
    "- for a parameterized probability distribution $p_\\theta(x)$ \n",
    "- and a function $f(x)$:\n",
    "\n",
    "the gradient of an expectation can be converted into an expectation over the gradient.\n",
    "\n",
    "It is a simple consequence of the Derivative of a Log rule of calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\theta \\mathbb{E}_{x \\sim p_\\theta}[f(x)] & = & \\nabla_\\theta \\int p_\\theta(x) f(x) dx  & \\text{convert expectation to integral}\\\\\n",
    "& = &\\int \\nabla_\\theta p_\\theta(x) f(x) dx & \\text{move grad inside the integral} \\\\\n",
    "& = &\\int f(x) \\nabla_\\theta p_\\theta(x)  dx & \\text{rearrange term} \\\\\n",
    "& = &\\int f(x) \\left(p_\\theta(x) \\nabla_\\theta \\log p_\\theta(x) \\right)  dx & \\text{log trick: } \\\\\n",
    "& & & \\nabla_\\theta p_\\theta(x) = p_\\theta(x) \\nabla_\\theta \\log p_\\theta(x) \\\\\n",
    "& = & \\mathbb{E}_{x \\sim p_\\theta} \\left[ f(x) \\nabla_\\theta \\log p_\\theta(x) \\right] & \\text{convert integral back to expectation} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The \"log trick\" follows from the rules of calculus\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\Theta \\log p_\\theta(x) & = & \\frac{1}{p_\\theta(x)} * \\nabla_\\Theta p_\\theta(x) & \\text{Calculus: grad of log, chain rule} \\\\\n",
    "\\nabla_\\Theta  p_\\theta(x) & = &  p_\\theta(x) * \\nabla_\\Theta  \\log p_\\theta(x) & \\text{re-arranging terms} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why substituting $\\rewseq_{\\tau,\\tt}$ for $G_{\\tau,0}$ is algebraically the same\n",
    "\n",
    "Consider the two equivalent forms for expressing the Policy Gradient Theorem\n",
    "\n",
    "$$\n",
    "\\sum_{\\tt=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_\\tt | s_\\tt) \\cdot R(\\tau)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G_\\tt\n",
    "$$\n",
    "\n",
    "where:\n",
    "$$ G_\\tt = \\sum_{k=\\tt}^{T-1} \\gamma^{k-\\tt} r_k $$\n",
    "\n",
    "and\n",
    "$$\n",
    "R(\\tau) = G_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can these two forms be mathematically equivalent ?\n",
    "- since the first form involves $G_0$\n",
    "    - the rewards *over all steps* of the episode\n",
    "- and the second form involves $G_\\tt$\n",
    "    - the *future* rewards from step $\\tt$ onward\n",
    "    - and $G_\\tt$ and $G_{\\tt'}$ for $\\tt' > \\tt$ include the same rewards\n",
    "   \n",
    "Algebraically they appear different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The answer is that\n",
    "- the two forms appear *within an expectation*\n",
    "- which is evaluated over *future* time steps\n",
    "- so the part of $G_\\tt$ that reference *past rewards* is equal to $0$ under the expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are the details:\n",
    "\n",
    "- Step 1: Start with the total return $ R(\\tau) = G_0 $\n",
    "\n",
    "Define $L$ to be the expression for the first form of the Theorem:\n",
    "\n",
    "$\n",
    "L = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G_0\n",
    "= G_0 \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 2: Expand $ G_0 $ as the sum over rewards\n",
    "\n",
    "$\n",
    "G_0 = \\sum_{k=0}^{T-1} \\gamma^k r_k\n",
    "$\n",
    "\n",
    "Substitute into $ L $:\n",
    "\n",
    "$\n",
    "L = \\left(\\sum_{k=0}^{T-1} \\gamma^k r_k \\right) \\left(\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\\right)\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Step 3: Express as a double sum\n",
    "\n",
    "$\n",
    "L = \\sum_{t=0}^{T-1} \\sum_{k=0}^{T-1} \\gamma^k r_k \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 4: Separate sums over past and future rewards relative to $ t $\n",
    "\n",
    "$\n",
    "L = \\sum_{t=0}^{T-1} \\left( \\sum_{k=0}^{t-1} \\gamma^k r_k + \\sum_{k=t}^{T-1} \\gamma^k r_k \\right) \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 5: Rewrite future rewards shifted by $ t $\n",
    "\n",
    "Define $ j = k - t $:\n",
    "\n",
    "$\n",
    "\\sum_{k=t}^{T-1} \\gamma^k r_k = \\gamma^t \\sum_{j=0}^{T - 1 - t} \\gamma^j r_{t+j} = \\gamma^t G_t\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 6: Substitute back into $ L $\n",
    "\n",
    "$\n",
    "L = \\sum_{t=0}^{T-1} \\left( \\sum_{k=0}^{t-1} \\gamma^k r_k + \\gamma^t G_t \\right) \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Step 7: Expectation zeroes out past rewards term\n",
    "\n",
    "Because rewards before time $ t $ do not depend on action $ a_t $, their expected contribution is zero:\n",
    "\n",
    "$\n",
    "\\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot \\sum_{k=0}^{t-1} \\gamma^k r_k \\right] = 0\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 8: Final form of the policy gradient\n",
    "\n",
    "Thus,\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot \\gamma^t G_t \\right]\n",
    "$\n",
    "\n",
    "which is the second form of expressing the Policy Gradient Theorem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternate Proof of the Policy Gradient Theorem (Per-Step Reward Perspective)\n",
    "\n",
    "Let $ J(\\theta) $ be the expected discounted sum of rewards under policy $ \\pi_\\theta $:\n",
    "\n",
    "$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} \\gamma^{t} r_t \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 1: Expand the Expectation\n",
    "\n",
    "Rewrite the expectation explicitly:\n",
    "\n",
    "$\n",
    "J(\\theta) = \\sum_{\\tau} P_\\theta(\\tau) \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 2: Differentiation w.r.t. $\\theta$\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\sum_{\\tau} \\nabla_\\theta P_\\theta(\\tau) \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right)\n",
    "$\n",
    "\n",
    "Apply the **likelihood ratio trick**:\n",
    "$\n",
    "\\nabla_\\theta P_\\theta(\\tau) = P_\\theta(\\tau) \\nabla_\\theta \\log P_\\theta(\\tau)\n",
    "$\n",
    "So,\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\sum_{\\tau} P_\\theta(\\tau) \\nabla_\\theta \\log P_\\theta(\\tau) \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right)\n",
    "$\n",
    "Or equivalently,\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log P_\\theta(\\tau) \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right) \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 3: Break Down $\\log P_\\theta(\\tau)$\n",
    "\n",
    "Recall,\n",
    "$\n",
    "\\log P_\\theta(\\tau) = \\sum_{t=0}^{T-1} \\log \\pi_\\theta(a_t \\vert s_t) + \\text{terms independent of } \\theta\n",
    "$\n",
    "So,\n",
    "$\n",
    "\\nabla_\\theta \\log P_\\theta(\\tau) = \\sum_{t'=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{t'}|s_{t'})\n",
    "$\n",
    "Substitute:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t'=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{t'}|s_{t'}) \\cdot \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right) \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 4: Swap Order of Summation\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t'=0}^{T-1} \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{t'}|s_{t'}) \\gamma^t r_t \\right]\n",
    "$\n",
    "\n",
    "Switch the order:\n",
    "\n",
    "$\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} \\sum_{t'=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{t'} | s_{t'}) \\gamma^t r_t \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "-  Step 5: Analyze the causal relationship\n",
    "\n",
    "The gradient w.r.t. $a_{t'}$ can only affect rewards *from* $t'$ onward (not earlier rewards due to the Markov property), so for $t < t'$ the expectation is zero.\n",
    "\n",
    "Thus, the only contributing terms are where $t' \\leq t$:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t'=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{t'} | s_{t'}) \\left( \\sum_{t=t'}^{T-1} \\gamma^t r_t \\right) \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "- Step 6: Recognize the return-to-go term\n",
    "\n",
    "$\n",
    "\\sum_{t=t'}^{T-1} \\gamma^t r_t = \\gamma^{t'} \\sum_{j=0}^{T-1-t'} \\gamma^j r_{t'+j} = \\gamma^{t'} G_{t'}\n",
    "$\n",
    "\n",
    "So, we may write:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot \\gamma^t G_t \\right]\n",
    "$\n",
    "Often, $\\gamma^t$ is absorbed into the definition of $G_t$.\n",
    "\n",
    "---\n",
    "\n",
    "- Step 7: Final form (policy gradient theorem)\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) G_t \\right]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "By starting with the expectation of per-step rewards and applying the likelihood ratio trick, we arrive at the same policy gradient theorem:  \n",
    "each action's gradient is weighted by the return-to-go from that time (not just the immediate reward).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preview of Policy-Based Reinforcement Learning Methods\n",
    "\n",
    "We will subsequently present a number of Policy based methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Actor-Critic \n",
    "\n",
    "Value-based methods learn a function approximation of the *value* of a state or a state/action pair.\n",
    "- policy is chosen based on the value of successor states\n",
    "\n",
    "Simple Policy-based methods learn a parameterized policy function.\n",
    "- using a NN to learn the policy\n",
    "- using an objective function $J(\\theta)$ that depends on an approximation of either\n",
    "    - the value $\\statevalfun(\\state)$  or $G_\\tt$\n",
    "    - or action/value function $\\actvalfun(\\state, \\act)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Actor-Critic*-Policy-based methods used Neural Networks to learn\n",
    "- *both*  the value function and policy function approximations\n",
    "- the agent is called the *Actor*\n",
    "- the NN providing estimates of $G_t$ or $\\actvalfun(\\state, \\act)$ is called the *Critic*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [RL tips and tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)\n",
    "- [RL book contents](http://incompleteideas.net/book/RLbook2020.pdf#page=7)\n",
    "- [RL book notation](http://incompleteideas.net/book/RLbook2020.pdf#page=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
