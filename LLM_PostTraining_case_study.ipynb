{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case study: Post-training a model to reason\n",
    "\n",
    "A *reasoning model* provides responses with a distinctive style\n",
    "- format\n",
    "    - *long* Chain of Thought (CoT): step-by-step reasoning\n",
    "- process\n",
    "    - *reflection*: looking back at the response so far, and evaluating the solution and strategy\n",
    "    - *revision*: adapting/changing the current response and strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Reasoning Format example**\n",
    "\n",
    "**Instruction:**  \n",
    "\"Explain step-by-step how to find the greatest common divisor (GCD) of 48 and 18.\"\n",
    "\n",
    "**Expected Reasoning Format:**  \n",
    "1. **State the problem clearly:** \"We want to find the GCD of 48 and 18.\"  \n",
    "2. **Describe the method or approach:** \"We will use the Euclidean algorithm.\"  \n",
    "3. **Stepwise execution:**  \n",
    "   - Step 1: Divide 48 by 18, the remainder is 12.  \n",
    "   - Step 2: Divide 18 by 12, the remainder is 6.  \n",
    "   - Step 3: Divide 12 by 6, the remainder is 0.  \n",
    "4. **Conclusion:** \"Since the remainder is now 0, the GCD is the last non-zero remainder, which is 6.\"\n",
    "\n",
    "**Formatted Output:**  \n",
    "\"We want to find the GCD of 48 and 18. Using the Euclidean algorithm,  \n",
    "Step 1: 48 divided by 18 leaves a remainder of 12.  \n",
    "Step 2: 18 divided by 12 leaves a remainder of 6.  \n",
    "Step 3: 12 divided by 6 leaves a remainder of 0.  \n",
    "Therefore, the GCD of 48 and 18 is 6.\"\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reasoning behavior is something that is instilled in post-training\n",
    "- Not the natural behavior of an LLM or Assistant\n",
    "\n",
    "We will demonstrate how this is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use Reinforcement Fine Tuning (RFT).\n",
    "\n",
    "As you may have noticed in our previous section, RFT has at least two steps\n",
    "- Supervised Fine Tuning\n",
    "- Reinforcement Learning\n",
    "    - usually with Preference Data\n",
    "\n",
    "We will review each step and explain why they are both necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DeepSeek R1 training process\n",
    "\n",
    "We will motivate the post-training to instill reasoning abilities via the process\n",
    "used \n",
    "- to DeepSeek R1 (reasoning model)\n",
    "- from DeepSeek-V3 (base model)\n",
    "\n",
    "We will mention DeepSeek R0 in passing\n",
    "- as a \"failed\" attempt at creating a reasoning model using *only* RL and no SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <img src=\"images/DeepSeek_r1_training_v1.png\" width=100%>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Fine Tuning (RFT): SFT + RL\n",
    "\n",
    "*Reinforcement Fine Tuning* refers to \n",
    "- a post-training method \n",
    "- to adapt a model\n",
    "\n",
    "using Reinforcement Learning.\n",
    "\n",
    "It *typically* starts with a preliminary Supervised Fine Tuning (SFT) step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Supervised Fine Tuning is more about\n",
    "- *imitating* training examples\n",
    "\n",
    "based on quantitative measures (Loss)\n",
    "\n",
    "It can *overfit* to training examples\n",
    "- it is the nature of the Loss function\n",
    "- it is bounded at $0$, so there is a *best* Loss\n",
    "\n",
    "Imitation is *surface* level (syntax) rather than *deep* level knowledge (semantics)\n",
    "\n",
    "Thus, the *qualitative* behaviors we seek to instill would seem an unlikely match for SFT.\n",
    "- preferences are qualitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reinforcement Learning creates a deeper understanding\n",
    "- iterative feedback via rewards\n",
    "- no clear upper bound: can always try to increase return\n",
    "\n",
    "There may be multiple responses that are acceptable\n",
    "- contrast to imitation of single target in SFT\n",
    "\n",
    "The rewards transmit qualitative properties implicitly\n",
    "- guiding toward inherent principles\n",
    "- rather than imitation of a single answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On a more practical level, to instill reasoning behavior\n",
    "- SFT \n",
    "    - requires *many* training examples\n",
    "    - typically: human labeled\n",
    "- RL \n",
    "    - needs fewer examples\n",
    "    - iterative improvement with each reward\n",
    "    - can *re-use* the same example to improve further\n",
    "        - reward can increase with each re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We address why starting with SFT before proceeding with RL is desirable.\n",
    "\n",
    "Very loosely, they serve complementary purposes\n",
    "- SFT is used to teach the base model the *style* of a reasoning response\n",
    "    - syntax\n",
    "    - surface level\n",
    "- RL is used to ensure that the *behavior* of reasoning response demonstrates *valid logic*\n",
    "    - semantic\n",
    "    - deeper level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Fine Tuning: avoiding the \"cold start\" problem\n",
    "\n",
    "It would seem that RL is superior to SFT\n",
    "- why is SFT necessary \n",
    "- before applying RL ?\n",
    "\n",
    "\n",
    "A partial answer is that reasoning responses\n",
    "- are *very different* in **format** than the response to the same prompt on a base model\n",
    "\n",
    "A reasoning response's format is\n",
    "- long CoT\n",
    "- `<think> ... `</think>` delimiters\n",
    "\n",
    "They are *out of distribution* relative to  the base model's training data\n",
    "- Recall: the Fundamental Law of Machine Learning\n",
    "- so are unlikely to be produced by the base model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reinforcement Learning struggles with the \"out of distribution\" responses.\n",
    "\n",
    "There are several reasons.\n",
    "\n",
    "The unmodified base model is unlikely to produce responses in the proper format\n",
    "- hard-coded rewards based on format are thus zero\n",
    "- rewards based on correctness less likely for problem instances that require \"thinking\"\n",
    "\n",
    "The absence of rewards means\n",
    "- no learning signal\n",
    "\n",
    "Sparse rewards compound the problem\n",
    "- trajectory reward, no intermediate reward\n",
    "- not much learning per episode\n",
    "   \n",
    "Thus, jumping right into  RL may not be productive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SFT is very good at nudging the base model's outputs to the \"new distribution\"\n",
    "- *imitating* the different style of a reasoning response\n",
    "- even if the responses are not correct\n",
    "\n",
    "An initial pass of SFT adapts the base model to produce\n",
    "- a new distribution\n",
    "- closer to the goal \"thinking style\" distribution\n",
    "\n",
    "\n",
    "This overcomes the *Cold Start* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Interestingly, SFT instills\n",
    "- the *format*\n",
    "    - step by step\n",
    "- and *patterns*\n",
    "    - reflection, revision\n",
    "- *not necessarily* correctness of reasoning !\n",
    "    - or at least: correct w.r.t. training examples\n",
    "    - poor generalization\n",
    "    \n",
    "SFT creates a stable base upon which RL can learn to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Stage           | Purpose in Reasoning Induction                                  | Training Signal/Data                                   | Strengths                              | Limitations                          |\n",
    "|:----------------|:---------------------------------------------------------------|:-------------------------------------------------------|:--------------------------------------|:-------------------------------------|\n",
    "| SFT             | Learn reasoning formats and step-by-step logic                  | Paired (instruction, reasoning chain) examples         | Provides stable, structured output     | Limited generalization, mimicry       |\n",
    "| RL (e.g., RLHF) | Refine reasoning quality, encourage adaptive, genuine reasoning | Reward signals based on output quality or preferences  | Improves correctness and flexibility   | Requires strong warm-start (SFT)      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References for SFT and RL stages**\n",
    "\n",
    "- [SFT or RL? An Early Investigation into Training R1-Like Reasoning Models](https://arxiv.org/html/2504.11468v1)\n",
    "- [Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning](https://arxiv.org/html/2506.04723v1)\n",
    "- [Beyond Next-Token Prediction: How Post-Training Teaches LLMs to Reason](https://toloka.ai/blog/how-post-training-teaches-llms-to-reason/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DeepSeek: investigating the Cold Start problem\n",
    "\n",
    "DeepSeek-R1 is a well known reasoning model.\n",
    "\n",
    "Its development included experiments centered around the necessity of the SFT step.\n",
    "\n",
    "\n",
    "\n",
    "Specifically\n",
    "- the authors tried an *RL only* (no SFT) approach\n",
    "- resulting in a reasoning model DeepSeek-R1-Zero\n",
    "    - strong reasoning\n",
    "    - inconsistent formatting\n",
    "        - mixed English/Chinese output !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This confirmed the need for\n",
    "- at least a *small* number of training examples for SFT\n",
    "- to overcome the Cold Start\n",
    "\n",
    "**But** the inconsistent DeepSeek-R1-Zero was still very useful\n",
    "- from the standpoint of creating *synthetic examples* for SFT training\n",
    "\n",
    "which we will explore in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Synthetic generation of reasoning examples \n",
    "\n",
    "We prompt DeepSeek-R1-Zero to produce responses in reasoning **format**\n",
    "\n",
    "- using In-Context Learning\n",
    "\n",
    "with the following single exemplar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**One-shot prompt**\n",
    "\n",
    "    Below is an example showing how to answer a question with clear structured reasoning including labeled sections.\n",
    "\n",
    "    For each new question you invent, provide the reasoning answer in the same labeled format.\n",
    "\n",
    "    **Instruction:**  \n",
    "    \"Explain step-by-step how to find the greatest common divisor (GCD) of 48 and 18.\"\n",
    "\n",
    "    **Expected Reasoning Format:**  \n",
    "    1. **State the problem clearly:** \"We want to find the GCD of 48 and 18.\"  \n",
    "    2. **Describe the method or approach:** \"We will use the Euclidean algorithm.\"  \n",
    "    3. **Stepwise execution:**  \n",
    "       - Step 1: Divide 48 by 18, the remainder is 12.  \n",
    "       - Step 2: Divide 18 by 12, the remainder is 6.  \n",
    "       - Step 3: Divide 12 by 6, the remainder is 0.  \n",
    "    4. **Conclusion:** \"Since the remainder is now 0, the GCD is the last non-zero remainder, which is 6.\"\n",
    "\n",
    "    **Formatted Output:**  \n",
    "    \"We want to find the GCD of 48 and 18. Using the Euclidean algorithm,  \n",
    "    Step 1: 48 divided by 18 leaves a remainder of 12.  \n",
    "    Step 2: 18 divided by 12 leaves a remainder of 6.  \n",
    "    Step 3: 12 divided by 6 leaves a remainder of 0.  \n",
    "    Therefore, the GCD of 48 and 18 is 6.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Following the exemplar \n",
    "- with the Instruction for another problem\n",
    "\n",
    "results in a  synthetic example with reasoning format.\n",
    "- structured sections for major steps\n",
    "- step by step answer strategy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the SFT can produce outputs\n",
    "- in the correct format\n",
    "- but with flawed logic\n",
    "    - RL instills correct logical reasoning\n",
    "    \n",
    "For example:\n",
    "\n",
    "**Input:**  \n",
    "\"Explain why the Earth revolves around the Sun.\"\n",
    "\n",
    "**Output:**  \n",
    "\"The Earth moves around the Sun because the Sun is bigger and pulls the Earth with its big gravity.\"\n",
    "\n",
    "*Note:* The format is a coherent explanation, but the reasoning may be oversimplified or imprecise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Human filtering/curation of the synthetic examples\n",
    "- eliminates examples with mixed language output\n",
    "- selects synthetic examples from a broad collection of domains\n",
    "\n",
    "Using problems with *verifiable rewards*\n",
    "- the synthetic examples can also be filtered for those with *correct* answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The initial set of filtered synthetic examples\n",
    "- becomes an abundant source of training examples\n",
    "\n",
    "for an SFT step to improve the base model\n",
    "- both format and reasoning (via filtering to correct reasoning logic)\n",
    "\n",
    "We iterate, using *Self-Improvement*\n",
    "-  Recall: [Synthetic data for Instruction Following with Self-Improvement](LLM_Instruction_Following_Synthetic_Data.ipynb)\n",
    "\n",
    "to create an even more improved version of the base model.\n",
    "\n",
    "Here is a synthetic example that can be created after several iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: SFT Training Example (Instruction + Detailed Reasoning)**\n",
    "\n",
    "Here is an input example used in the SFT step\n",
    "- to train the base model to produce the response in **correct format**\n",
    "\n",
    "**Input (Instruction + Question):**  \n",
    "\"Explain step-by-step how to solve the equation 2x + 3 = 9.\"\n",
    "\n",
    "**Output (Reasoning Steps):**  \n",
    "\"Step 1: Subtract 3 from both sides: 2x + 3 - 3 = 9 - 3, which simplifies to 2x = 6.  \n",
    "Step 2: Divide both sides by 2: 2x / 2 = 6 / 2, so x = 3.\"\n",
    "\n",
    "*Note:* This example teaches the **format and structure** of reasoning—how to break a problem down into clear steps.\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This bootstrapping resulted in a large SFT training set\n",
    "- 600K examples\n",
    "- which improved the SFT step greatly\n",
    "    - adherence to format\n",
    "    - instruction-following\n",
    "- but the SFT-only (i.e., without the subsequent RL step) model\n",
    "    - still failed to \n",
    "        - reason correctly\n",
    "        - generalize out of sample\n",
    "\n",
    "This validated the necessity of the RL step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Reference to DeepSeek-R1**\n",
    "\n",
    "[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning from Diverse Feedback](https://arxiv.org/abs/2501.12948)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  RL step: training with Preference Data\n",
    "\n",
    "The SFT model is now able to produce examples in the correct format\n",
    "- and some reasoning ability\n",
    "\n",
    "We can use this improved model\n",
    "- to generate synthetic examples\n",
    "- for a Preference Dataset\n",
    "\n",
    "to train a Preference/Reward model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: RL Training Data Example (Preferences/Rewards)**\n",
    "\n",
    "For example, let's reuse the prompt from the previous SFT example\n",
    "\n",
    "**Input (Instruction + Question):**  \n",
    "\"Explain step-by-step how to solve the equation 2x + 3 = 9.\"\n",
    "\n",
    "Here are two possible responses that are sampled from the SFT-tuned model\n",
    "\n",
    "**Candidate Outputs for the same input:**\n",
    "\n",
    "- **Output A:**  \n",
    "\"Step 1: Subtract 3 from both sides: 2x = 6. Step 2: Divide both sides by 2: x = 3.\"  \n",
    "(Concise and logically correct.)\n",
    "\n",
    "- **Output B:**  \n",
    "\"Subtract 3 from both sides and divide by 2, so x = 3. This is because math.\"  \n",
    "(Vague and incomplete reasoning.)\n",
    "\n",
    "**Reward Signal:**  \n",
    "Output A is *preferred* and given a higher reward;\n",
    "\n",
    "Output B is penalized for lack of detailed and correct reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is another one that we previously used\n",
    "\n",
    "**Input:**  \n",
    "\"Explain why the Earth revolves around the Sun.\"\n",
    "\n",
    "**Candidate Outputs:**\n",
    "\n",
    "- **Output A:**  \n",
    "\"The Earth revolves around the Sun due to the gravitational force described by Newton's law of universal gravitation, where the Sun's mass exerts a force on Earth keeping it in orbit.\"\n",
    "\n",
    "- **Output B:**  \n",
    "\"The Sun is big and bright, so Earth moves around it.\"\n",
    "\n",
    "**Reward:**  \n",
    "Output A receives higher reward for scientifically accurate and logically sound reasoning, refining the correctness beyond SFT’s imitation.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now can use GRPO\n",
    "- to RL train the model\n",
    "- to produce reasoning responses\n",
    "\n",
    "that meet both format, correctness, and level-of-detail criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison: SFT, RL, RFT\n",
    "\n",
    "\n",
    "SFT and RL are different methods for fine tuning an LLM.\n",
    "- RFT combines an initial SFT with a subsequent RL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The distinction becomes every blurrier\n",
    "- when RL has intermediate rewards\n",
    "- rather than a single trajectory reward\n",
    "\n",
    "It is sometimes possible to cast a task into a form appropriate for either SFT or RL with per-step rewards\n",
    "- Next token prediction\n",
    "    - SFT: Cross Entropy Loss for every step\n",
    "    - RL: Per-step reward\n",
    "        - +1 reward for correct prediction/0 for incorrect prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But the choice of which method to use is often dependent on\n",
    "- the task\n",
    "- the available training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is hard to be precise, but here are some thematic comparisons.\n",
    "\n",
    "SFT \n",
    "- encourages imitation of  the label of an example\n",
    "    - exact match\n",
    "- enforces formatting/structure of response\n",
    "- \"surface\" level correctness\n",
    "- well-suited to precisely-defined tasks\n",
    "    - with *objective* measures of success\n",
    "    - quantitative measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RL \n",
    "- allows multiple \"correct\" answers\n",
    "    - which may be ranked\n",
    "- \"deeper\" understanding/generalization\n",
    "- well-suited to more loosely-defined tasks\n",
    "    - with *subjective* measures of success\n",
    "    - *qualitative* measures\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In terms of training data\n",
    "- SFT imitation requires *lots* of training examples\n",
    "    - exploration of alternatives doesn't come into play\n",
    "- RL can often be accomplished in a very small number of training examples\n",
    "    - exploration encourage\n",
    "    \n",
    "SFT and RL are *complementary* methods for fine tuning an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Criteria                 | SFT                                                        | RFT/RLHF                                             |\n",
    "|:-------------------------|:-----------------------------------------------------------|:-----------------------------------------------------|\n",
    "| Task type                | Objective, well-defined, clear correct answer tasks         | Subjective, ambiguous, or value-laden tasks          |\n",
    "| Data availability        | Large, high-quality labeled datasets available              | Little/no labeled data, but feedback/preference signals are available |\n",
    "| Training complexity      | Simpler (labeled pairs)                                    | More complex (reward model, RL optimization)         |\n",
    "| Desired outcome          | Accuracy, task performance, factual correctness             | Human preference alignment, style, quality, safety   |\n",
    "| Overfitting risk         | Higher, if data is limited                                 | Lower; learns general behavior from rewards          |\n",
    "| Generalization           | Prone to memorization                                      | Promotes adaptability, nuanced behaviors             |\n",
    "| Cost/resource needs      | Lower; less human-in-the-loop need                         | Higher; human feedback collection and more computation|\n",
    "| Ideal use cases          | Translation, classification, summarization, retrieval      | Chatbots, open-domain QA, content moderation, dialog |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is one rubric:\n",
    "\n",
    "<table>\n",
    "<img src=\"images/rft_vs_sft_decision.png\" width=90%>\n",
    "     \n",
    " Reference: https://predibase.com/blog/how-reinforcement-learning-beats-supervised-fine-tuning-when-data-is-scarce\n",
    "<table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**References for RFT vs SFT**\n",
    "\n",
    "- [Why Reinforcement Learning Beats SFT with Limited Data - Predibase](https://predibase.com/blog/how-reinforcement-learning-beats-supervised-fine-tuning-when-data-is-scarce)\n",
    "- [Preference Alignment vs Supervised Fine-Tuning in LLM Training](https://www.rohan-paul.com/p/preference-alignment-vs-supervised)\n",
    "- [Supervised Fine-Tuning vs. RLHF: How to Choose the Right Approach](https://www.invisible.co/blog/supervised-fine-tuning-vs-rlhf-how-to-choose-the-right-approach-to-train-your-llm)\n",
    "- [Fine-Tuning vs RLHF: Choosing the Best LLM Training Method](https://cleverx.com/blog/supervised-fine-tuning-vs-rlhf-choosing-the-right-path-to-train-your-llm)\n",
    "- [What is supervised fine-tuning? - BlueDot Impact](https://bluedot.org/blog/what-is-supervised-fine-tuning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Process Reward Model (PRM) vs Outcome Reward Model (ORM)\n",
    "\n",
    "Outcome Reward Model (ORM) = Trajectory Reward\n",
    "- single reward at end of trajectory\n",
    "\n",
    "Process Reward Model (PRM) = step by step reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References for Process Reward Models vs Outcome Reward Models**\n",
    "\n",
    "- [A Comprehensive Survey of Reward Models: Taxonomy and Applications](https://arxiv.org/html/2504.12328v1)\n",
    "- [Reward Modeling | RLHF Book by Nathan Lambert](https://rlhfbook.com/c/07-reward-models.html)\n",
    "- [Let’s Verify Step by Step (OpenAI, Process Supervision)](https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf)\n",
    "- [Getting LLMs To Reason With Process Rewards](https://patmcguinness.substack.com/p/getting-llms-to-reason-with-process)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "| Title (linked)                                                                                                                           | Commentary                                                                                                                                                          |\n",
    "|:-----------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [InstructGPT: Aligning Language Models with Human Intent via RLHF](https://arxiv.org/abs/2203.02155)                                      | Foundational paper laying out the RLHF approach to align LLMs with human intent using human preference data. Essential for understanding RLHF theory and practice. |\n",
    "| [A Survey on Post-Training of Large Language Models](https://arxiv.org/abs/2503.06072)                                                    | Comprehensive survey reviewing SFT, RLHF, and newer alignment methods. Synthesizes research trends and challenges in LLM post-training.                          |\n",
    "| [Reinforcement Learning from AI Feedback (RLAIF): A Scalable Alternative to RLHF](https://arxiv.org/abs/2309.00267)                      | Introduces RLAIF, replacing human feedback with AI-generated feedback for scalable alignment. Critical for understanding automated feedback approaches.           |\n",
    "| [Constitutional AI: Harmlessness from AI Feedback](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback)   | Proposes Constitutional AI, using a fixed ethical constitution for AI self-critique and revision to improve alignment without human labels.                       |\n",
    "| [LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://arxiv.org/abs/2502.21321)                                   | Examines post-training methods focused on improving reasoning in LLMs via SFT and RL, analyzing mechanics and challenges.                                         |\n",
    "| [SFT Memorizes, RL Generalizes: A Comparative Study of Post-Training Methods for LLMs](https://arxiv.org/abs/2501.17161)                  | Empirically compares SFT and RL in LLMs, showing SFT excels at memorization while RL generalizes better and improves alignment.                                    |\n",
    "| [How Reinforcement Learning Beats Supervised Fine-Tuning When Data Is Scarce](https://predibase.com/blog/how-reinforcement-learning-beats-supervised-fine-tuning-when-data-is-scarce) | Blog explaining why RL methods can outperform SFT in low-data regimes; offers practical insights for training efficiency.                                        |\n",
    "| [Beyond Next-Token Prediction: How Post-Training Teaches LLMs to Reason](https://toloka.ai/blog/how-post-training-teaches-llms-to-reason/) | Discusses how combining SFT and RL post-training enables complex reasoning in LLMs, with examples and experimental findings.                                      |\n",
    "| [Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)                                       | Blog unpacking the roles of SFT and RL in reasoning capability development; bridges theory and practice with clear explanations.                                  |\n",
    "| [RLHF vs RLAIF: A Detailed Comparison of AI Training Methods](https://www.sapien.io/blog/rlaif-vs-rlhf-understanding-the-differences)     | Detailed comparison of RLHF and RLAIF approaches, illustrating differences in feedback sources and workflows for AI alignment.                                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9 (new)",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
