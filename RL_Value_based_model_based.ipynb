{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Model-based Value methods\n",
    "\n",
    "\n",
    "We will study two common Model-based, Dynamic Programming type methods\n",
    "- Value Iteration\n",
    "- Policy Iteration\n",
    "\n",
    "and their associated Bellman equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Value iteration method\n",
    "\n",
    "The simplest method is to\n",
    "- iteratively update the Value function\n",
    "    - until convergence\n",
    "- derive the *final* Policy from the Value function\n",
    "    - chose the action leading to highest return\n",
    "    - based on the Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The estimate of the value of a state $\\state$ is improved iteratively.\n",
    "\n",
    "Let \n",
    "$$\n",
    "\\statevalfun_{k}(\\state) \n",
    "$$\n",
    "\n",
    "denote the *estimated* value of state $\\state$ after $k$ iterations.\n",
    "\n",
    "In iteration $k+1$, the value of *each* state $\\state \\in \\States$ is updated via the Bellman update\n",
    "\n",
    "\n",
    "$$\n",
    "\\statevalfun_{k+1}(\\state) = \\max{\\act} \\sum_{\\state'} \\transp(\\state' \\mid \\state, \\act)\\bigl(\\rew(\\state,\\act,\\state') + \\gamma \\statevalfun_k(\\state')\\bigr).\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\rew(\\state,\\act,\\state')\n",
    "$$\n",
    "\n",
    "denotes the reward returned by the environment when action $\\act$ in state $\\state$\n",
    "results in successor state $\\state'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is\n",
    "- $\\statevalfun_{k+1}(\\state)$ \n",
    "- learns the *previous iteration's $(k)$* estimate $$\\statevalfun_k(\\state')$$  \n",
    "of each successor state $\\state'$\n",
    "- and chooses the action leading to highest return\n",
    "\n",
    "The backup in each iteration\n",
    "- moves information about potential future (successor) states\n",
    "- to the current state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The previous iteration's estimate of the successors of $\\state$\n",
    "$$\\statevalfun_k(\\state')$$  \n",
    "\n",
    "in turn, are based on the iteration $(k-1)$ values of the successors of $\\state'$\n",
    "\n",
    "So we need $i$ iterations\n",
    "- in order for information of states $i$ steps ahead\n",
    "- to reach state $s$\n",
    "\n",
    "Hence the need to repeat until convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Subtlety**\n",
    "\n",
    "Prior to convergence\n",
    "- $\\statevalfun_k(\\state)$ is a *non-random* variable\n",
    "    - $\\transp(\\state, \\act)$ is *known* for all $\\state, \\act$ since method is model-based\n",
    "- the estimate is *biased*\n",
    "    - it is non-random, but not yet correct\n",
    "\n",
    "We will subsequently contrast this to the iterative estimates produced by model-free methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pseudo code for Value Iteration**\n",
    "\n",
    "Here is some pseudo-code\n",
    "\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "    <center><strong>Value iteration</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/RL_value_iteration_alg.png\" width=90%>\n",
    "    </tr>\n",
    "    \n",
    "\n",
    "</table>\n",
    "\n",
    "Attribution: http://incompleteideas.net/book/RLbook2020.pdf#page=105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Value iteration</strong></center>\n",
    "\n",
    "    Initialize value function V(s) arbitrarily (e.g., zero for all states)\n",
    "\n",
    "    Repeat:\n",
    "        delta = 0\n",
    "        For each state s:\n",
    "            old_value = V(s)\n",
    "            V(s) = max over a [ R(s, a) + γ * sum over s' [ P(s' | s, a) * V(s') ] ]\n",
    "            delta = max(delta, |old_value - V(s)|)\n",
    "\n",
    "        Until delta < threshold\n",
    "\n",
    "    # Derive policy after value function converges\n",
    "    For each state s:\n",
    "        π(s) = argmax over a [ R(s, a) + γ * sum over s' [ P(s' | s, a) * V(s') ] ]\n",
    "\n",
    "    Return π, V\n",
    "    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Policy iteration method\n",
    "\n",
    "The Value Iteration method has one expensive step\n",
    "- the $\\max{\\act}$ is a search over all possible actions $\\act \\in \\Actions$\n",
    "\n",
    "$$\n",
    "\\statevalfun_{k+1}(\\state) = \\max{\\act} \\sum_{\\state'} \\transp(\\state' \\mid \\state, \\act)\\bigl(\\rew(\\state,\\act,\\state') + \\gamma \\statevalfun_k(\\state')\\bigr).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Policy Iteration is a less computationally-expensive way to reach the optimal $\\statevalfun$.\n",
    "\n",
    "Rather than updating the Policy once (after Value function convergence)\n",
    "- we introduce a method that periodically updates the Policy.\n",
    "\n",
    "\n",
    "*Policy iteration* is an algorithm that improves $\\pi_p$ to $\\pi_{p+1}$ by alternating two steps\n",
    "during round $p$\n",
    "\n",
    "The algorithm alternates between\n",
    "- Policy evaluation\n",
    "    - update the estimate of $\\statevalfun_{\\pi_p}$ to $\\statevalfun_{\\pi_{p+1}}$\n",
    "- Policy improvement\n",
    "    - update $\\pi_p$ to $\\pi_{p+1}$ using the newly updated $\\statevalfun_{\\pi_{p+1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It uses the Bellman update\n",
    "\n",
    "$$\n",
    "\\statevalfun_{\\pi,k+1}(\\state) =  \\sum_{\\state'} \\transp(\\state' \\mid \\state, \\pi(\\state))\\bigl(\\rew(\\state,\\pi(\\state),\\state') + \\gamma \\statevalfun_{\\pi,k}(\\state')\\bigr).\n",
    "$$\n",
    "\n",
    "where\n",
    "- we add an explicit subscript $\\pi$\n",
    "- to the value function\n",
    "$$\n",
    "\\statevalfun_{\\pi,k+1}(\\state)\n",
    "$$\n",
    "- to indicate the dependence of the update on the *current policy* $\\pi$\n",
    "\n",
    "Rather than\n",
    "- evaluating *all* actions $\\act \\in \\Actions$\n",
    "- it chooses a single action $\\state,\\pi(\\state)$ according to the current policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is some pseudo-code\n",
    "\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "    <center><strong>Policy iteration</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/RL_policy_iteration_alg.png\" width=90%>\n",
    "    </tr>\n",
    "    \n",
    "\n",
    "</table>\n",
    "\n",
    "Attribution: http://incompleteideas.net/book/RLbook2020.pdf#page=102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pseudo code for Policy Iteration**\n",
    "\n",
    "<table>\n",
    "    <center><strong>Policy iteration</strong></center>\n",
    "\n",
    "    Initialize policy π arbitrarily (e.g., random policy)\n",
    "    Initialize value function V(s) arbitrarily (e.g., zero for all states)\n",
    "\n",
    "    Repeat:\n",
    "        # Policy Evaluation (compute V for current policy π)\n",
    "        Repeat:\n",
    "            For each state s:\n",
    "                V(s) = R(s, π(s)) + γ * sum over s' [ P(s' | s, π(s)) * V(s') ]\n",
    "            Until V(s) converges (changes smaller than threshold)\n",
    "\n",
    "        # Policy Improvement (update policy based on current V)\n",
    "        policy_stable = True\n",
    "        For each state s:\n",
    "            old_action = π(s)\n",
    "            π(s) = argmax over a [ R(s, a) + γ * sum over s' [ P(s' | s, a) * V(s') ] ]\n",
    "            if old_action != π(s):\n",
    "                policy_stable = False\n",
    "\n",
    "    Until policy_stable is True\n",
    "\n",
    "    Return π, V\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both Value Iteration and Policy Iteration will converge to the same policy.\n",
    "\n",
    "The advantage of alternating between Policy Evaluation and Policy Improvement\n",
    "- faster convergence \n",
    "    - the Value function under the current policy is fully evaluated\n",
    "    - before the Policy is updated\n",
    "- more stable convergence\n",
    "    - Policy doesn't change until Value function (under current policy) is fully known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the best action in a Value-based method\n",
    "\n",
    "The Value-based methods don't directly give you a policy\n",
    "- the Value function gives you the best successor state $\\state'$ from current state $\\state$\n",
    "- **but** it doesn't directly tell you the action $\\act$ that leads to $\\state'$\n",
    "\n",
    "In order to find $\\act$ you either\n",
    "- need a model\n",
    "    - search over all possible actions, using the model to determine the value of action's successor state\n",
    "- use actual experience to estimate the effect of performing action $\\act$ in state $\\state$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is some pseudo-code that uses a model to determine the best action:\n",
    "\n",
    "    # For each possible action a in current state s:\n",
    "    for each action a in actions:\n",
    "        # Take action a from state s in the environment\n",
    "        observe next state s', reward r\n",
    "        # Estimate value for taking action a from s\n",
    "        A[a] = r + gamma * V[s']\n",
    "    # Find the maximum estimated action value\n",
    "    A_max = max over a of A[a]\n",
    "    \n",
    "    # Update the value function for state s\n",
    "    V[s] = V[s] + alpha * (A_max - V[s])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Q-learning: From Value function to State-Action function\n",
    "\n",
    "We can more readily identify the optimal *action* to take from state $\\state$\n",
    "- the one leading to $\\state'$ with maximal $\\statevalfun(\\state')$\n",
    "\n",
    "with slightly more detailed bookkeeping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Define\n",
    "a *State Value function* $\\actvalfun_\\pi(\\state, \\act)$ \n",
    "$$\n",
    "\\actvalfun_{\\pi}: \\States \\times \\Actions \\to \\Reals\n",
    "$$\n",
    "\n",
    "to map a state/action pair $(\\state, \\act)$ to the expected value of the successor state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Re-write the Value function backup equation\n",
    "$$\n",
    "\\statevalfun_{k+1}(\\state) = \\max{\\act} \\sum_{\\state'} \\transp(\\state' \\mid \\state, \\act)\\bigl(\\rew(\\state,\\act,\\state') + \\gamma \\statevalfun_k(\\state')\\bigr).\n",
    "$$\n",
    "\n",
    "as\n",
    "\n",
    "$$\n",
    "\\statevalfun_{k+1}(\\state) = \\max{\\act} \\actvalfun_k(\\state, \\act)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\actvalfun(\\state,\\act) = \\sum_{\\state'} \\transp(\\state' \\mid \\state, \\act)\\bigl(\\rew(\\state,\\act,\\state') + \\gamma \\statevalfun_k(\\state')\\bigr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Differences Between Value Iteration and Policy Iteration\n",
    "\n",
    "Here is a comparison of the Value and Policy Iteration methods.\n",
    "\n",
    "| Feature              | Value Iteration                                                      | Policy Iteration                                    |\n",
    "|:----------------------|:---------------------------------------------------------------------|:-----------------------------------------------------|\n",
    "| Approach             | Updates value function until convergence                            | Alternates between value evaluation and improvement |\n",
    "| Convergence          | When value function $V(s)$ stabilizes                               | When policy $\\pi(s)$ stops improving                |\n",
    "| Computational Cost   | Higher per iteration, simpler logic                                 | Lower per iteration, more complex                   |\n",
    "| Speed                | Requires more iterations                                            | Fewer iterations; often faster                      |\n",
    "| Policy Output        | Extracted after value convergence                                   | Updated during each iteration                       |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Feature | Value Iteration | Policy Iteration |\n",
    "| :-- | :-- | :-- |\n",
    "| **Approach** | Updates value function iteratively using the Bellman Optimality Equation in one step | Alternates between full policy evaluation and policy improvement steps |\n",
    "| **Policy Handling** | Policy is implicitly updated after value convergence | Explicitly maintains and updates policy each iteration |\n",
    "| **Initialization** | Starts with an initial value function | Starts with an initial policy |\n",
    "| **Iteration Steps** | Single step combining evaluation and improvement | Two-step process: separate evaluation and improvement |\n",
    "| **Convergence Criterion** | Value function converges (changes below a threshold) | Policy stabilizes (no change between iterations) |\n",
    "| **Computation per Iteration** | Potentially more expensive per iteration (max over all actions for each state) | More computationally intensive due to full policy evaluation, but fewer total iterations needed |\n",
    "| **Number of Iterations** | Typically more iterations | Usually fewer iterations |\n",
    "| **Complexity** | Simpler to implement | More complex implementation |\n",
    "| **Suitability** | Suitable for smaller state spaces or when full policy evaluation is expensive | Can handle larger state spaces more efficiently when full evaluation is feasible |\n",
    "| **Policy Updates** | Policy derived after convergence of value function | Policy updated after each evaluation phase |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model-based Value methods: Classical vs Deep Learning\n",
    "\n",
    "In \"Classical\" Model-base Value methods\n",
    "- the state and action spaces are finite\n",
    "- leading to methods based on tables\n",
    "\n",
    "Dynamic Programming style solutions are efficient methods for exploring the finite possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But non-finite action and state spaces may be accomodated\n",
    "- by using Neural Networks (NN)\n",
    "- to *learn*\n",
    "    - an approximation of the model\n",
    "    - the value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Aspect                        | Classical Model-based RL (Tabular) | Model-based Value Method with NN                     |\n",
    "|:----------------------------- |:---------------------------------- |:---------------------------------------------------- |\n",
    "| State/action space            | finite                             | continuous/infinite (via approximation)              |\n",
    "| Model representation          | table or analytic function         | neural network (NN), e.g.,fϕf_\\\\phifϕ                |\n",
    "| Value function representation | table                              | NN:Vθ(s)V_\\\\theta(s)Vθ(s),Qθ(s,a)Q_\\\\theta(s,a)Qθ(s,a) |\n",
    "| Planning/Improvement          | Bellman backup over all states     | Rollouts/approximate backups, often via sampling     |\n",
    "| Example use cases             | gridworlds, small MDPs             | robotics, control, games with pixel input            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
