{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preferences vs Rewards\n",
    "\n",
    "There are problems where providing *exact scalar rewards*\n",
    "- is harder than *ranking* potential outputs.\n",
    "\n",
    "For example\n",
    "- I may prefer chocolate to vanilla\n",
    "- but I can't quantify how much more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Technically\n",
    "- rewards form a total order\n",
    "    - a reward has a magnitude\n",
    "    - *all* rewards can be compared and ordered\n",
    "- preferences form a partial order\n",
    "    - we can order *some* pairs of outputs\n",
    "    - without providing a magnitude\n",
    "    \n",
    "        Good > Bad\n",
    "        \n",
    "        Big  > Small\n",
    "        \n",
    "        Partial order: \n",
    "        \n",
    "            Good > Small ?   undefined\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Problems related to aligning the *style* of an LLM's output\n",
    "is a case of preferences.\n",
    "- multiple answers may be \"correct\"\n",
    "- but one answer may be \"preferred\"\n",
    "\n",
    "For example\n",
    "\n",
    "**Prompt:** \"How do I change a tire?\"\n",
    "- **Reply A:** An accurate step-by-step answer.\n",
    "- **Reply B:** A brief, incomplete answer.\n",
    "\n",
    "Both replies are \"correct\" but the first is subjectively better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An example of *Preference Data* is a triple\n",
    "\n",
    "$$\n",
    "(x, y^+, y^-)\n",
    "$$\n",
    "- input $x$\n",
    "- the preferred output $y^+$\n",
    "- the non-preferred output $y^-$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although both $y^+$ and $y^-$ may be *acceptable* responses, $y^+$ is preferred.\n",
    "\n",
    "It is the *qualitative* nature of preferences (rather than correctness)\n",
    "- that makes Reinforcement Learning a better tool for alignment with Prererences\n",
    "- as opposed to Supervised Fine Tuning\n",
    "\n",
    "In this module, we explore Reinforcement Learning for Preference Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The case for preferences\n",
    "\n",
    "| Scenario                                   | Why Preference Data?                     | Typical Example            |\n",
    "|:---------------------------------------------|:------------------------------------------|:----------------------------|\n",
    "| RLHF & LLM alignment                       | Human feedback easier as comparisons     | Choosing better LLM output |\n",
    "| Hard-to-define or subjective “success”      | Preference judgments more reliable       | Dialogue, safety, style    |\n",
    "| Biased or noisy scalar rewards              | Preferences less affected by outliers    | Creative tasks, open-ended |\n",
    "| Interpretability needs                      | Preferences can include rationales       | Transparent value alignment|\n",
    "| DPO-style methods                          | Direct optimization over preferences     | Pairwise/choice based loss |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of a Preference Dataset \n",
    "\n",
    "Here is a [link](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n",
    "to the UltraFeedback dataset.\n",
    "\n",
    "It is used to train an Assistant to be\n",
    "- Helpful\n",
    "    - answers the user's prompt; doesn't evade or decline\n",
    "- Honest\n",
    "    - gives a truthful answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The methodology for constructing it is given [here](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized#dataset-card-for-ultrafeedback-binarized).\n",
    "\n",
    "- The authors gather a number of prompts across multiple domains.\n",
    "\n",
    "- An AI assistant is asked to proved multiple responses to a prompt\n",
    "\n",
    "- A second AI assistant\n",
    "    - critiques the response based on, e.g., the Helpfulness criteria\n",
    "    - gives a numerical evaluation\n",
    "\n",
    "- Which creates a ranking of the responses to a prompt\n",
    "\n",
    "**Synthetic Data** in practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"binarized\" dataset that we viewed\n",
    "- selects the highest ranked answer as \"Chosen\"\n",
    "- randomly selects the other responses as \"Rejected\"\n",
    "\n",
    "Note the use of AI feedback rather than Human Feedback.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LLM Next Token prediction task and Reinforcement Learning\n",
    "\n",
    "One motivation for Preference Methods\n",
    "- is to post-train an LLM\n",
    "- to exhibit desirable characteristics\n",
    "- which is often expressed with Preference Data\n",
    "    - Preferred output vs. Non-Preferred output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We translate the LLM \"Next Token Prediction\" task to an instance of Reinforcement Learning\n",
    "- in order to be able to use Reinforcement Learning for post-training an LLM\n",
    "\n",
    "The Language Modeling task, formally, is\n",
    "- Predict the Next Token conditional on the previously generated tokens of the response\n",
    "- producing output $y$ given input $x$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For each output sequence $\\y$ of tokens \n",
    "- We can associate a *state* corresponding to *each prefix* of $y$\n",
    "    - e.g, $\\stateseq_\\tt = \\y_{[0:\\tt-1]}$\n",
    "- An action is\n",
    "    - choosing a token from the Vocabulary as the next output\n",
    "    \n",
    "- The policy $\\pi_\\theta( \\act | \\state)$ is thus equivalent to\n",
    "$$\\pi_\\theta(\\y_\\tp | \\y_{[0:\\tt-1]})$$\n",
    "\n",
    "- the probability distribution of the next token, conditional on the prefix\n",
    "\n",
    "Each sequence $\\y$ becomes an episode/trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can write the probability $\\pi_\\theta( \\y | \\x ) $\n",
    "- of the *entire sequence* $\\y$\n",
    "- as the chained probabilities of each action given a state\n",
    "\n",
    "$$\n",
    "\\pi_\\theta( \\y | \\x ) = \\prod_{\\tt=1}^T { \\pi( \\y_\\tp | \\y_{[0:\\tt-1]} ) }\n",
    "$$\n",
    "\n",
    "This will be convenient \n",
    "- in that we can compare the probabilities of\n",
    "- a Preferred response $y^+$ to a Non-Preferred response $y^=$\n",
    "\n",
    "rather than having to write the chained probability of each token in the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9 (new)",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
