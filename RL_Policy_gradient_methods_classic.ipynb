{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy gradient based methods: concepts\n",
    "\n",
    "Recall the Policy Gradient Theorem\n",
    "- formulated with single reward $\\rewseq(\\tau)$ for the entire trajectory\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \n",
    "\\Exp{\\tau \\sim \\pr{\\theta}} { \n",
    "\\sum_{\\tt=0}^{|\\tau|} {\n",
    "\\nabla_\\theta   \\log \\pi(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,\\tt}) ) \\, \\,\\rewseq(\\tau)\n",
    "}\n",
    "} \n",
    "$$\n",
    "\n",
    "- formulated with intermediate rewards\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{\\tt=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,t}) G_{\\tau,\\tt} \\right]\n",
    "$$\n",
    "\n",
    "Policy based methods\n",
    "update parameters $\\theta$\n",
    "- in the direction (gradient)\n",
    "- that increases expected return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantage vs Return/Reward: Baselines\n",
    "\n",
    "Suppose all returns/rewards are positive.\n",
    "\n",
    "The Policy Gradient Theorem would suggest that we update parameters\n",
    "- to favor *all* actions\n",
    "- with positive gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But consider two actions at some step $\\tt$\n",
    "- one with a *very large* positive return/reward\n",
    "- one with a *very small* positive return/reward\n",
    "\n",
    "We probably want to favor the action with large reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, we want to *relativize* the return/reward by comparing it to a *baseline* for the state.\n",
    "\n",
    "The relativized value of an action is called the *advantage*.\n",
    "\n",
    "Often, the advantage is derived from the action's return/reward by\n",
    "- subtracting a *baseline*\n",
    "- where the baseline value\n",
    "    - is a function of *all* possible actions from the state\n",
    "    - usually $\\statevalfun_\\pi ( \\stateseq_{\\tt})$, the value of the state as would be computed by a Value function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example: consider a baseline that is the average return/reward from the state\n",
    "- above average returns have positive advantage\n",
    "- below average returns have a negative advantage\n",
    "\n",
    "Thus, in the case of all returns/rewards being positive\n",
    "- we favor actions that result in positive advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Subtracting a baseline from the return/reward has other advantages\n",
    "- reduces the *magnitude* of the parameter update\n",
    "    - smoother training\n",
    "- reduces \"noise\"\n",
    "    - the baseline value for a state is common to *all actions* from the state\n",
    "    - so the advantage is relative to the signal *only* from the action itself\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Aspect                    | Effect of Subtracting Baseline                         |\n",
    "|:---------------------------|:-------------------------------------------------------|\n",
    "| Noise cancellation        | Removes expected (common) return, reducing fluctuations |\n",
    "| Bias introduction         | None, baseline does not depend on action              |\n",
    "| Zero-centering            | Advantages centered around zero, stabilizing updates  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unified Policy Gradient Formulation: Advantage Definitions\n",
    "\n",
    "Using the concept of Advantage, we can rewrite the Policy Gradient Theorem\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{\\tt=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,t}) \\advseq_{\\tau,\\tt} \\right]\n",
    "$$\n",
    "\n",
    "The form of the advantage might vary depending on whether there is\n",
    "- a single per-trajectory reward\n",
    "- or intermediate rewards.\n",
    "\n",
    "We can characterize many policy-based methods\n",
    "- by defining their advantage calculation\n",
    "$$\n",
    "\\advseq_{\\tau,\\tt} \n",
    "$$\n",
    "\n",
    "We thus refer to the above formulation as the Unified Policy Gradient Formulation.\n",
    "\n",
    "The policy-based methods we present will typically be variations of this \"vanilla\" gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Surrogate Loss\n",
    "\n",
    "Rather than\n",
    "- maximization of return\n",
    "\n",
    "Policy methods often switch to\n",
    "- minimization of a *surrogate loss*\n",
    "\n",
    "The surrogate loss often **imposes constraints** on the derived policy and  on the solution process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Policy Gradient Theorem provides\n",
    "- the *theoretical* basis for an optimal policy\n",
    "- using the Returns or Advantages\n",
    "\n",
    "The Surrogate Loss provides\n",
    "- the *practical* objective for finding the optimal policy\n",
    "- subject to practical constraints: stability, convergence\n",
    "\n",
    "The actual implementation of our models will\n",
    "usually revert to the Minimization of Surrogate Loss formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some practical constraints limit policy changes\n",
    "- prevents drastic policy shifts after a parameter update\n",
    "- by constraining the new policy to be close to a \"reference\" policy\n",
    "\n",
    "Other constraints try to promote stability and convergence in the solution process\n",
    "- limiting the magnitude of a gradient update\n",
    "- promoting low variance of the gradient updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Constraints**\n",
    "\n",
    "| Reason                          | Explanation                                               |\n",
    "|:---------------------------------|:-----------------------------------------------------------|\n",
    "| Trust Region Constraints        | Avoid overly large policy updates that destabilize learning |\n",
    "| Regularization                  | KL divergence or clipping adds penalty to maintain stability  |\n",
    "| Stability & Convergence         | Ensures smooth, incremental improvement of the policy       |\n",
    "| Computational Tractability      | Easier to optimize surrogate than raw return objective      |\n",
    "| Exploration-Exploitation Tradeoff | Clipping controls how much policy can change per step      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Relationship between Policy Gradient Theorem and Surrogate Loss**\n",
    "\n",
    "| Aspect                  | Policy Gradient Theorem                                     | Surrogate Loss                                               |\n",
    "|:------------------------|:------------------------------------------------------------|:-------------------------------------------------------------|\n",
    "| Role                    | Theoretical formula for expected return gradient            | Practical approximation/objective for policy update          |\n",
    "| Gradient                | Directly gives unbiased gradient of $J(\\theta)$             | Its gradient approximates policy gradient but includes stabilizing terms |\n",
    "| Constraints             | None intrinsic; pure gradient formula                        | Includes clipping, penalties to enforce trust region and avoid large steps |\n",
    "| Use in Algorithms       | Foundation for policy gradient methods                       | Used in PPO-style algorithms to compute safe gradient steps   |\n",
    "| Goal                    | Maximize expected return $J(\\theta)$                         | Provide stable, incremental improvement proxy                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a preview of the Surrogate Losses of the policy-based methods we will explore.\n",
    "\n",
    "| Method     | Surrogate Return/Objective Used for Gradient                | Credit Assignment         |\n",
    "|:------------|:------------------------------------------------------------|:--------------------------|\n",
    "| REINFORCE  | $G_t$ (return-to-go from $t$)                          | Monte Carlo trajectory return |\n",
    "| PPO        | Clipped ratio times $A_t$ (advantage)                    | Advantage-based, stable  |\n",
    "| GRPO       | Relative advantages over candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy gradient based methods: Preview\n",
    "\n",
    "We will show several common policy-based methods.\n",
    "- REINFORCE\n",
    "- PPO\n",
    "- GRPO (relatively new: 2024)\n",
    "\n",
    "Details of each follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a preview, we show the definition of the Advantage for each.\n",
    "\n",
    "\n",
    "| Method  | Intermediate Reward Advantage $\\advseq_{\\tau, \\tt}$      | Single Trajectory Reward Advantage $\\advseq_\\tau$         |\n",
    "|:---------|:--------------------------------------------|:--------------------------------------------------|\n",
    "| REINFORCE | $\\advseq_t = G_t - b_t$                        | $\\advseq = R(\\tau) - b$                              |\n",
    "| PPO     | $\\advseq_t = \\hat{A}_t$ (e.g., GAE estimator) | $\\advseq = R(\\tau) - b$ applied per step             |\n",
    "| GRPO    | Relative advantages per candidate           | Same per-candidate advantage applied per step    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And a comparison:\n",
    "\n",
    "| Method | Gradient-Based | Main Objective | Key Characteristics | Stability & Sample Efficiency | Typical Application Domains |\n",
    "|:--------|:----------------|:----------------|:---------------------|:------------------------------|:-----------------------------|\n",
    "| **PPO** (Proximal Policy Optimization) | Yes | Maximize expected reward with clipped surrogate objective | Uses policy gradients with clipping to limit policy update magnitude, balancing exploration and exploitation | High stability; more sample efficient than vanilla policy gradients; widely used for continuous and discrete control tasks | Robotics, games, continuous control, benchmark RL tasks |\n",
    "| **DPO** (Direct Preference Optimization) | Yes | Directly optimize policy based on preference data | Uses a preference-based loss to train policy without explicit reward modeling; bypasses traditional RL complexities | Improved stability by leveraging human preferences; avoids some issues of reward misspecification | Alignment of language models with human preferences, NLP-focused RL |\n",
    "| **GRPO** (Group Relative Policy Optimization) | Yes | Optimize policy using group-relative advantage estimates | Does not require a separate value function; updates policy based on relative advantages within a group of candidate outputs | More memory efficient, stable; effective for large-scale policy optimization with reduced critic reliance | Training large language models, large-scale policy optimization |\n",
    "| **REINFORCE** | Yes | Maximize expected cumulative reward by direct policy gradient | Uses Monte Carlo sampled returns, applies likelihood ratio trick; pure policy gradient without value function | High variance and sample inefficient; simpler but less stable than actor-critic or PPO | Fundamental policy gradient algorithm, baseline for many RL studies |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# REINFORCE\n",
    "\n",
    "REINFORCE is a method that is very close to the Policy Gradient Theorem formulation\n",
    "- uses Advantage rather than the episode Return\n",
    "- the Advantage subtracts a \"baseline\" from the episode Return\n",
    "\n",
    "| Method  | Intermediate Reward Advantage $A_{\\tau, \\tt}$      | Single Trajectory Reward Advantage $A_\\tau$         |\n",
    "|:---------|:--------------------------------------------|:--------------------------------------------------|\n",
    "| REINFORCE | $\\advseq_\\tt = G_\\tt - b_\\tt$                        | $\\advseq = R(\\tau) - b$                              |\n",
    "|\n",
    "\n",
    "The purpose of subtracting a baseline is to\n",
    "- reduce the magnitude of the gradients\n",
    "- reduces the variance of the gradients\n",
    "- and hence: the change in policy parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In  single,  end-of-episode reward, the Advantage is typically computed as\n",
    "$$\n",
    "\\advseq_{\\tau,\\tt} = \\left( \\rewseq(\\tau) - b \\right)\n",
    "$$\n",
    "- $b$ is often the moving average (over trajectories $\\tau$) of $\\rewseq(\\tau)$ \n",
    "- advantage is the same for every step $\\tt$\n",
    "\n",
    "With intermediate rewards, the Advantage is typically computed as\n",
    "$$\n",
    "\\advseq_{\\tau,\\tt} = \\left( G_{\\tau, \\tt} - b_\\tt \\right)\n",
    "$$\n",
    "- where $b_\\tt$ is often a proxy for the *expected* Value function $\\statevalfun_\\pi(\\stateseq_\\tt)$ of state $\\stateseq_\\tt$\n",
    "    \n",
    "\n",
    "\n",
    "It uses a Monte-Carlo method to estimate the gradient\n",
    "- based on a sample of trajectories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pseudo code for REINFORCE\n",
    "\n",
    "    # REINFORCE training for LLM\n",
    "    for prompt in training_prompts:\n",
    "        output = llm.generate(prompt)\n",
    "        reward = evaluate_output(output) # Human or automated score\n",
    "        logprob = llm.logprob(output, prompt)\n",
    "\n",
    "        # Monte Carlo policy gradient update (no critic)\n",
    "        baseline = compute_baseline() # Optional: running mean for variance reduction\n",
    "        loss = -logprob * (reward - baseline)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "REINFORCE is very close to the vanilla Policy Gradient.\n",
    "\n",
    "But it is considered high variance.\n",
    "\n",
    "Subsequent methods will take explicit steps to reduce variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that\n",
    "in the *mathematical* formulation of REINFORCE\n",
    "$$G_{\\tt, \\tau}$$\n",
    "\n",
    "which is part of the Advantage\n",
    "- is computed for *each trajectory* $\\tau$ independently\n",
    "- the stochastic Transition Probability for a given State/Action pair $(\\state, \\act)$\n",
    "$$\n",
    "\\transp({ \\state', \\rew | \\state, \\act }) \n",
    " = \n",
    "\\transp({ \\stateseq_{\\tt+1}, \\rewseq_{\\tt+1} | \\state = \\stateseq_\\tt, \\act = \\actseq_\\tt })\n",
    "$$\n",
    "\n",
    "is a *single sample* from a probability distribution for the State/Action pair $(\\state, \\act)$.\n",
    "\n",
    "\n",
    "This leads to *high variance* estimates of $G_{\\tt, \\tau}$\n",
    "\n",
    "**Note**\n",
    "\n",
    "we observe the *effect* of the probability, not its value directly because we are model-free\n",
    "- by summing up effects over multiple episodes: we can a probability-weighted sum\n",
    "- without observing the probabilities directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    \n",
    "In the *practical* implementation of REINFORCE\n",
    "- we estimate $G_{\\tt}$\n",
    "- over a *batch* of episodes $\\tau_1, \\ldots, $\n",
    "\n",
    "so we have *multiple samples*  from the probability distribution for the State/Action pair $(\\state, \\act)$.\n",
    "- hopefully leading to a lower variance approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PPO\n",
    "\n",
    "\n",
    "## Intutition\n",
    "\n",
    "PPO optimizes a **clipped surrogate** loss\n",
    "- which can be interpretted as variant of the standard policy gradient theorem.\n",
    "\n",
    "The goal of the surrogate objective is to control how rapidly the policy changes from epoch to epoch of training.\n",
    "- avoid large policy shifts\n",
    "- variance reduction\n",
    "- monotonic improvement of policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Surrogate Loss for PPO\n",
    "\n",
    "The Surrogate Loss for PPO is\n",
    "$$\n",
    "J_{\\mathrm{PPO}}(\\theta) = \\mathbb{E}_\\tt \\left[\n",
    "\\min{} \\left(\n",
    "r_\\tt(\\theta) \\hat{A}_\\tt,\\;\n",
    "\\mathrm{clip}\\left(r_\\tt(\\theta), 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{\\advseq}_\\tt\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "We will explain how this potentially intimidating equation came about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We express the \n",
    "- relative change in policy (for a given State/Action pair) \n",
    "- between epochs\n",
    "- with the *probability ratio*\n",
    "$$\n",
    "r_\\tt(\\theta) = \\frac{\\pi_\\theta(\\actseq_\\tt |\\stateseq_\\tt )}{\\pi_{\\theta_{\\rm old}}(\\actseq_\\tt |\\stateseq_\\tt)}\n",
    "$$\n",
    "\n",
    "where $\\pi_{\\theta_{\\rm old}}$ is the *reference policy*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The reference policy is the policy in effect at the start of each epoch of training\n",
    "- before this epoch modifies the policy parameters\n",
    "\n",
    "During an epoch of training\n",
    "- trajectories are generated  using the reference policy\n",
    "- the policy parameters are updated based on the surrogate loss for these trajectories\n",
    "- the updated policy becomes the reference policy for the next epoch\n",
    "\n",
    "By keeping the probability ratio close to $1$, we constrain the Gradient Ascent update step to a small change in policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to we keep the probability ratio close to $1$ ?\n",
    "- by using clipping to constrain it to the range $[1 - \\epsilon, 1 + \\epsilon ]$ for small $\\epsilon$\n",
    "\n",
    "this is the *clipped* part of the clipped surrogate objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formally\n",
    "\n",
    "The *surrogate loss* in PPO is:\n",
    "\n",
    "$$\n",
    "L_{\\text{sur}}(\\theta) = \\mathbb{E}_\\tt \\left[ r_\\tt(\\theta) \\hat{\\advseq}_\\tt \\right] \n",
    "= \\mathbb{E}_\\tt \\left[ \\frac{\\pi_\\theta(\\actseq_\\tt|\\stateseq_\\tt)}{\\pi_{\\theta_{\\rm old}}(\\actseq_\\tt|\\stateseq_\\tt)} \\hat{\\advseq}_\\tt \\right]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ r_\\tt(\\theta)$  is the probability ratio,\n",
    "- $\\hat{\\advseq}_t$ is the advantage estimate at step $t$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We modify the Surrogate Loss to limit the effect of the Probability Ratio.\n",
    "\n",
    "The *clipped surrogate loss* is\n",
    "\n",
    "$$\n",
    "L_{\\text{clip}}(\\theta) = \\mathbb{E}_\\tt \\left[ \\min{} \\left( r_\\tt(\\theta) \\hat{\\advseq}_\\tt, \\; \\mathrm{clip}\\left(r_\\tt(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{\\advseq}_\\tt \\right) \\right]\n",
    "$$\n",
    "\n",
    "By minimizing the clipped surrogate loss\n",
    "- we maximize $J(\\theta)$ (our goal in Gradient Ascent)\n",
    "- in a controlled manner\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relation to the Unified Gradient Formulation\n",
    "\n",
    "The (unclipped) Surrogate Loss does not resemble the  Unified Gradient Formulation.\n",
    "- absence of the term\n",
    "\n",
    "$$\n",
    "\\log \\pi_\\theta(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,t}) \n",
    "$$\n",
    "\n",
    "that multiplies the Advantage.\n",
    "\n",
    "Moreover,\n",
    " there is an extra term\n",
    "$$\n",
    "r_\\tt(\\theta) = \\frac{\\pi_\\theta(\\actseq_\\tt |\\stateseq_\\tt )}{\\pi_{\\theta_{\\rm old}}(\\actseq_\\tt |\\stateseq_\\tt)}\n",
    "$$\n",
    "\n",
    "in the form of the Probability Ratio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But, these terms appear inside the Expectation of the Gradient and, by using the Likelihood Ratio trick\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\theta r_\\tt(\\theta) & = & r_\\tt (\\theta) \\nabla_\\theta \\log r_\\tt (\\theta) & \\text{Likelihood Ratio trick} \\\\\n",
    "& = & r_\\tt (\\theta) \\nabla_\\theta \\left( \n",
    "    \\log \\pi_\\theta(\\actseq_\\tt|\\stateseq_\\tt) - \\log \\pi_{\\rm old} \\actseq_\\tt|\\stateseq_\\tt) \n",
    "\\right)  & \\text{log of ratio converted to difference in logs} \\\\\n",
    "& = & r_\\tt (\\theta) \\nabla_\\theta \\log \\pi_\\theta(\\actseq_\\tt|\\stateseq_\\tt) & \\text{since } \\log \\pi_{\\rm old} \\text{ is not a function of } \\theta \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Thus, taking the gradient of the Probability Ratio\n",
    "- yields the usual\n",
    "$$\n",
    "\\log \\pi_\\theta(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,t}) \n",
    "$$\n",
    "\n",
    "term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "We still have an extra $r_\\tt (\\theta)$ multiplicative term (relative to the Unified Gradient Formulation).\n",
    "\n",
    "So we don't obtain an *identical* expression\n",
    "- but we note that, with clipping, $r_\\tt (\\theta) \\approx 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantage for PPO\n",
    "\n",
    "To complete the presentation in terms of the Unified Gradient formulation, we define the Advantage.\n",
    "\n",
    "| Reward Setting         | Advantage Estimate                                  |\n",
    "|:-----------------------|:-----------------------------------------------------|\n",
    "| Intermediate Rewards  | $\\hat{\\advseq}_t = G_t -  \\statevalfun_\\pi(\\stateseq_\\tt)$ or GAE                 |\n",
    "| Single Total Reward   | $\\hat{\\advseq}_t = R(\\tau) - b$   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common ways to compute Advantage Estimate$\\hat{\\advseq}_t$ for Intermediate Rewards case\n",
    "\n",
    "- **Using Return-to-go and value estimate:**\n",
    "\n",
    "$\n",
    "\\hat{\\advseq}_t = G_t - V(s_t)\n",
    "$\n",
    "\n",
    "where $G_t$ is the discounted sum of rewards (return-to-go) starting from time step $t$, and $V(s_t)$ is the estimated state value.\n",
    "\n",
    "---\n",
    "\n",
    "- **Generalized Advantage Estimation (GAE):**\n",
    "\n",
    "$\n",
    "\\hat{\\advseq}_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}\n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "$\n",
    "\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
    "$\n",
    "\n",
    "with\n",
    "\n",
    "- $\\gamma$ as the discount factor,\n",
    "- $\\lambda \\in [0, 1]$ controlling the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- When $\\lambda = 0$, GAE reduces to the **1-step Temporal Difference (TD) advantage**:\n",
    "\n",
    "- When $\\lambda = 1$, GAE reduces to the **Monte Carlo advantage**:\n",
    "\n",
    "GAE smoothly interpolates between high-bias low-variance and low-bias high-variance advantage estimates, making it very effective in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pseudo code for PPO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # PPO training for LLM\n",
    "    for prompt in training_prompts:\n",
    "        output = llm.generate(prompt)\n",
    "        reward = evaluate_output(output)\n",
    "        logprob_old = llm.logprob(output, prompt) # From previous policy\n",
    "        value = critic(output, prompt) # Critic gives value baseline\n",
    "\n",
    "        # Calculate advantage\n",
    "        advantage = reward - value\n",
    "\n",
    "        # Compute importance ratio\n",
    "        logprob_new = llm_new.logprob(output, prompt)\n",
    "        ratio = exp(logprob_new - logprob_old)\n",
    "\n",
    "        # Clipped surrogate objective for stability\n",
    "        clip_epsilon = 0.2\n",
    "        loss1 = ratio * advantage\n",
    "        loss2 = clip(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantage\n",
    "        loss = -min(loss1, loss2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        critic_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## PPO with batches\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_loader:\n",
    "            # Generate trajectories\n",
    "            contexts = batch[\"contexts\"]\n",
    "            responses = model.sample(contexts)  # generate outputs\n",
    "\n",
    "            # Score trajectories with reward model\n",
    "            rewards = reward_model.score_batch(contexts, responses)\n",
    "\n",
    "            # Compute value estimates and advantages (using GAE or similar)\n",
    "            values = value_model.predict(contexts, responses)\n",
    "            advantages = compute_advantages(rewards, values)\n",
    "\n",
    "            # Get old policy log-probs for PPO ratio calculation\n",
    "            old_log_probs = model.log_prob(contexts, responses).detach()\n",
    "\n",
    "            # Forward pass to get new log probabilities\n",
    "            new_log_probs = model.log_prob(contexts, responses)\n",
    "\n",
    "            # Calculate PPO ratio\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "            # PPO clipped surrogate objective\n",
    "            clip_eps = 0.2\n",
    "            surrogate1 = ratios * advantages\n",
    "            surrogate2 = torch.clamp(ratios, 1-clip_eps, 1+clip_eps) * advantages\n",
    "            loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            # Backprop and update policy parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that all the mathematical operations\n",
    "- are performed in parallel on each example in the batch\n",
    "- the per-example loss\n",
    "    - is reduced to a single scalar\n",
    "    - via the `.mean()`\n",
    "    \n",
    "            loss = -torch.min(surrogate1, surrogate2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "PPO takes explicit steps\n",
    "- clipped probability ratio\n",
    "\n",
    "to reduce variance of Gradient estimates.\n",
    "\n",
    "It is robust and is a very common method when performing RL Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trust Region Policy Optimization (TRPO) Theory (Brief)\n",
    "\n",
    "We stated that, constraining the probability ratio\n",
    "$$\n",
    "r_t(\\theta) = \\frac{\\pi_\\theta(\\actseq_\\tt |\\stateseq_\\tt )}{\\pi_{\\theta_{\\rm old}}(\\actseq_\\tt |\\stateseq_\\tt)}\n",
    "$$\n",
    "to the small range $[1 - \\epsilon, 1 + \\epsilon]$\n",
    "\n",
    "is the key to the monotonic improvement of the optimization objective.\n",
    "\n",
    "This is based on *Trust Region Policy Optimizaton (TRPO)*.\n",
    "\n",
    "We state TRPO briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TRPO formulates policy optimization as a constrained optimization problem to ensure stable and monotonic policy improvement.\n",
    "\n",
    "It maximizes a surrogate objective subject to a constraint on how much the new policy can deviate from the old policy:\n",
    "\n",
    "$\n",
    "\\max{}_\\theta \\quad \\mathbb{E}_{s,a \\sim \\pi_{\\theta_{\\text{old}}}} \\left[\n",
    "    \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\n",
    "\\right]\n",
    "$\n",
    "\n",
    "subject to the trust region constraint:\n",
    "\n",
    "$\n",
    "\\mathbb{E}_{s \\sim \\pi_{\\theta_{\\text{old}}}} \\left[\n",
    "    D_{\\mathrm{KL}}\\big( \\pi_{\\theta_{\\text{old}}}(\\cdot|s) \\,\\|\\, \\pi_\\theta(\\cdot|s) \\big)\n",
    "\\right] \\leq \\delta\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\pi_{\\theta_{\\text{old}}}$ is the old (reference) policy,\n",
    "- $\\pi_\\theta$ is the new policy parameterized by $\\theta$,\n",
    "- $A^{\\pi_{\\theta_{\\text{old}}}}(s,a)$ is the advantage function with respect to the old policy,\n",
    "- $D_{\\mathrm{KL}}(\\cdot \\| \\cdot)$ is the Kullback-Leibler (KL) divergence measuring the difference between the old and new policies,\n",
    "- $\\delta$ is a small positive constant controlling the maximum allowed policy update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The trust region constraint (the KL divergence term)\n",
    "- **limits the size of the policy update** \n",
    "- to ensure the new policy does not differ drastically from the old policy\n",
    "-  preventing performance collapse.\n",
    "\n",
    "This facilitates **monotonic improvement** in policy performance.\n",
    "\n",
    "\n",
    "PPO can be interpreted as a practical approximation of TRPO \n",
    "- that replaces the hard KL constraint\n",
    "- with a clipped probability ratio \n",
    "\n",
    "in the objective.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GRPO\n",
    "\n",
    "GRPO is a Policy Gradient based methods that uses *preferences* rather than Rewards.\n",
    "\n",
    "It will be presented in a separate module with other Preference based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9 (new)",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
