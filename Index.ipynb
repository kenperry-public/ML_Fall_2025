{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)\n",
    "\n",
    "<!--- #include (README.md) --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1: Introduction\n",
    "**Plan**\n",
    "- Motivate Machine Learning\n",
    "- Introduce notation used throughout course\n",
    "- Plan for initial lectures\n",
    "    - *What*: Introduce, motivate a model\n",
    "    - *How*:  How to use a model: function signature, code (API)\n",
    "    - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
    "\n",
    "        \n",
    "- [Course Overview](Course_overview_NYU.ipynb)\n",
    "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
    "- [Intro to Classical ML](Intro_Classical_ML.ipynb)\n",
    "\n",
    "## Using an AI Assistant\n",
    "\n",
    "AI Assistants are often very good at coding.\n",
    "\n",
    "But using one to just \"get the answer\" deprives you of a valuable tool\n",
    "- you can ask the Assistant *why* it chose to do something\n",
    "- keep on asking\n",
    "- treat it as a private tutor !\n",
    "\n",
    "[Learning about the Landscape of ML](https://www.perplexity.ai/search/i-am-interested-in-the-landsca-_yO63NWfSGS8iHR5nyQYVA)\n",
    "\n",
    "[Learning about KNN using an Assistant as a private tutor](https://www.perplexity.ai/search/using-python-and-sklearn-pleas-407oe3uzTXu1i9xEHVR2MQ)\n",
    "- [Code answer from Assistant](KNN_illustration_Perplexity.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 2 (early start in Week 1)\n",
    "\n",
    "We began covering the \n",
    "**Recipe, as illustrated by Linear Regression**\n",
    "\n",
    "[The Recipe for Machine Learning: Solving a Regression task](Recipe_via_Linear_Regression.ipynb)\n",
    "- A *process* for Machine Learning\n",
    "    - Go through the methodical, multi-step process\n",
    "        - Quick first pass, followed by Deeper Dives\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2: Regression task\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We will learn the Recipe for Machine Learning,  a disciplined approach to solving problems in Machine Learning.\n",
    "\n",
    "We will illustrate the Recipe while, at the same time,\n",
    "introducing a model for the Regression task: Linear Regression.\n",
    "\n",
    "Our coverage of the Recipe will be rapid and shallow (we use an extremely simple example for illustration).\n",
    "\n",
    "I highly recommend reviewing and understanding\n",
    "this [Geron notebook](external/handson-ml2/02_end_to_end_machine_learning_project.ipynb)\n",
    "in order to acquire a more in-depth appreciation of the Recipe.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L3_S4_ML_Process.png\" width=\"100%\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "**Recipe, as illustrated by Linear Regression**\n",
    "\n",
    "[The Recipe for Machine Learning: Solving a Regression task (continued)](Recipe_via_Linear_Regression.ipynb#Create-a-test-set)\n",
    "\n",
    "- A *process* for Machine Learning\n",
    "    - Go through the methodical, multi-step process\n",
    "        - Quick first pass, followed by Deeper Dives\n",
    "     \n",
    "**Fitting a model: details**\n",
    "\n",
    "Recall: fitting a model (finding optimal value for the parameters) is found by minimizing a Loss function.\n",
    "\n",
    "Let's examine a typical Loss function for Regression\n",
    "- [Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
    "\n",
    "**Iterative training: when to stop**\n",
    "\n",
    "Increasing the number of parameters of a model improves in-sample fit (reduces Loss) but may compromise\n",
    "out-of-sample prediction (generalization).\n",
    "\n",
    "We examine the issues of having too many/too few parameters.\n",
    "- [When to stop iterating: Bias and Variance](Bias_and_Variance.ipynb)\n",
    "\n",
    "**Get the data: Fundamental Assumption of Machine Learning**\n",
    "\n",
    "- [Getting *good* training examples](Recipe_Training_data.ipynb)\n",
    "\n",
    "**Regression: final thoughts (for now)**\n",
    "\n",
    "- [Regression: coda](Regression_coda.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [Fine tuning techniques](Fine_tuning.ipynb)\n",
    "\n",
    "## Recipe \"Prepare the Data\" step: Transformations\n",
    "\n",
    "We discuss the importance of adding *synthetic* features to our Linear Regression example\n",
    "- and *preview* the *mechanical* process of creating these features via *Transformations*\n",
    "\n",
    "**Transformations**\n",
    " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    "\n",
    "## Validation\n",
    "\n",
    "Our test dataset can be used only once, yet\n",
    "- we have an iterative process for developing models\n",
    "- each iteration requires a proxy for out of sample data to use in the Performance Metric\n",
    "\n",
    "The solution: create a proxy for out of sample that is a *subset* of the training data.\n",
    "\n",
    "\n",
    "- [Validation and Cross-Validation](Recipe_via_Linear_Regression.ipynb#Validation-and-Cross-Validation)\n",
    "- [Avoiding cheating in Cross-Validation](Prepare_data_Overview.ipynb#Using-pipelines-to-avoid-cheating-in-cross-validation)\n",
    "\n",
    "## Using an AI Assistant\n",
    "\n",
    "[Learning about Linear Regression using an Assistant as private tutor](https://www.perplexity.ai/search/using-python-sklearn-and-matpl-vTYy7oGdRQ6upR5L5OSjrg)\n",
    "- [Code Answer from Assistant](LinearRegression_Illustration_Perplexity.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 3 (early start in Week 1)\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview](Classification_Overview.ipynb)\n",
    "- [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "**Categorical variables** (contained as subsections of Classification and Categorical Variables)\n",
    "\n",
    "We examine the proper treatment of categorical variables (target or feature).\n",
    "\n",
    "Along the way, we run into a subtle difficulty: the Dummy Variable Trap.\n",
    "\n",
    "- [Classification and Categorical Variables: Categorical Variables](Classification_Notebook_Overview.ipynb#Categorical-variables)\n",
    "    - [Categorical variables, One Hot Encoding (OHE)](Categorical_Variables.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3: Classification task\n",
    "\n",
    "**Non-feature dimensions**\n",
    "\n",
    "In response to questions about Assignment 1, \n",
    "- we will clarify \n",
    "the limitations in our ability to handle *timeseries* data with our current tools.\n",
    "\n",
    "\n",
    "[Non-feature dimensions: preview](Non-feature_dimensions_preview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Plan**\n",
    "- We introduce a model for the Classification task: Logistic Regression\n",
    "- How to deal with Categorical (non-numeric) variables\n",
    "    - classification target\n",
    "    - features\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview](Classification_Overview.ipynb)  **Covered last week**\n",
    "- [Classification and Categorical Variables (continued)](Classification_and_Non_Numerical_Data.ipynb#Recipe-Step-B:-Exploratory-Data-Analysis-(EDA))\n",
    "    - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "**Categorical variables** (contained as subsections of Classification and Categorical Variables)\n",
    "\n",
    "We examine the proper treatment of categorical variables (target or feature).\n",
    "\n",
    "Along the way, we run into a subtle difficulty: the Dummy Variable Trap.\n",
    "\n",
    "- [Classification and Categorical Variables: Categorical Variables](Classification_Notebook_Overview.ipynb#Categorical-variables)\n",
    "    - [Categorical variables, One Hot Encoding (OHE)](Categorical_Variables.ipynb)\n",
    "\n",
    "**Multinomial Classification**\n",
    "\n",
    "We generalize Binary Classification into classification into more than two classes.\n",
    "\n",
    "- [Multinomial Classification](Multinomial_Classification.ipynb)\n",
    "\n",
    "**Error Analysis**\n",
    "\n",
    "We can only improve our model's out of sample Performance Metric\n",
    "- by diagnosing the in-sample errors\n",
    "- that is the goal of the Error Analysis step of the Recipe\n",
    "- We explain Error Analysis for the Classification Task, with a detailed example\n",
    "- How Training Loss can be improved\n",
    "\n",
    "The conversion of a probability (e.g., model output) to a Class (categorical variable) for Classification\n",
    "- often involves the comparison of a probability to a threshold\n",
    "- we show how varying the threshold changes the conditional Performance Metric for Classification\n",
    "    - the threshold is a hyper-parameter, thus this is a kind of Fine-Tuning\n",
    "  \n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [linked notebook](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "    - [Worked example](Error_Analysis_MNIST.ipynb)**Deferred**\n",
    "\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    "\n",
    "**Classification and Categorical variables wrapup**\n",
    "\n",
    "- [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
    "- [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "- [OHE issue: Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "\n",
    "\n",
    "**Classification: final thoughts (for now)**\n",
    "\n",
    "- [Classification: coda](Classification_coda.ipynb)\n",
    "\n",
    "**Plan**\n",
    "**Deeper dives**\n",
    "- [Log odds](Classification_Log_Odds.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4: Transformations\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Now you know how to create models.  What happens if the Performance Metric for your model\n",
    "is disappointing ?\n",
    "\n",
    "The first step is recognizing the issue, and diagnosing it.  That is the role of Error Analysis.\n",
    "\n",
    "The second step is attempting to improve Performance.  Quite often we will need to perform\n",
    "*Feature Engineering*.\n",
    "\n",
    "We explain\n",
    "- why it is often necessary to create *synthetic* features to augment or replace *raw* feature\n",
    "- the mechanical process in `sklearn` that makes the application of transformations easy and consistent\n",
    "\n",
    "## Error Analysis: worked example (deferred from prior week)\n",
    "\n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [linked notebook](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "    - [Worked example](Error_Analysis_MNIST.ipynb)\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Transformations: the \"why\"\n",
    "\n",
    "Part of becoming a better Data Scientist is transforming raw features into more useful synthetic features.\n",
    "\n",
    "We focus\n",
    "on the necessity (the \"why\"): transforming raw data into something that tells a story.\n",
    "\n",
    "We will then discuss the mechanics (how to use `sklearn` to implement transformation Pipelines) of Transformations.\n",
    "\n",
    "\n",
    "\n",
    "<!--- #include (nvda_normalization_data.csv) --->\n",
    "<!--- #include (MORTGAGE30US.csv) --->\n",
    "\n",
    "- [Becoming a successful Data Scientist](Becoming_a_successful_Data_Scientist.ipynb)\n",
    "- [Transformations: overview](Transformations_Overview.ipynb)\n",
    "    - linked notebooks:\n",
    "        - [Transformations: adding a missing feature](Transformations_Missing_Features.ipynb)\n",
    "\n",
    "##  Transformations: the \"how\"\n",
    "\n",
    "Having hopefully motivated the use of transformations in theory\n",
    "- we turn to the *mechanical* process of creating these features via *Transformations in `sklearn`*\n",
    "\n",
    "**Transformations**\n",
    " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    "\n",
    "### Transformations: Avoiding cheating when using Cross-Validation\n",
    "\n",
    "Our test dataset can be used only once, yet\n",
    "- we have an iterative process for developing models\n",
    "- each iteration requires a proxy for out of sample data to use in the Performance Metric\n",
    "\n",
    "The solution: create a proxy for out of sample that is a *subset* of the training data.\n",
    "\n",
    "- [Validation and Cross-Validation](Recipe_via_Linear_Regression.ipynb#Validation-and-Cross-Validation)\n",
    "(**Covered in week 1**)\n",
    "- [Avoiding cheating in Cross-Validation](Prepare_data_Overview.ipynb#Using-pipelines-to-avoid-cheating-in-cross-validation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5: Other Classification models\n",
    "\n",
    "## Imbalanced data\n",
    "- [Imbalanced data](Imbalanced_Data.ipynb)\n",
    "\n",
    "## More models for classification\n",
    "\n",
    "**Plan**\n",
    "- More models: Decision Trees, Naive Bayes, Support Vector Classifier\n",
    "    - Different flavor: more procedural, less mathematical\n",
    "    - Decision Trees: a model with *non-linear* boundaries\n",
    "- Ensembles\n",
    "    - Bagging and Boosting\n",
    "    - Random Forests\n",
    "\n",
    "**Decision Trees, Ensembles**\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)\n",
    "\n",
    "\n",
    "**Naive Bayes**\n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n",
    "\n",
    "**Support Vector Classifiers**\n",
    "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)  \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n",
    "**Classification: final thoughts**\n",
    "\n",
    "- [Classification: coda -- review again](Classification_coda.ipynb)\n",
    "\n",
    "\n",
    "## Using an AI Assistant to learn about other models for classification\n",
    "- [SVC conversation](https://www.perplexity.ai/search/what-is-the-relationship-betwe-Pq8r22pISH.gUGmM1gkgbg#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6: Unsupervised Learning\n",
    "\n",
    "## More models for classification (continued)\n",
    "\n",
    "**SVC: review**\n",
    "\n",
    "- [SVC: Key points](SVM_Large_Margin.ipynb#SVC:-Key-points)\n",
    "\n",
    "- [SVC conversation](https://www.perplexity.ai/search/what-is-the-relationship-betwe-Pq8r22pISH.gUGmM1gkgbg#4)\n",
    "\n",
    "\n",
    "**SVM (deferred from last week)**\n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n",
    "\n",
    "**Naive Bayes** (deferred from last week)\n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n",
    "**Classification coda**\n",
    "- [Classification: probability distribution over classes](Classification_coda.ipynb#Output:-probabilities-or-just-classes-?)\n",
    "\n",
    "## Unsupervised Learning\n",
    "**Unsupervised Learning: PCA**\n",
    "- [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Unsupervised.ipynb)\n",
    "- [PCA in Finance](PCA_Yield_Curve_Intro.ipynb)\n",
    "\n",
    "**Unsupervised Learning: PCA** (continued)\n",
    "- [Importance of number of components: visualization](Unsupervised.ipynb#Visualizing-the-fidelity-of-the-reduced-dimension-representation)\n",
    "- [Interpreting the components](Unsupervised.ipynb#Can-we-interpret-the-components-?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7: DL Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "## Bridge between Classical ML and Deep Learning\n",
    "\n",
    "**Gradient Descent** \n",
    "\n",
    "Machine Learning is based on minimization of a Loss Function.  Gradient Descent is one algorithm\n",
    "to achieve that.\n",
    "- [Gradient Descent](Gradient_Descent.ipynb)\n",
    "\n",
    "\n",
    "**Recommender Systems (Pseudo SVD)**\n",
    "\n",
    "How does Amazon/Netflix/etc. recommend products/films to us ?  We describe a method similar to SVD\n",
    "but that is solved using Gradient Descent.\n",
    "\n",
    "This theme of creating a custom Loss Functions and minimizing it via Gradient Descent is a recurring\n",
    "theme in the upcoming Deep Learning second half of the course.\n",
    "\n",
    "- [Recommender Systems](Recommender_Systems.ipynb)\n",
    "- [Preview: Some cool Loss functions](Loss_functions.ipynb#Loss-functions-for-Deep-Learning:-Preview)\n",
    "\n",
    "**Deeper Dives**\n",
    "\n",
    "\n",
    "- [Other matrix factorization methods](Unsupervised_Other_Factorizations.ipynb)\n",
    "\n",
    "\n",
    "## Classical ML: deeper dives\n",
    "\n",
    "**Loss functions: mathematical basis** (deferred)\n",
    "\n",
    "Where do the Loss functions of Classical Machine Learning come from ?  We take a brief mathematical\n",
    "detour into Loss functions.\n",
    "\n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "- [Loss functions: the math](Loss_functions.ipynb)\n",
    "    - Maximum likelihood\n",
    "    - Preview: custom loss functions and Deep Learning\n",
    "\n",
    "**Deeper Dives**\n",
    "- [Linear Regression in more depth](Linear_Regression_fitting.ipynb)\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "- [Missing data: clever ways to impute values](Missing_Data.ipynb)\n",
    "- [Feature importance](Feature_Importance.ipynb)\n",
    "- [SVC Loss function derivation](SVM_Derivation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning: Introduction\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Deep Learning/Neural networks\n",
    "\n",
    "- [Set up your Tensorflow environment](Tensorflow_setup.ipynb)\n",
    "- [Neural Networks Overview](Neural_Networks_Overview.ipynb)\n",
    "\n",
    "\n",
    "**Neural network: practical**\n",
    "- Coding Neural Networks:  Keras\n",
    "    - [Intro to Keras](Keras_intro.ipynb)\n",
    "\n",
    "    - **Note**\n",
    "        - If you have problems using the `plot_model` function in Keras on your local machine: see [here](Setup_ML_Environment_NYU.ipynb#Tools-for-visualization-of-graphs-(optional)) for a fix.\n",
    "\n",
    "    - Linked notebooks\n",
    "    <!--- #include (DNN_Keras_example.ipynb) --->\n",
    "        - [DNN Keras example](DNN_Keras_example.ipynb) **local machine**\n",
    "        - [DNN Keras example Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2025/blob/master/DNN_Keras_example.ipynb) (**Google Colab**)\n",
    "\n",
    "\n",
    "- Practical Colab\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (neural_net_helper.py) --->\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (Colab_practical.ipynb)) --->\n",
    "<!--- #include (CommonLib.py) --->\n",
    "   - **Colab**: [Practical Colab Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2025/blob/master/Colab_practical.ipynb)\n",
    "   \n",
    "**Practical advice**\n",
    "\n",
    "- Karpathy: [Recipe for training Neural Nets](Karpathy_Recipe_for_training_NN.ipynb)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 8 (DL Week 2)  Intro to NN (continued); Convolutional Neural Networks\n",
    "\n",
    "## Introduction (continued)\n",
    "\n",
    "Here is a quick review of Neural Networks\n",
    "\n",
    "- [Neural Network summary](Neural_Network_summary.ipynb)\n",
    "\n",
    "Overview continued:\n",
    "\n",
    "- [Overview (continued)](Neural_Networks_Overview.ipynb#What-is-$W_\\llp$-?-Where-did-$\\Theta$-go-?)\n",
    "\n",
    "Coding a Neural Network\n",
    "\n",
    "- Coding Neural Networks:  Keras\n",
    "    - [Intro to Keras](Keras_intro.ipynb) **covered last lecture**\n",
    "\n",
    "    - **Note**\n",
    "        - If you have problems using the `plot_model` function in Keras on your local machine: see [here](Setup_ML_Environment_NYU.ipynb#Tools-for-visualization-of-graphs-(optional)) for a fix.\n",
    "\n",
    "    - Linked notebooks\n",
    "    <!--- #include (DNN_Keras_example.ipynb) --->\n",
    "        - [DNN Keras example](DNN_Keras_example.ipynb) **local machine**\n",
    "        - [DNN Keras example Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2025/blob/master/DNN_Keras_example.ipynb) (**Google Colab**)\n",
    "\n",
    "\n",
    "- Practical Colab\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (neural_net_helper.py) --->\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (Colab_practical.ipynb)) --->\n",
    "<!--- #include (CommonLib.py) --->\n",
    "   - **Colab**: [Practical Colab Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2025/blob/master/Colab_practical.ipynb)\n",
    "   \n",
    "**Practical advice** (continued)\n",
    "- Karpathy: [Recipe for training Neural Nets](Karpathy_Recipe_for_training_NN.ipynb)\n",
    "\n",
    "## NN: in depth\n",
    "\n",
    "**Plan**\n",
    "\n",
    "The topics introduced in the Neural Networks Overview are now covered more in-depth.\n",
    "- Where do Neural Networks get their power from ?\n",
    "- How exactly do we compute the gradients ?\n",
    "- How does a special language/library facilitate automatic computation of the gradients ?\n",
    "\n",
    "**Neural network theory**\n",
    "- [A neural network is a Universal Function Approximator](Universal_Function_Approximator.ipynb)\n",
    "\n",
    "**Training Neural Networks (introduction)**\n",
    "- [Intro to Training](Neural_Networks_Intro_to_Training.ipynb)\n",
    "- [Training Neural Networks - Back propagation](Training_Neural_Network_Backprop.ipynb)\n",
    "\n",
    "**How to compute gradients automatically**\n",
    "- [Why TensorFlow ?: Gradients made easy](Training_Neural_Network_Operation_Forward_and_Backward_Pass.ipynb)\n",
    "\n",
    "   \n",
    "**Deeper Dives**\n",
    "<!--- #include (Raw_TensorFlow.ipynb)) --->\n",
    "- [Keras, from past to present](Tensorflow_Keras_Archaeology.ipynb)\n",
    "- [History/Computation Graphs: Tensorflow version 1](DNN_TensorFlow_Using_TF_version_1.ipynb)\n",
    "- [Raw_TensorFlow example Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2025/blob/master/Raw_TensorFlow.ipynb) (**Colab**)\n",
    "- [Computation Graphs](Computation_Graphs.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 9 (DL Week 3): Training dynamics; Convolutional Neural Network Layer\n",
    "\n",
    "## Training Neural Networks: details\n",
    "\n",
    "**Plan**\n",
    "- Why training a Neural Network can be difficult: fine-details of training\n",
    "\n",
    "**Training Neural Networks: the fine details**\n",
    "- [The dynamics of training](Training_Neural_Networks_Overview.ipynb)\n",
    "    - Effects of changing: activation functions; weight initialization\n",
    "    - initialization and scaling\n",
    "    - dropout\n",
    "    - learning rate schedules\n",
    "    - vanishing/exploding gradients\n",
    "    \n",
    "## Convolutional Neural Network (CNN) Layer\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We introduce a new layer type.\n",
    "\n",
    "This is motivated by inputs with dimensions in addition to the feature dimension.\n",
    "\n",
    "The Convolutional Neural Network layer type\n",
    "\n",
    "- [Non-feature dimensions: preview](Non-feature_dimensions_preview.ipynb)\n",
    "\n",
    "- [Introduction to CNN](Intro_to_CNN.ipynb)\n",
    "    - [CNN pictorial](CNN_pictorial.ipynb)\n",
    "- [Notational standards, definitions](CNN_Notation.ipynb)\n",
    "- [CNN: Space and Time](CNN_Space_and_Time.ipynb)\n",
    "    <!--- #include (CNN_Keras.ipynb) --->\n",
    "    - [CNN example from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2025/blob/master/CNN_Keras.ipynb) (**Colab**) \n",
    "    - [CNN example from github](CNN_Keras.ipynb) (**local machine**) \n",
    "\n",
    "The following notebooks are an older attempt at a *visual* explanation of the CNN.\n",
    "\n",
    "Hopefully, the \"Introduction\" notebook is more intuitive and may supercede these visual notebooks.\n",
    "\n",
    "- [CNN: explained in pictures](CNN_Overview.ipynb)\n",
    "\n",
    "\n",
    "**Deeper dives**\n",
    "- [Convolution as Matrix Multiplication](CNN_Convolution_as_Matrix_Multiplication.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 10 (DL Week 4): Recurrent Neural Networks\n",
    "\n",
    "**CNN: code**\n",
    "\n",
    "<!--- #include (CNN_Keras.ipynb) --->\n",
    "- [CNN example from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2025/blob/master/CNN_Keras.ipynb) (**Colab**) \n",
    "- [CNN example from github](CNN_Keras.ipynb) (**local machine**) \n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "**Recurrent Neural Networks (RNN)**\n",
    "- [Introduction to Recurrent Neural Network (RNN)](Intro_to_RNN.ipynb)\n",
    "- [Recurrent Neural Network Overview](RNN_Overview.ipynb)\n",
    "    - [linked notebook: RNN in code -- Imdb sentiment classification](NLP_Keras.ipynb#Try-an-LSTM-as-a-means-of-obtaining-a-finite-length-representation-of-the-sequence)\n",
    "\n",
    "\n",
    "**RNN: Issues**\n",
    "- [Gradients of an RNN](RNN_Gradients.ipynb)\n",
    "- [RNN: Gradients that Vanish/Explode](RNN_Vanishing_and_exploding_gradients.ipynb)\n",
    "- [RNN: Visualization](RNN_Visualization.ipynb)\n",
    "\n",
    "## Advanced Recurrent Architectures: LSTM\n",
    "\n",
    "\n",
    "**Plan**\n",
    "\n",
    "The \"vanilla\" Recurrent Neural Network (RNN) layer we learned is very much exposed to the problem of vanishing/exploding gradients.\n",
    "\n",
    "We will review the issue and demonstrate a related layer type (the LSTM) designed to mitigate the problem.\n",
    "\n",
    "We  present an extremely useful trick (Transfer Learning) for leveraging the hard work that others have done.\n",
    "\n",
    "**Concepts**\n",
    "\n",
    "There are a number of pieces of the LSTM which can appear overwhelming when seen together for the first time.  We will explore these concepts separately before seeing how they are integrated into the LSTM.\n",
    "\n",
    "- [Residual connections](RNN_Residual_Networks.ipynb)\n",
    "- [Neural Programming](Neural_Programming.ipynb)\n",
    "\n",
    "**LSTM: An improved RNN**\n",
    "\n",
    "- [Introduction to the LSTM](Intro_to_LSTM.ipynb)\n",
    "- [LSTM Overview](LSTM_Overview.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Layer types: review\n",
    "\n",
    "Sprint is over ! We have covered the basic layer types; time for you to learn by experimenting.\n",
    "\n",
    "**Review of layer types**\n",
    "- [What layer type to choose](Neural_Network_Layer_Review.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [RNN: How to deal with long sequences](RNN_Long_Sequences.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 11 (DL Week 5 ): Transfer Learning; Natural Language Processing\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "Transfer learning allows us to adapt a model trained for one task to be able to solve a new task with a small amount of work.  As models get bigger and bigger, the future of Deep Learning may be one where you use Transfer Learning more than developing your own models from scratch.\n",
    "\n",
    "- [Transfer Learning (Continued)](Transfer_Learning.ipynb)\n",
    "\n",
    "     - [Transfer Learning example from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2025/blob/master/TransferLearning_Keras.ipynb) (**Colab**)\n",
    "     - [Transfer Learning example from github](TransferLearning_Keras.ipynb) (**local machine**)\n",
    "\n",
    "     - [Utility notebook](Dogs_and_Cats_reformat.ipynb)\n",
    "         - Takes the *very large* raw data (from Kaggle) used in the Transfer Learning example\n",
    "         - Creates a much smaller subset, using a different directory structure\n",
    "         - The above notebook uses this reorganized, smaller subset\n",
    "\n",
    "## NLP\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We will make an initial pass on the topic of learning from text: Natural Language Processing.\n",
    "\n",
    "The first pass will use well-established techniques that are relatively easy to follow.\n",
    "\n",
    "We then explore some recent advances that have greatly increased the power of NLP.\n",
    "\n",
    "The Transformer architecture is a key contributor.\n",
    "\n",
    "**Learning from text: Deep Learning for Natural Language Processing (NLP)**\n",
    "- [Natural Language Processing Overview](NLP_Overview.ipynb)\n",
    "\n",
    "We revisit some code we had previously studied\n",
    "- in the RNN module: to illustrate various ways to eliminate the time dimension\n",
    "- but this time with an emphasis on the NLP aspects\n",
    "    - [NLP from github (Colab)](https://colab.research.google.com/github/kenperry-public/ML_Fall_2025/blob/master/NLP_Keras.ipynb)\n",
    "    - [NLP from github (local machine)](NLP_Keras.ipynb)\n",
    "   \n",
    "<!--- #include (squad_show.csv) --->\n",
    "\n",
    "**Evolution of Word representations**\n",
    "- [How to represent a word: syntax](NLP_Tokenization.ipynb)\n",
    "- [How to represent a word: meaning](NLP_Word_Representations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers: motivation\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We present Attention, a way to enhance the power of RNN's, which is heavily used in a new layer type for sequence processing: the Transformer.  \n",
    "\n",
    "The Transformer layer type is now predominant in the area of Natural Language Processing (NLP).\n",
    "We give a quick introduction but we will revisit it in the module on advanced NLP.\n",
    "\n",
    "### Attention\n",
    "\n",
    "- [Attention: motivation](Intro_to_Attention.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 12 (DL Week 6)\n",
    "\n",
    "We summarize (with illustrations) the key points regarding attention.\n",
    "- The evolution of the Encoder/Decoder RNN to\n",
    "    - a \"loop-free\" architecture via Self-Attention\n",
    "    - combined with Cross attention from Encoder to Decoder\n",
    "\n",
    "- [Transformer motivation: Illustrated](Attention_motivation_illustrated.ipynb)\n",
    "\n",
    "These ideas are combined into a new architecture called the **Transformer**\n",
    "\n",
    "\n",
    "## Transformers: details\n",
    "- [Transformer](Transformer.ipynb)\n",
    "\n",
    "\n",
    "### Attention: in depth\n",
    "\n",
    "- [Implementing Attention](Attention_Lookup.ipynb)\n",
    "\n",
    "\n",
    "**Deeper dives**\n",
    "- Transformer\n",
    "    - [Keras example: pre-defined Attention layers](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
    "        - [notebook](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_with_transformer.ipynb)\n",
    "    - [TensorFlow tutorial: implements Attention, positional encoding](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "       - [notebook](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb)\n",
    "\n",
    "\n",
    "**Further reading**\n",
    "- Attention\n",
    "    - [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "    - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- Transfer Learning    \n",
    "    - [Sebastian Ruder: Transfer Learning](https://ruder.io/transfer-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning (RL)\n",
    "\n",
    "**Reference**\n",
    "\n",
    "[Sutton and Barto: Reinforcement Learning: An Introduction, 2nd edition](http://incompleteideas.net/sutton/book/the-book-2nd.html)\n",
    "\n",
    "- Note: this is the website of one author: Sutton\n",
    "\n",
    "A preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 13 (DL Week 7): Reinforcement Learning (continued), Language Models\n",
    "\n",
    "## Reinforcement Learning (do-over)\n",
    "\n",
    "**Reference**\n",
    "\n",
    "[Sutton and Barto: Reinforcement Learning: An Introduction, 2nd edition](http://incompleteideas.net/sutton/book/the-book-2nd.html)\n",
    "\n",
    "- Note: this is the website of one author: Sutton\n",
    "\n",
    "**Introduction to Reinforcement Learning**\n",
    "\n",
    "- [Introduction to Reinforcement Learning](RL_intro.ipynb)\n",
    "\n",
    "    **Colab**\n",
    "    - [RL Intro via Gymnasium](https://colab.research.google.com/drive/1d-JjgUp7Xjjtf5TsDFULBJvwj-dP_E40#scrollTo=ddf2bfa9)\n",
    "    - [RL Playground](https://colab.research.google.com/drive/1Ei39dUXXA3d3H5AzdxFmbFOF-QS__vqT#scrollTo=de23d490)\n",
    "\n",
    "**Value based methods**\n",
    "\n",
    "- [Value-based methods: Introduction](RL_Value_based_intro.ipynb)\n",
    "- [Value-based methods (model-based](RL_Value_based_model_based.ipynb)\n",
    "- [Value-based methods  (model-free)](RL_Value_based_model_free.ipynb)\n",
    "\n",
    "    **On-Policy vs Off-Policy: Supplemental notebooks**\n",
    "    \n",
    "    - On-Policy vs Off-Polic: code examples](RL_OnPolicy_vs_OffPolicy_code_examples.ipynb)\n",
    "    - [On-Policy SARSA vs DQN](https://colab.research.google.com/drive/1vItk1GUHLYd4vsYma5lLGBnNEbSUR1ya)\n",
    "\n",
    "**Policy based methods**\n",
    "\n",
    "- [Policy-based methods: Introduction](RL_Policy_gradient_methods_intro.ipynb)\n",
    "- [Policy-based methods for RL](RL_Policy_gradient_methods_classic.ipynb)\n",
    "\n",
    "\n",
    "**Preference methods**\n",
    "\n",
    "[RL Preference methods: introduction](RL_Preference_methods_intro.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language Models\n",
    "\n",
    "**Language Models: the future (present ?) of NLP ?**\n",
    "\n",
    "- [Language Models, the future (present ?) of NLP: Review](Review_LLM.ipynb)\n",
    "\n",
    "<!--- #include (squad_show.csv) --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional Deep Learning resources\n",
    "\n",
    "Here are some resources that I have found very useful.\n",
    "\n",
    "Some of them are very nitty-gritty, deep-in-the-weeds (even the \"introductory\" courses)\n",
    "- For example: let's make believe PyTorch (or Keras/TensorFlow) didn't exists; let's invent Deep Learning without it !\n",
    "    - You will gain a deeper appreciation and understanding by re-inventing that which you take for granted\n",
    "    \n",
    "\n",
    "## [Andrej Karpathy course: Neural Networks, Zero to Hero](https://karpathy.ai/zero-to-hero.html)\n",
    "- PyTorch\n",
    "- Introductory, but at a very deep level of understanding\n",
    "    - you will get very deep into the weeds (hand-coding gradients !) but develop a deeper appreciation\n",
    "    \n",
    "## fast.ai\n",
    "\n",
    "`fast.ai` is a web-site with free courses from Jeremy Howard.\n",
    "- PyTorch\n",
    "- Introductory and courses \"for coders\"\n",
    "- Same courses offered every few years, but sufficiently different so as to make it worthwhile to repeat the course !\n",
    "    - [Practical Deep Learning](https://course.fast.ai/)\n",
    "    - [Stable diffusion](https://course.fast.ai/Lessons/part2.html)\n",
    "        - Very detailed, nitty-gritty details (like Karpathy) that will give you a deeper appreciation\n",
    "        \n",
    "## [Stefan Jansen: Machine Learning for Trading](https://github.com/stefan-jansen/machine-learning-for-trading)\n",
    "\n",
    "An excellent github repo with notebooks\n",
    "- using Deep Learning for trading\n",
    "- Keras\n",
    "- many notebooks are cleaner implementations of published models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "- Assignment notebook: [Using Machine Learning for Hedging](assignments/Regression%20task/Using_Machine_Learning_for_Hedging.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Regression task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "- Assignment notebook: [Ships in satellite images](assignments/Classification%20task/Ships_in_satellite_images.ipynb#)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Classification task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Midterm Project: Bankruptcy One Year Ahead\n",
    "- Assignment notebook [Bankruptcy One Year Ahead](assignments/bankruptcy_one_yr/Bankruptcy_oya.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Bankruptcy One Year Ahead\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras practice\n",
    "- Assignment notebook [Ships in satellite images: Neural Network](assignments/keras_intro/Ships_in_satellite_images_P1.ipynb)\n",
    "- Data (same as for the Classification assignment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "- Assignment notebook [Ships in satellite images: Neural Network](assignments/CNN_intro/Ships_in_satellite_images_P2.ipynb)\n",
    "- Data (same as for the Classification assignment)\n",
    "    - please repeat the directions given in that assignment for obtaining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final project; Stock prediction\n",
    "\n",
    " - Assignment notebooks:\n",
    "    - [Stock prediction](assignments/stock_prediction/Final_project_StockPrediction.ipynb)\n",
    "    - [Submission guidelines](assignments/stock_prediction/Final_project.ipynb)\n",
    "   \n",
    " - Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Stock Prediction\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook, submission guidelines notebook\n",
    "        - A directory named `Data/train`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9 (new)",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
