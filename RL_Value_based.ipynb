{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value-based methods\n",
    "\n",
    "The simplest model-based approach are *Value-based* methods.\n",
    "\n",
    "They revolve around the idea of\n",
    "- assigning a state value function $\\statevalfun_\\pi(\\state)$ to each state $\\state \\in \\States$\n",
    "$$\n",
    "\\statevalfun_{\\pi}: \\States \\to \\Reals\n",
    "$$\n",
    "- $\\statevalfun_\\pi(\\state)$ is an approximation of \n",
    "$$\n",
    "\\E_\\pi ( G_\\tt  | \\stateseq_\\tt =  \\state )\n",
    "$$\n",
    "the expected return achievable from state $\\state$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given $\\statevalfun_\\pi$, the optimal deterministic policy is\n",
    "\n",
    "$$\n",
    "\\pi^*(\\state) = \\argmax{\\,\\,\\,\\,\\,\\,\\,\\,\\act \\\\  \\transp({ \\state', \\rew | \\state, \\act }) \\ne 0}\\statevalfun_\\pi(\\state')\n",
    "$$\n",
    "\n",
    "- From state $\\state$\n",
    "- Choose the action $\\act$\n",
    "- that results in next state $\\state'$\n",
    "- with maximal $\\statevalfun_\\pi(\\state')$\n",
    "\n",
    "\n",
    "\n",
    "Note: the $\\argmax{}$ results in a *deterministic* policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will show two broad classes of Value-based methods\n",
    "- Model-based\n",
    "- Model-free\n",
    "\n",
    "<br>\n",
    "\n",
    "    Value-Based Methods\n",
    "    ├── Model-Based\n",
    "    │   └── DP Methods\n",
    "    │       ├── Value Iteration\n",
    "    │       └── Policy Iteration\n",
    "    │\n",
    "    └── Model-Free\n",
    "        ├── Monte Carlo (MC)\n",
    "        ├── Temporal Difference (TD)\n",
    "        └── Q-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model-based Value methods\n",
    "\n",
    "Using a value-based method is practical only if we can\n",
    "discover the state value function $\\statevalfun$, which is initially unknown.\n",
    "\n",
    "Most methods are iterative in nature and create a sequence of increasingly accurate approximations of $\\statevalfun$ \n",
    "$$\n",
    "\\statevalfun_{\\pi,0} \\ldots \\statevalfun_{\\pi,k} \\ldots\n",
    "$$\n",
    "\n",
    "In the limit \n",
    "$$\n",
    "\\lim_{k \\to \\infty}  \\statevalfun_{\\pi,k} = \\statevalfun_\\pi \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we obtain an improved approximation $\\statevalfun_{\\pi, k+1}$ \n",
    "- we may reflect this improved knowledge by updating the policy\n",
    "\n",
    "Thus, we periodically improve the policy based on improved approximations of $\\statevalfun_\\pi$\n",
    "\n",
    "This results in a sequence of increasingly accurate approximations of the policy $\\pi$\n",
    "\n",
    "$$\n",
    "\\pi_0, \\ldots, \\pi_p, \\ldots\n",
    "$$\n",
    "which hopefully converges to $\\pi^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dynamic Programming \n",
    "\n",
    "Given a model that describes the behavior of the Environment\n",
    "- we can determine the value state function via Dynamic Programming (DP) based techniques.\n",
    "\n",
    "as follows.\n",
    "\n",
    "The expected returns from state $\\state$\n",
    "$$\n",
    "\\E_\\pi ( G_\\tt  | \\stateseq_\\tt =  \\state )\n",
    "$$\n",
    "\n",
    "are computed via the *Bellman Equation*\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\statevalfun_\\pi(\\stateseq_\\tt) & = & \\E_\\pi ( G_\\tt  | \\stateseq_\\tt =  \\state ) \\\\\n",
    "& = & \\E_\\pi  \\left( \\rewseq_{\\tt+1} + \\disc G_{\\tt+1} | \\stateseq_\\tt =  \\state  \\right) & \\text{immediate reward } \\rewseq_{\\tt+1} \\\\\n",
    "    & & & \\text{ plus discounted future rewards } G_{\\tt+1} \\\\\n",
    "& = & \\E_\\pi  \\left( \\rewseq_{\\tt+1} + \\disc \\statevalfun_\\pi ( \\stateseq_{\\tt+1})  | \\stateseq_\\tt =  \\state  \\right) & \\text{since } \\statevalfun_\\pi ( \\stateseq_{\\tt+1}) = \\E_\\pi \\left( G_{\\tt+1} | \\stateseq_\\tt =  \\state \\right) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Bellman Equation asserts that\n",
    "$$\n",
    "\\statevalfun_\\pi(\\stateseq_\\tt)\n",
    "$$\n",
    "\n",
    "can be derived from\n",
    "- the immediate reward $\\rewseq_{\\tt+1}$\n",
    "- and the discounted (by $\\gamma$) value of the successor state $\\stateseq_{\\tt+1}$\n",
    "$$\n",
    "\\statevalfun_\\pi ( \\stateseq_{\\tt+1}) \n",
    "$$\n",
    "\n",
    "This recursive equation terminates \n",
    "- since the successor state $\\stateseq_{\\tt+1}$ on the RHS\n",
    "- is one transition closer to the end \n",
    "of episode $\\pi$ \n",
    "- than the LHS state $\\stateseq_{\\tt}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting the Expectation \n",
    "\n",
    "Expanding the expectation into a sum\n",
    "\n",
    "\\begin{array} \\\\\n",
    "\\statevalfun_\\pi(\\stateseq_\\tt)\n",
    "& = & \\sum_{\\act}  \\pi(\\act, \\stateseq_\\tt) \\sum_{\\state', \\rew} \\transp(\\state', \\rew | \\stateseq_\\tt, \\act)  \n",
    "    \\left( \\rew + \\disc \\statevalfun_\\pi(\\state') \\right) & \\text{expectation over all possible actions } \\act \\text{ from state } \\state \\\\\n",
    "    & & & \\text{of immediate reward } \\rew \\\\\n",
    "    & & & \\text{and discounted future reward} \\\\\n",
    "    & & & \\text{resulting from successor state } \\state' \\text{ of action } \\act \\\\\n",
    "\\end{array}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This equation guides our approximation of\n",
    "$\\statevalfun_\\pi(\\stateseq_\\tt) $ given current policy $\\pi$\n",
    "\n",
    "- for each action $\\act$ that can be chosen by the Agent in state $\\stateseq_\\tt$\n",
    "    - with probability $\\pi(\\act, \\stateseq_\\tt)$ (stochastic policy)\n",
    "- use the return received from taking the action\n",
    "    - determined by the Environment, which chooses\n",
    "        - immediate reward $\\rew$\n",
    "        - successor state $\\state'$\n",
    "    - with probability $\\transp(\\state', \\rew | \\stateseq_\\tt, \\act) $ (stochastic environment)\n",
    "- use the current approximation $v$ of the value of the successor state $state'$   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This equation can be computed only if we know the behavior of the Environment\n",
    "- the reward and successor state chosen by the Environment\n",
    "- given the Agent choosing action $\\act$ in state $\\stateseq_\\tt$\n",
    "- i.e.\n",
    "$$\n",
    "\\transp(\\state', \\rew | \\stateseq_\\tt, \\act) \n",
    "$$\n",
    "\n",
    "Hence: this is *model-based* and not model-free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Model-based methods have access to the Environment's behavior. \n",
    "\n",
    "This may come about because\n",
    "- the model is given to us\n",
    "- we incorporate methods to *learn* a model simultaneously with learning the Value function\n",
    "\n",
    "For now: we assume the model is given to us.\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simplifiction for deterministic policy or environment**\n",
    "\n",
    "\n",
    "For deterministic policy $\\pi$\n",
    "- as given by \n",
    "    $$\n",
    "\\pi(\\state) = \\argmax{\\,\\,\\,\\,\\,\\,\\,\\,\\act \\\\  \\transp({ \\state', \\rew | \\state, \\act }) \\ne 0}\\statevalfun_\\pi(\\state')\n",
    "$$\n",
    "    - chose the action resulting in successor state with maximum state value\n",
    "\n",
    "all the probability is concentrated at single action $\\actseq_\\tt$\n",
    "$$\n",
    "\\pi(\\stateseq_\\tt, \\act^*) = 1 \\text{ for } \\act^*  =\\argmax{\\,\\,\\,\\,\\,\\,\\,\\,\\act \\\\  \\transp({ \\state', \\rew | \\state, \\act }) \\ne 0}\\statevalfun_\\pi(\\state')\n",
    "$$\n",
    "so you can drop the $\\sum_{\\act}  \\pi(\\act, \\stateseq_\\tt)$ from the equation\n",
    "- and equate $\\act = \\actseq_\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "When the Environment is deterministic\n",
    "all the probability is concentrated at a single response:\n",
    "    \n",
    "$$\n",
    "\\transp({ \\stateseq_{\\tt+1}, \\rewseq_{\\tt+1} | \\stateseq_\\tt, \\actseq_\\tt }) = 1\n",
    "$$\n",
    "and the Agent's policy chooses the $\\actseq_\\tt$ that results in a successor state $\\state' = \\stateseq_{\\tt+1}$ that maximizes\n",
    "$\\statevalfun(\\state')$\n",
    "$$\n",
    "\\statevalfun_\\pi(\\stateseq_{\\tt+1}) = \\max{\\state'}{\\statevalfun_\\pi(\\state')}\n",
    "$$\n",
    "\n",
    "Thus, in the case of deterministic Policy and Environment, the RHS becomes\n",
    "\n",
    "\n",
    "$$\n",
    "\\disc \\left( \\rewseq_{\\tt+1} +  \\max{\\state'}{\\statevalfun_\\pi(\\state')} \\right) \n",
    "$$\n",
    "\n",
    "which is how it typically appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Value iteration method\n",
    "\n",
    "The simplest method is to\n",
    "- iteratively update the Value function\n",
    "    - until convergence\n",
    "- derive the *final* Policy from the Value function\n",
    "    - chose the action leading to highest return\n",
    "    - based on the Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pseudo code for Value Iteration**\n",
    "\n",
    "Here is some pseudo-code\n",
    "\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "    <center><strong>Value iteration</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/RL_value_iteration_alg.png\" width=90%>\n",
    "    </tr>\n",
    "    \n",
    "\n",
    "</table>\n",
    "\n",
    "Attribution: http://incompleteideas.net/book/RLbook2020.pdf#page=105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Value iteration</strong></center>\n",
    "\n",
    "    Initialize value function V(s) arbitrarily (e.g., zero for all states)\n",
    "\n",
    "    Repeat:\n",
    "        delta = 0\n",
    "        For each state s:\n",
    "            old_value = V(s)\n",
    "            V(s) = max over a [ R(s, a) + γ * sum over s' [ P(s' | s, a) * V(s') ] ]\n",
    "            delta = max(delta, |old_value - V(s)|)\n",
    "\n",
    "        Until delta < threshold\n",
    "\n",
    "    # Derive policy after value function converges\n",
    "    For each state s:\n",
    "        π(s) = argmax over a [ R(s, a) + γ * sum over s' [ P(s' | s, a) * V(s') ] ]\n",
    "\n",
    "    Return π, V\n",
    "    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Subtlety**\n",
    "\n",
    "Notice that the Bellman equation in the code is modified\n",
    "- to reflect the deterministic, optimal choice of action\n",
    "\n",
    "    max over a { ... }\n",
    "    \n",
    "rather than an expectation over all possible actions\n",
    "\n",
    "$$\\sum_{\\act}  \\pi(\\act, \\stateseq_\\tt) \\{ \\ldots \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Policy iteration method\n",
    "\n",
    "Rather than updating the Policy once (after Value function convergence)\n",
    "- we introduce a method that periodically updates the Policy.\n",
    "\n",
    "\n",
    "*Policy iteration* is an algorithm that improves $\\pi_p$ to $\\pi_{p+1}$ by alternating two steps\n",
    "during round $p$\n",
    "\n",
    "The algorithm alternates between\n",
    "- Policy evaluation\n",
    "    - update the estimate of $\\statevalfun_{\\pi_p}$ to $\\statevalfun_{\\pi_{p+1}}$\n",
    "- Policy improvement\n",
    "    - update $\\pi_p$ to $\\pi_{p+1}$ using the newly updated $\\statevalfun_{\\pi_{p+1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is some pseudo-code\n",
    "\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "    <center><strong>Policy iteration</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/RL_policy_iteration_alg.png\" width=90%>\n",
    "    </tr>\n",
    "    \n",
    "\n",
    "</table>\n",
    "\n",
    "Attribution: http://incompleteideas.net/book/RLbook2020.pdf#page=102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pseudo code for Policy Iteration**\n",
    "\n",
    "<table>\n",
    "    <center><strong>Policy iteration</strong></center>\n",
    "\n",
    "    Initialize policy π arbitrarily (e.g., random policy)\n",
    "    Initialize value function V(s) arbitrarily (e.g., zero for all states)\n",
    "\n",
    "    Repeat:\n",
    "        # Policy Evaluation (compute V for current policy π)\n",
    "        Repeat:\n",
    "            For each state s:\n",
    "                V(s) = R(s, π(s)) + γ * sum over s' [ P(s' | s, π(s)) * V(s') ]\n",
    "            Until V(s) converges (changes smaller than threshold)\n",
    "\n",
    "        # Policy Improvement (update policy based on current V)\n",
    "        policy_stable = True\n",
    "        For each state s:\n",
    "            old_action = π(s)\n",
    "            π(s) = argmax over a [ R(s, a) + γ * sum over s' [ P(s' | s, a) * V(s') ] ]\n",
    "            if old_action != π(s):\n",
    "                policy_stable = False\n",
    "\n",
    "    Until policy_stable is True\n",
    "\n",
    "    Return π, V\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Iterative improvement of $\\statevalfun_{\\pi, k}$ to $\\statevalfun_{\\pi, k+1}$ is via the equation\n",
    "\n",
    "\\begin{array} \\\\\n",
    "\\statevalfun_{\\pi, k+1} (\\state_\\tt) & = &  \\sum_{\\state', \\rew} {\n",
    "\\transp(\\state', r | \\state, \\pi(\\state) ) * \\left( \\rew + \\gamma \\statevalfun_{\\pi, k}(\\state') \\right)\n",
    "} & \\text{Expectation of return across all possible } \\\\\n",
    "& & & \\text{environment responses } \\state' \\\\\n",
    "& & & \\text{given that agent's action is } \\pi(\\state) \\\\\n",
    "\\end{array}\n",
    "\n",
    "We continue iterating  until, for all states $\\state$,\n",
    "- the difference between\n",
    "$\\statevalfun_{\\pi, k+1}(\\state)$ and $\\statevalfun_{\\pi, k}(\\state)$ is smaller than a threshold value $\\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Subtleties**\n",
    "\n",
    "The Policy Evaluation equation\n",
    "\n",
    "\\begin{array} \\\\\n",
    "\\statevalfun_{\\pi, k+1} (\\state_\\tt) & = &  \\sum_{\\state', \\rew} {\n",
    "\\transp(\\state', r | \\state, \\pi(\\state) ) * \\left( \\rew + \\gamma \\statevalfun_{\\pi, k}(\\state') \\right)\n",
    "} & \\text{Expectation of return across all possible } \\\\\n",
    "& & & \\text{environment responses } \\state' \\\\\n",
    "& & & \\text{given that agent's action is } \\pi(\\state) \\\\\n",
    "\\end{array}\n",
    "\n",
    "is *similar* (but not identical) to the Bellman equation.\n",
    "- we assume a *deterministic*  Policy\n",
    "    - $\\pi(\\state)$ in the pseudo-code is a single choice\n",
    "- i.e., the *optimal* one\n",
    "- choosing the action that leads to the successor state $\\state'$ with maximum value\n",
    "    - as determined by the current Value function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The deterministic policy  $\\pi$\n",
    "is improved from\n",
    "$\\pi_p$ to $\\pi_{p+1}$ with the\n",
    "Policy improvement step \n",
    "\n",
    "\\begin{array} \\\\\n",
    "\\pi'_{p+1}(\\state) & = & \\argmax{\\act} \\sum_{\\state', \\rew} {   \\transp(\\state', \\rew | \\state, \\act) ( \\rew + \\disc \\statevalfun_\\pi( \\state' ) )  } & \\text{chose value maximizing action } \\act \\\\\n",
    "\\end{array}\n",
    "\n",
    "That is: the agent in state $\\state$ chooses the action with maximal return.\n",
    "\n",
    "The alternation between Policy Evaluation and Policy Improvement\n",
    "- continues until the updated and previous policy are identical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Convergence to the correct value function is guaranteed\n",
    "- Each update transfers information to a state from all successor states\n",
    "- This ensures that information about all states eventually reaches each affected state\n",
    "\n",
    "The updated policy is no worse than the previous one.\n",
    "- Reference: *Policy Improvement Theorem*\n",
    "\n",
    "There are a finite number of *deterministic* policies\n",
    "$$\n",
    "| \\Actions | ^{ | \\States | }\n",
    "$$\n",
    "\n",
    "So Policy Iteration eventually arrives at the optimal policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The advantage of alternating between Policy Evaluation and Policy Improvement\n",
    "- faster convergence \n",
    "    - the Value function under the current policy is fully evaluated\n",
    "    - before the Policy is updated\n",
    "- more stable convergence\n",
    "    - Policy doesn't change until Value function (under current policy) is fully known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Differences Between Value Iteration and Policy Iteration\n",
    "\n",
    "| Feature | Value Iteration | Policy Iteration |\n",
    "| :-- | :-- | :-- |\n",
    "| **Approach** | Updates value function iteratively using the Bellman Optimality Equation in one step | Alternates between full policy evaluation and policy improvement steps |\n",
    "| **Policy Handling** | Policy is implicitly updated after value convergence | Explicitly maintains and updates policy each iteration |\n",
    "| **Initialization** | Starts with an initial value function | Starts with an initial policy |\n",
    "| **Iteration Steps** | Single step combining evaluation and improvement | Two-step process: separate evaluation and improvement |\n",
    "| **Convergence Criterion** | Value function converges (changes below a threshold) | Policy stabilizes (no change between iterations) |\n",
    "| **Computation per Iteration** | Potentially more expensive per iteration (max over all actions for each state) | More computationally intensive due to full policy evaluation, but fewer total iterations needed |\n",
    "| **Number of Iterations** | Typically more iterations | Usually fewer iterations |\n",
    "| **Complexity** | Simpler to implement | More complex implementation |\n",
    "| **Suitability** | Suitable for smaller state spaces or when full policy evaluation is expensive | Can handle larger state spaces more efficiently when full evaluation is feasible |\n",
    "| **Policy Updates** | Policy derived after convergence of value function | Policy updated after each evaluation phase |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a comparison of the Value and Policy Iteration methods.\n",
    "\n",
    "| Feature              | Value Iteration                                                      | Policy Iteration                                    |\n",
    "|----------------------|---------------------------------------------------------------------|-----------------------------------------------------|\n",
    "| Approach             | Updates value function until convergence                            | Alternates between value evaluation and improvement |\n",
    "| Convergence          | When value function $V(s)$ stabilizes                               | When policy $\\pi(s)$ stops improving                |\n",
    "| Computational Cost   | Higher per iteration, simpler logic                                 | Lower per iteration, more complex                   |\n",
    "| Speed                | Requires more iterations                                            | Fewer iterations; often faster                      |\n",
    "| Policy Output        | Extracted after value convergence                                   | Updated during each iteration                       |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the best action in a Value-based method\n",
    "\n",
    "The Value-based methods don't directly give you a policy\n",
    "- the Value function gives you the best successor state $\\state'$ from current state $\\state$\n",
    "- **but** it doesn't directly tell you the action $\\act$ that leads to $\\state'$\n",
    "\n",
    "In order to find $\\act$ you either\n",
    "- need a model\n",
    "    - search over all possible actions, using the model to determine the value of action's successor state\n",
    "- use actual experience to estimate the effect of performing action $\\act$ in state $\\state$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is some pseudo-code that uses a model to determine the best action:\n",
    "\n",
    "    # For each possible action a in current state s:\n",
    "    for each action a in actions:\n",
    "        # Take action a from state s in the environment\n",
    "        observe next state s', reward r\n",
    "        # Estimate value for taking action a from s\n",
    "        A[a] = r + gamma * V[s']\n",
    "    # Find the maximum estimated action value\n",
    "    A_max = max over a of A[a]\n",
    "    \n",
    "    # Update the value function for state s\n",
    "    V[s] = V[s] + alpha * (A_max - V[s])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value-based methods: Advantages/Disadvantages\n",
    "\n",
    "### Credit assignment (implied intermediate rewards)\n",
    "\n",
    "In many episodes,\n",
    "$$\n",
    "\\ldots \\stateseq_\\tt, \\actseq_\\tt, \\rewseq_{\\tt+1}, \\ldots\n",
    "$$\n",
    "the only reward comes from entering the terminal state, thus\n",
    "$$\n",
    "\\rewseq_{\\tt+1} = 0\n",
    "$$\n",
    "for many time steps $\\tt$.\n",
    "\n",
    "With $\\statevalfun_\\pi$ in-hand\n",
    "- we can interpret the *increment* in value\n",
    "$$\n",
    "\\statevalfun_\\pi (\\stateseq_{\\tt+1}) - \\statevalfun_\\pi ( \\stateseq_\\tt )\n",
    "$$\n",
    "of the action  that takes us from $\\stateseq_\\tt$ to $\\stateseq_{\\tt+1}$\n",
    "- as an implicit reward that provides immediate feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations: deterministic policy; discrete actions\n",
    "\n",
    "Policy is deterministic; can't have stochastic policy\n",
    "\n",
    "$$\n",
    "\\pi^*(\\state) = \\argmax{\\act} \\actvalfun(\\state, \\act)\n",
    "$$\n",
    "- actions are discrete, not continuous\n",
    "    - the magnitude of angles (when turning) or velocity (when moving) are not continuous\n",
    "    - a consequence of the $\\max{\\act'}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model-free methods\n",
    "\n",
    "In the absence of a model\n",
    "- We will learn the dynamics of the Environment\n",
    "- Maintaining *estimates* of the optimal policy\n",
    "- Updating the estimates through the feedback of reward/next state transition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Greed is not (always) good\n",
    "\n",
    "Before presenting specific methods, we discuss the concepts of exploitation and exploration.\n",
    "- which will be used in subsequent methods\n",
    "\n",
    "Given the current approximation of the action value function $\\actvalfun_\\pi(\\state, \\act)$\n",
    "- the obvious policy choice for action $\\actseq_\\tt$ is the one with\n",
    "$$\n",
    "\\max{a'} \\actvalfun_\\pi(\\state_\\tt, a')\n",
    "$$\n",
    "\n",
    "The problem with this greedy choice of action is that, initially, our estimate of the true $\\actvalfun_\\pi$ is\n",
    "inaccurate.\n",
    "- by choosing the current estimate of \"optimal\" action\n",
    "- we may fail to ever choose the true optimal\n",
    "- and we will never learn the optimal action as a result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Choosing the current \"best\" action is called *exploitation*.\n",
    "\n",
    "Sometimes *exploration* (making a seemingly sub-optimal choice) sacrifices short term gain for long term gain.\n",
    "\n",
    "This is called the *exploration-exploitation* trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "The example above \n",
    "- illustrates the concept using the Q-learning method (to be introduced subsequently)\n",
    "- but would be similar for any other method that updates and estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This reflects the primary difference between Model-based and Model-free methods\n",
    "- Model-based methods have knowledge of *all* possible transitions\n",
    "- Model-free methods have knowledge only of the single transition reflected by the chosen action\n",
    "\n",
    "The single experience sampled by Model-free methods is *noisy*\n",
    "- hence, we moderate changes to estimates\n",
    "\n",
    "<br>\n",
    "\n",
    "| Update Style | Method                 | Learning Rate? | Basis of Update          |\n",
    "|:------------ |:---------------------- |:-------------- |:------------------------ |\n",
    "| Replacement  | Model-based (DP)       | No             | All possible transitions |\n",
    "| Incremental  | Model-free (TD, Q, MC) | Yes ($\\\\alpha$) | One sampled transition   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temporal Difference (TD) methods\n",
    "\n",
    "There is a family of methods (TD) in which\n",
    "- there is some estimate associated with a state $\\stateseq_\\tt$ \n",
    "    - $\\statevalfun_{\\pi,k}(\\stateseq_{\\tt})$ for V-learning\n",
    "    - $\\actvalfun_{\\pi,k}(\\stateseq_\\tt, \\actseq_\\tt)$ for Q-learning\n",
    "- the estimate evolves sequentially\n",
    "- by adding an *increment* to the current estimate\n",
    "    - e.g., $$\\statevalfun_{\\pi,k+1}(\\stateseq_{\\tt}) = \\statevalfun_{\\pi,k}(\\stateseq_{\\tt}) + \\delta$$\n",
    "\n",
    "$\\delta$ is called the *Temporal Difference Error*\n",
    "\n",
    "The increment is often moderated by a *learning rate* $\\alpha$\n",
    "\n",
    "$$\\statevalfun_{\\pi,k+1}(\\stateseq_{\\tt}) = \\statevalfun_{\\pi,k}(\\stateseq_{\\tt}) +  \\alpha * \\delta$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " How do we obtain the increment $\\delta$ ?\n",
    " - via the Bellman-like equation that defines the estimate\n",
    " \n",
    " $$\n",
    " \\begin{array} \\\\\n",
    " \\statevalfun_{\\pi,k+1}(\\stateseq_{\\tt})  & = & \\rewseq_{\\tt+1}  & \\text{immediate reward} \\\\\n",
    "  && + \\\\\n",
    "  && \\disc \\statevalfun_{\\pi,k} ( \\stateseq_{\\tt+1})\n",
    "  &\\text{ current estimated value of successor} \\\\\n",
    " \\delta & = &\\statevalfun_{\\pi,k+1}(\\stateseq_{\\tt}) - \\statevalfun_{\\pi,k}(\\stateseq_{\\tt}) \\\\\n",
    " \\end{array}\n",
    " $$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Backups**\n",
    "\n",
    "This equation utilizes a common technique referred to as *value backup*\n",
    "- the value of successor states\n",
    "    $$\\statevalfun_{\\pi,k}(\\stateseq_{\\tt+1})$$\n",
    "- are propagated \"back\" to the current state $\\stateseq_\\tt$\n",
    "\n",
    "The term\n",
    "$$\\rewseq_{\\tt+1} + \\disc \\statevalfun_{\\pi,k}(\\stateseq_{\\tt+1}) $$\n",
    "\n",
    "is called the *target*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The target is considered a \"more informed\" estimate of true\n",
    " $$\\statevalfun_{\\pi}(\\stateseq_\\tt)$$\n",
    " \n",
    "than the current estimate $\\statevalfun_{\\pi,k}(\\stateseq_\\tt)$ as it includes information about\n",
    "- the reward $\\rewseq_{\\tt+1}$\n",
    "    - which will only be received *after* we take the action $\\actseq_\\tt$\n",
    "- and a successor state: $\\statevalfun_{\\pi,k}(\\stateseq_{\\tt+1})$\n",
    "\n",
    "We improve our estimate of $\\statevalfun_\\pi(\\stateseq_\\tt)$ to $\\statevalfun_{\\pi,k+1}(\\stateseq_\\tt)$ \n",
    "- by moving in the direction of the target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Bootstrapping**\n",
    "\n",
    "**Note** that $\\delta$ depends on an *estimated* value\n",
    "- $\\statevalfun_{\\pi,k} ( \\stateseq_{\\tt+1})$\n",
    "\n",
    "So our new estimate for $ \\statevalfun_{\\pi,k+1}(\\stateseq_{\\tt})$ is based on estimate $\\statevalfun_{\\pi,k} ( \\stateseq_{\\tt+1})$.\n",
    "\n",
    "When one estimate is based on another estimate\n",
    "- this is called *bootstrapping*\n",
    "\n",
    "Bootstrapping is the *defining characteristic* of the TD technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**p-step ahead updates**\n",
    "\n",
    "The update above is based on the immediate (one step ahead) reward.\n",
    "\n",
    "This method is called *1-step TD*\n",
    "\n",
    "We can generalize this to *p-step TD* which uses\n",
    "- rewards $\\{ \\rewseq_{\\tt'} \\; | \\;  \\tt \\lt \\tt' \\le \\tt+p \\} $\n",
    "- $\\statevalfun_{\\pi,k} (\\stateseq_{\\tt+p})$\n",
    "\n",
    "\n",
    "$$\n",
    "\\delta = \\left[ \\sum_{i=1}^{p} \\gamma^{i-1} R_{t+i} + \\gamma^{p} \\statevalfun_{\\pi,k}(\\stateseq_{t+p}) \\right] - \\statevalfun_{\\pi,k}(\\stateseq_{\\tt})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the value used for the end state $\\stateseq_{\\tt+p}$\n",
    "is\n",
    "- the pre-update value\n",
    "$$\\statevalfun_{\\pi,k}(\\stateseq_{\\tt+p})$$\n",
    "- rather than the post-update value\n",
    "$$\\statevalfun_{\\pi,k+1}(\\stateseq_{\\tt+p})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " That is\n",
    "- all backups occur simultaneously\n",
    "- not sequentially\n",
    "    - so the update to $\\statevalfun_{\\pi}(\\stateseq_{\\tt+1})$ *does not* influence the update to $\\statevalfun_{\\pi}(\\stateseq_{\\tt})$\n",
    "    - even though we might happen to evaluate it first (bottom up)\n",
    "\n",
    "To illustrate the backups:\n",
    "\n",
    "    State_0 --a--> State_1 --a--> ... --a--> State_T (terminal)\n",
    "\n",
    "    TD:    Update $V(s)$ at every transition using $V(s_{t+1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pseudo-code for p-step TD**\n",
    "\n",
    "We give some code below.\n",
    "\n",
    "The apparent complexity of the code arises\n",
    "- because p-step TD, for $p \\gt 1$\n",
    "- involves returns that won't be experienced\n",
    "- until $(p-1)$ steps after step $\\tt$\n",
    "\n",
    "So the update for $\\statevalfun_{\\pi,k}(\\stateseq_\\tt)$ is delayed $p-1$ steps\n",
    "- until we accumulate the remaining rewards\n",
    "- at the beginning of update $(k+1)$ we already have the value of $\\statevalfun_{\\pi}(\\stateseq_{\\tt+p})$ needed\n",
    "    - i.e., $\\statevalfun_{\\pi,k}(\\stateseq_{\\tt+p})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "        from collections import defaultdict\n",
    "\n",
    "        gamma = 0.99             # Discount factor\n",
    "        alpha = 0.1              # Learning rate\n",
    "        k = 3                    # Number of steps (set k as needed)\n",
    "        V = defaultdict(float)   # State-value function\n",
    "\n",
    "        def td_k_episode(env, policy, k):\n",
    "            states = []\n",
    "            rewards = []\n",
    "\n",
    "            s = env.reset()\n",
    "            states.append(s)\n",
    "            done = False\n",
    "            t = 0\n",
    "\n",
    "            while not done:\n",
    "                a = policy(s)\n",
    "                s_next, r, done, info = env.step(a)\n",
    "                rewards.append(r)\n",
    "                states.append(s_next)\n",
    "                t += 1\n",
    "\n",
    "                # Only update state t-k once enough future steps are observed\n",
    "                if t >= k:\n",
    "                    # Build k-step TD target: sum future rewards, bootstrap from V(states[t+1])\n",
    "                    G = sum(gamma**i * rewards[t-k+i] for i in range(k))\n",
    "                    G += gamma**k * V[states[t+1]]  # Bootstrapped value if not terminal\n",
    "                    V[states[t-k]] += alpha * (G - V[states[t-k]])\n",
    "\n",
    "                s = s_next\n",
    "\n",
    "            # Final updates for states near episode end (insufficient future rewards for full k-step backup)\n",
    "            T = len(states) - 1\n",
    "            for t in range(T - k, T):\n",
    "                if t >= 0:  # skip negative indices for short episodes\n",
    "                    effective_k = T - t\n",
    "                    G = sum(gamma**i * rewards[t+i] for i in range(effective_k))\n",
    "                    V[states[t]] += alpha * (G - V[states[t]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Q-learning: Action-value function\n",
    "\n",
    "We present a TD method called Q-learning.\n",
    "\n",
    "The Value-function method makes it a little awkward to extract the action leading to\n",
    "the successor state with highest value.\n",
    "- The state-value function associates the return (discounted future rewards) with a state\n",
    "- without explicit reference to the action that leads to this return\n",
    "- to find the best action, we either\n",
    "    - need a model; use it to measure the value of each action\n",
    "    - use experience to simulate the effect of each action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple extension of the Value function into an *Action-Value function* simplifies the\n",
    "determination of the next action:\n",
    "\n",
    "$$\n",
    "\\actvalfun_\\pi: \\state \\times \\act \\to \\Reals\n",
    "$$\n",
    "\n",
    "The Action-Value function maps a state and chosen action into the value of the successor state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the action $\\act^*$ that leads to maximum $\\statevalfun_\\pi(\\state')$ \n",
    "is easily obtained from the Action-Value function via\n",
    "\n",
    "$$\n",
    "\\pi^*(\\state) = \\argmax{\\act} \\actvalfun(\\state, \\act)\n",
    "$$\n",
    "\n",
    "Note: the $\\argmax{}$ results in a *deterministic* policy just as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Bellman equation for the Action-Value function is\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\actvalfun_\\pi(\\state, \\act)\n",
    "& = & \\E_\\pi \\left( \\rewseq_{\\tt+1} + \\disc \\max{a'} \\actvalfun_\\pi(\\state_{\\tt+1}, a') \\right) & \\text{immediate reward } \\rewseq_{\\tt+1} \\\\\n",
    "& & & \\text{plus discounted future reward earned by} \\\\\n",
    "& & & \\text{choosing best action } \\act' \\text{ in new state } \\state_{\\tt+1} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Thus\n",
    "$$\n",
    "\\actvalfun_\\pi(\\state, \\act)\n",
    " =  \\E_\\pi ( G_\\tt  | \\stateseq_\\tt = \\state, \\actseq_\\tt = a )\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "*Q-learning* is a method of learning the\n",
    "Action-Value function.\n",
    "\n",
    "It implements the $\\actvalfun_\\pi$ function (mapping state/action pairs to return) via\n",
    "a *tabular* lookup\n",
    "- table is built dynamically through experience\n",
    "\n",
    "It updates the estimated $\\actvalfun_\\pi$ similar to the method used in the Model-based approach.\n",
    "- except that it can only update the action *taken* by the current policy\n",
    "- the experience is gathered by taking the action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pseudo code for Q-learning**\n",
    "\n",
    "    Initialize Q(s, a) arbitrarily for all states s and actions a (often Q(s, a) = 0)\n",
    "    \n",
    "    Set learning rate alpha, discount factor gamma, exploration rate epsilon\n",
    "    \n",
    "    For episode = 1 to number_of_episodes:\n",
    "        Initialize state s\n",
    "        \n",
    "        Repeat until s is terminal:\n",
    "            With probability epsilon:\n",
    "                Choose a random action a (exploration)\n",
    "            Otherwise:\n",
    "                Choose action a = argmax_a Q(s, a) (exploitation)\n",
    "                \n",
    "            Take action a, observe reward r and next state s'\n",
    "            \n",
    "            Update Q(s, a) using:\n",
    "                Q(s, a) = Q(s, a) + alpha * [r + gamma * max_{a'} Q(s', a') - Q(s, a)]\n",
    "                \n",
    "            s = s'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the above code\n",
    "- we build a table $Q$ for the action value function $\\actvalfun_\\pi$ \n",
    "- You can see the $\\epsilon$-greedy strategy\n",
    "\n",
    "        With probability epsilon:\n",
    "            Choose a random action a (exploration)\n",
    "        Otherwise:\n",
    "            Choose action a = argmax_a Q(s, a) (exploitation)\n",
    "- Update assumes subsequent action choices are greedy  \n",
    "\n",
    "        max_{a'} Q(s', a')\n",
    "        \n",
    "        \n",
    "- Model-free: Notice that we don't make use of\n",
    "    -  $\\transp(\\state', \\rew | \\state, \\act)$\n",
    "    - or any reward other than the one received by taking the chosen action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An $\\epsilon$-greedy policy manages the exploration-exploitation trade-off\n",
    "- by choosing the (current) best action with probability $(1 - \\epsilon)$\n",
    "- choosing a random action with probability $\\epsilon$\n",
    "\n",
    "Q-learning will use such an $\\epsilon$-greedy policy ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Q-learning (DQN)\n",
    "\n",
    "The method for Q-learning presented involved creating a *table* implementing the mapping $\\actvalfun_\\pi$.\n",
    "\n",
    "This is only practical when the size of the table is small\n",
    "- the number of states for many problems (e.g., games) is extremely large\n",
    "- not practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Deep Q-Learning*\n",
    "- treats $\\actvalfun_\\pi$ as a function\n",
    "- which is approximated by a Neural Network (the *Deep Q-Network (DQN)*)\n",
    "\n",
    "The function is *trained*\n",
    "- to reproduce the value calculated in ordinary Q-learning\n",
    "- via a Loss function (MSE)\n",
    "    - that minimizes the difference between\n",
    "        - the NN output\n",
    "        - and the mathematical definition of the Q function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is:\n",
    "- We train the NN function $Q_{\\theta}(\\stateseq_\\tt, \\actseq_\\tt)$  to approximate true value $\\actvalfun_\\pi( \\stateseq_\\tt, \\actseq_\\tt)$\n",
    "- Using an MSE per-example loss\n",
    "$$\n",
    "(Q_{\\theta}(\\stateseq_\\tt, \\actseq_\\tt) - \\actvalfun_\\pi( \\stateseq_\\tt, \\actseq_\\tt) )^2\n",
    "$$\n",
    "\n",
    "The loss is\n",
    "- calculated on a mini-batch of steps\n",
    "- gradient descent on the batch loss results in an update to the NN parameters $\\Theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So one fundamental addition to the ordinary Q-learning algorithm\n",
    "- record state transitions in a Replay Memory\n",
    "\n",
    "Mini-batches are sampled from the Replay Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Replay Memory enables *Experience Replay*\n",
    "- off-line training\n",
    "    - can train on the *same state transition* without executing the episode again\n",
    "    - efficient use of transitions (can reuse)\n",
    "- batch creation\n",
    "- having previous transitions in a batch\n",
    "    - prevents forgetting that might occur by only seeing *new* transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Pseudo code for Deep Q-learning**\n",
    "\n",
    "        Initialize replay memory D to capacity N\n",
    "        Initialize main Q-network with random weights θ\n",
    "        Initialize target Q-network with weights θ^- = θ\n",
    "        \n",
    "        Set exploration rate ε, discount factor γ, learning rate α\n",
    "        \n",
    "        For episode = 1 to M:\n",
    "            Initialize state s\n",
    "            \n",
    "            For each step in episode:\n",
    "                With probability ε:\n",
    "                    Choose random action a (exploration)\n",
    "                Else:\n",
    "                    Choose a = argmax_a Q(s, a; θ) (exploitation)\n",
    "\n",
    "                Take action a, observe reward r and next state s'\n",
    "                \n",
    "                Store transition (s, a, r, s') in replay memory D\n",
    "\n",
    "                Sample random mini-batch of transitions (s_j, a_j, r_j, s'_j) from D\n",
    "\n",
    "                For each sample in the batch:\n",
    "                    y_j = r_j + γ * max_{a'} Q(s'_j, a'; θ^-)   # Target Q-value\n",
    "                    # Predicted Q-value\n",
    "                    Q_pred = Q(s_j, a_j; θ)\n",
    "\n",
    "                Compute loss = mean squared error between y_j and Q_pred\n",
    "\n",
    "                Update network weights θ by minimizing loss via gradient descent\n",
    "\n",
    "                Every C steps:\n",
    "                    Update target network weights θ^- = θ\n",
    "\n",
    "                Update state s = s'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Key aspects of the above code:\n",
    "\n",
    "- $\\epsilon$-greedy choice of action\n",
    "\n",
    "- Replay memory D\n",
    "    - stores experiences *using the policy that was in effect* when the experience was created\n",
    "        - not necessarily the current policy (different values of parameter $\\theta$)\n",
    "    - a batch of experiences is sampled from D to train the NN computing `Q_pred`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Updates in a *batch* \n",
    "\n",
    "    - in the \"basic\" value-action method\n",
    "        - $\\actvalfun_\\pi$ is updated for *each action* of the Agent\n",
    "    - in Q-learning\n",
    "        - $Q$ (analagous to $\\actvalfun_\\pi$) is updated with *multiple, randomly chosen* **prior** actions of the Agent\n",
    "        - mini-batch: target computed for each example in the batch\n",
    "\n",
    "                For each sample in the batch:\n",
    "                    y_j = r_j + γ * max_{a'} Q(s'_j, a'; θ^-)   # Target Q-value\n",
    "                    # Predicted Q-value\n",
    "                    Q_pred = Q(s_j, a_j; θ)\n",
    "                    \n",
    "        - per-example loss: compare target `y_j` with prediction `Q_pred`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Loss function: MSE between `Q_pred` and calculated `y_j`\n",
    "\n",
    "        Compute loss = mean squared error between y_j and Q_pred\n",
    "\n",
    "    - `Q_pred` is the value predicted by the NN\n",
    "        - for a *batch* of examples\n",
    "    \n",
    "    - `y_j` is the \"target value\":\n",
    "        - for the batch\n",
    "        - the true value that the NN is trying to match\n",
    "        - *defined* by the same calculation as regular Q-learning (Bellman equation)\n",
    "    \n",
    "        r + gamma * max_{a'} Q(s', a')\n",
    "        \n",
    "        but `Q(s', a')` replaced by NN calculated `Q(s', a'; θ^-)` \n",
    "        - where θ^- are the NN's lagged weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The advantage of updating on multiple actions rather than just the current one\n",
    "- smoother updates\n",
    "    - changes \"averaged\" over many actions, not just the current one\n",
    "- avoids catastrophic forgetting\n",
    "    - emphasizes retention of past knowledge, not just current action\n",
    "    \n",
    "This is all similar to the reason we use Mini-Batch Gradient Descent in Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why the lagged weights $\\theta^-$ in computing the target value via `Q(s', a'; θ^-)` ?\n",
    "\n",
    "To introduce stability in training.\n",
    "\n",
    "If we don't lag the weights: the targets computed for other mini-batches will be based on different weights\n",
    "- so we have a moving target as well as a moving function\n",
    "\n",
    "The lagged weights are periodically synchronized with the most recent weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Q-learning (Action-value function) vs V-learning (Value function)\n",
    "\n",
    "The main difference between the Value function and the Action-Value function\n",
    "- Value function: the value of a state is averaged over *all* actions\n",
    "- Action-Value function\n",
    "    - absent a model: estimate effect of a single action\n",
    "    \n",
    "The averaging in V-learning can make the estimates\n",
    "- more accurate and less noisy\n",
    "- particularly when the action chosen is via the \"max\"\n",
    "\n",
    "(We will quantify this in the section on Bias and Variance)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo (MC) method (not a TD method)\n",
    "\n",
    "In the *Monte Carlo* method\n",
    "- the update $\\delta$ to *each state*\n",
    "- is based on the return to go from the state\n",
    "    - return accumulated *to the end of the episode*\n",
    "- rather than the return of just the next $p$ experiences\n",
    "\n",
    "Loosely, it is TD($\\infty$)\n",
    "\n",
    "\n",
    "**Note**\n",
    "\n",
    "Technically: MC is *not* a TD method\n",
    "- as it does not rely on bootstrapping\n",
    "    - not based on other estimated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To highlight the difference between TD an MC:\n",
    "- consider an episode from initial $\\stateseq_1$ to $\\stateseq_2$ to terminal state $\\stateseq_3$ \n",
    "- earning a reward of $+1$ on the transition from $\\stateseq_1$ and $\\stateseq_2$ \n",
    "- with initial state values\n",
    "$$\\statevalfun_\\pi(\\stateseq_{1,0}) = \\statevalfun_\\pi(\\stateseq_{2,0}) = 0 $$\n",
    "\n",
    "After episode $1$\n",
    "- using TD\n",
    "$$\\statevalfun_\\pi(\\stateseq_{1,1}) = \\statevalfun_\\pi(\\stateseq_{2,1}) = 1 $$\n",
    "- using MC\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\statevalfun_\\pi(\\stateseq_{2,1}) & = 1 & \\text{same as TD} \\\\\n",
    "\\statevalfun_\\pi(\\stateseq_{1,1}) & = 2 & \\text{because it uses } \\statevalfun_\\pi(\\stateseq_{1,1}) = 1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate the backups:\n",
    "\n",
    "    State_0 --a--> State_1 --a--> ... --a--> State_T (terminal)\n",
    "\n",
    "    MC:    Wait for episode to finish, then update $V(s)$\n",
    "    TD:    Update $V(s)$ at every transition using $V(s_{t+1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Method                  | Update Basis                                     | Backup Target                                                     |\n",
    "|:----------------------- |:------------------------------------------------ |:----------------------------------------------------------------- |\n",
    "| TD(0)                   | After 1 step, immediate reward + next value      | $R_{t+1} + \\gamma V(S_{t+1})$                                    |\n",
    "| TD(k)                   | After k steps, sum of k rewards + value at $t+k$ | $R_{t+1} + \\ldots + \\gamma^{k-1} R_{t+k} + \\gamma^{k} V(S_{t+k})$ |\n",
    "| MC (TD($\\infty)$ / TD($\\lambda=1$)) | End of episode, full sample return               | $G_t$ (total return to episode end)                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We summarize the comparison of TD and MC:\n",
    "\n",
    "| Feature            | Monte Carlo                   | Temporal Difference           |\n",
    "|:-------------------|:-----------------------------|:-----------------------------|\n",
    "| Reliance           | Full episode return           | Single step + estimated value |\n",
    "| Bootstrapping      | No                            | Yes                          |\n",
    "| Update Timing      | End of episode                | At every step                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias and Variance\n",
    "\n",
    "We now have multiple methods for RL\n",
    "- how can we compare them ?\n",
    "\n",
    "We will introduce the concepts of Bias and Variance as one metric with which to compare methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be concrete:\n",
    "\n",
    "What are the advantages/disadvantages of Temporal Difference vs Monte Carlo in updating\n",
    "the Value function ?\n",
    "\n",
    "In both methods\n",
    "- the reward that the update to $\\statevalfun_\\pi(\\stateseq_{\\tt,k}) $ depends on  *directly* \n",
    "    - is the immediate one-step reward $\\rew_{\\tt+1}$\n",
    "- subsequent rewards are included via the Value $\\statevalfun_\\pi (\\stateseq_{\\tt+1})$ of the successor state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider iteration $k$:\n",
    "\n",
    "In TD (amd Dynamic Programming)\n",
    "- the value of the successor state is *not* the true end-of-episode $k$ return\n",
    "- it is an *estimate* of the state's value based on the *previous* iteration\n",
    "- updating one estimate using another estimated is called *bootstrapping*\n",
    "\n",
    "In MC\n",
    "- the value of the successor state is the *true* end-of-episode $k$ return\n",
    "\n",
    "Because TD depends on an estimate\n",
    "- we say that TD is *biased*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, let's suppose that rewards are stochastic\n",
    "- hence, each reward is a random variable\n",
    "\n",
    "TD(0) update \n",
    "- depends on **exactly one** random variable $$\\rew_{\\tt+1}$$\n",
    "\n",
    "TD(p) update\n",
    "- depends on $p+1$ random variables: returns for times $\\tt+1, \\ldots, \\tt+p+1$\n",
    "\n",
    "MC update\n",
    "- depends on **at least one**\n",
    "    - rewards of *all* subsequent states of the episode $\\pi$\n",
    " $$\\rew_{\\tt+1}, \\ldots, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the *variance* of $\\delta$\n",
    "- will be smallest for TD(0)\n",
    "- will increase for TD(p) as $p > 0$ increases\n",
    "- will be larger for MC\n",
    "    - remaining length of episode\n",
    "\n",
    "**But**\n",
    "- the *bias* decreases in the opposite direction of the increase of the variance\n",
    "- estimate based on more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias and Variance of the methods presented\n",
    "\n",
    "| Method                   | Bias          | Variance      | Notes                                                                                                        |\n",
    "|:------------------------ |:------------- |:------------- |:------------------------------------------------------------------------------------------------------------ |\n",
    "| DP (Dynamic Programming) | Low           | Low           | Uses full environment model; updates use true expectations.                                                  |\n",
    "| MC (Monte Carlo)         | None          | High          | Unbiased (targets equal expected return), but episodes can have widely varying returns.                      |\n",
    "| TD (Temporal Difference) | Moderate      | Moderate/Low  | Bootstrap introduces bias (approximate next value), but reduces variance compared to MC.                     |\n",
    "| V-learning (State Value) | Moderate      | Low           | Like TD if bootstrapped; low-variance due to value averaging.                                                |\n",
    "| Q-learning               | Moderate/High | Moderate/High | Off-policy, can have bias (maximization, bootstrapping); variance may increase due to noise in max operator. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key Points**\n",
    "\n",
    "DP (Dynamic Programming): Lowest bias and variance, but needs a full model (rarely available in practice).\n",
    "\n",
    "MC: Unbiased estimates, but high variance due to sampling full returns.\n",
    "\n",
    "TD: Bootstrapping introduces bias but greatly lowers variance versus MC.\n",
    "\n",
    "V-learning: Similar profile to TD (if bootstrapped); often lower variance than Q-learning due to value averaging.\n",
    "\n",
    "Q-learning: Can be more biased/variable due to off-policy backups and maximization over sampled estimates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On-policy versus Off-policy\n",
    "\n",
    "For many methods (including Q-learning above) the method uses two sub-policies\n",
    "\n",
    "- the *behavior policy*: the one that chooses an action.  For Q-learning this is\n",
    "\n",
    "        With probability ε:\n",
    "            Choose random action a (exploration)\n",
    "        Else:\n",
    "            Choose a = argmax_a Q(s, a; θ) (exploitation)\n",
    "\n",
    "- the *target policy*: the one that we are trying to learn\n",
    "    - target value\n",
    "    \n",
    "           y_j = r_j + γ * max_{a'} Q(s'_j, a'; θ^-)   # Target Q-value\n",
    "           \n",
    "    - the value to which Gradient Descent will guide the NN\n",
    "    - in minimizing Loss\n",
    "        - compares target `y_j` with prediction `Q_pred = Q(s_j, a_j; θ)`\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When the Behavior and Target policies are identical\n",
    "- we call the method *On-policy*\n",
    "\n",
    "If the Behavior and Target polices are potentially different\n",
    "- we call the method *Off-policy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Q-learning is an **off-policy** method for several reasons.\n",
    "\n",
    "The primary reasons are\n",
    "- target policy is always greedy\n",
    "- behavior policy\n",
    "    - $\\epsilon$-greedy\n",
    "    - example from replay buffer may have been conduced with *an older behavior policy* (different NN weights)\n",
    "        - even if the example's $\\epsilon$-greedy choice was from the \"greedy\" side of the choice\n",
    "\n",
    "| Reason                                 | Behavior Policy (used to collect experience)                                              | Target Policy (used in Q-value update)                                          |\n",
    "|:-------------------------------------- |:----------------------------------------------------------------------------------------- |:------------------------------------------------------------------------------- |\n",
    "| Greedy backup (max operator in update) | Behavior policy may choose non-greedy actions due to exploration                          | Update always targets the action with highest Q-value (optimal policy estimate) |\n",
    "| Experience replay buffer               | Policy used during sampling in past episodes (possibly older $\\epsilon$-greedy or random) | Greedy policy from current network: $a^* = \\argmax{a} Q(s, a; \\theta^-)$     |          |\n",
    "| Policy/parameter mismatch over time    | Behavior policy determined by weights $\\theta$ at time of sampling                       | Target policy determined by weights $\\theta^-$ at time of update (can differ)  |\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SARSA: On-Policy Q-learning\n",
    "\n",
    "We present another member of the TD family.\n",
    "\n",
    "There is a method called *Deep SARSA* that modified Q-learning to be on-policy.\n",
    "- does not uses Experience Replay buffer\n",
    "- changes the target from\n",
    "\n",
    "        y_j = r_j + γ * max_{a'} Q(s'_j, a'; θ^-)\n",
    "\n",
    "- to\n",
    "\n",
    "        y_j = r_j + γ * Q(s'_j, a'_j; θ^-)\n",
    "\n",
    "    - target uses the action choice `a'_j` of current behavior policy\n",
    "    \n",
    "            Q(s'_j, a'_j; θ^-)\n",
    "        \n",
    "    - rather than the action that **maximizes** value\n",
    "    \n",
    "            max_{a'} Q(s'_j, a'; θ^-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Deep SARSA is more stable than DQN\n",
    "- it is \"risk-aware\"\n",
    "    - target is same as behavior when the \"exploratory\" choice is made for policy\n",
    "    - so SARSA can learn to avoid risky choices\n",
    "        - exploratory choices with extreme (negative) rewards\n",
    "- it is more conservative\n",
    "    - DQN always makes the greedy choice\n",
    "    \n",
    "Compared to DQN, these characteristics make it\n",
    "- lower variance\n",
    "- more likely to converge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But SARSA is higher bias  compared to DQN\n",
    "- the exploratory choice is biased away from the true (optimal) policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid-world: comparing conservative SARSA to risk-loving DQN\n",
    "    \n",
    "We need to navigate in a $(4 \\times 12)$ grid\n",
    "- from start cell $S$\n",
    "- to goal cell $G$\n",
    "- without \"falling off the cliff\" (large negative reward: $-100$) by navigating to cliff cells $C$\n",
    "\n",
    "There is a negative reward ($-1$) for each time step so the reward is maximized by getting to the goal quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    ".  .  .  .  .  .  .  .  .  .  .  .\n",
    ".  .  .  .  .  .  .  .  .  .  .  .\n",
    ".  .  .  .  .  .  .  .  .  .  .  .\n",
    "S  C  C  C  C  C  C  C  C  C  C  G\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DQN will favor a path\n",
    "- that hugs the edge of the cliff\n",
    "    - faster route to goal $G$\n",
    "- but that will fall off the cliff during exploration\n",
    "\n",
    "SARSA will avoid the cliff\n",
    "- slower route\n",
    "    \n",
    "<!--- #include (images/cliffwalking_paths.gif) --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/gif": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(filename='images/cliffwalking_paths.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each time a SARSA episode results in falling off a cliff\n",
    "- a large negative value for $Q(s', a')$ is learned when $a'$ leads to falling off the cliff from state $s'$\n",
    "- the large negative reward propagates back through all the preceding (safe) states and actions along the path\n",
    "    - any path that *could* lead to $s'$ with some probability of $a'$ being chosen *becomes deprecated*\n",
    "- so policy becomes biased toward moving far from the Cliff\n",
    "- *even if* the action $a'$ was an *exploratory* action (that won't be repeated)\n",
    "\n",
    "DQN always assumes the continuation of an episode from the current state is optimal\n",
    "- the `max` prevents $a'$ from being chosen in state $s'$\n",
    "- so any path that leads to $s'$ *does not become deprecated*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SARSA vs DQN: Summary\n",
    "\n",
    "| Feature                     | SARSA (On-Policy)                                             | DQN (Off-Policy)                                                 |\n",
    "|:--------------------------- |:------------------------------------------------------------- |:---------------------------------------------------------------- |\n",
    "| Policy Updated              | Behavior policy ($\\epsilon$-greedy)                          | Greedy (optimal) policy                                          |\n",
    "| Update Target               | $r + \\gamma Q(s', a')$ (actual next action)                  | $r + \\gamma \\max{a'} Q(s', a')$ (max over all actions)        |\n",
    "| Bias/Variance               | Higher bias, lower variance                                   | Lower bias, higher variance (risk of instability/overestimation) |\n",
    "| Risk Awareness              | Accounts for exploration risk (safer in risky environments)   | Does not account for risk of exploratory actions                 |\n",
    "| Performance in Large Spaces | Limited without function approx.                              | Handles large/continuous spaces with neural networks             |\n",
    "| Experience Replay           | Possible but less common                                      | Standard in DQN                                                  |\n",
    "| Stability                   | Often more stable, especially in risky/confusing environments | Can be less stable, especially with bad hyperparameters          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning via Code\n",
    "\n",
    "[Here](RL_OnPolicy_vs_OffPolicy_code_examples.ipynb) is a notebook that demonstrates\n",
    "- Q-Learning\n",
    "    - with and without replay memor\n",
    "- SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How Each Example Highlights a Salient Point\n",
    "\n",
    "1. **Q-learning (Off-policy):**\n",
    "    - **Purpose:** Demonstrates how the behavior policy (epsilon-greedy; explores randomly) differs from the target policy (greedy; always chooses highest Q-value).\n",
    "    - **Salient Point:** Shows that updates use the greedy maximum Q-value (off-policy), even when the sampled action was exploratory.\n",
    "2. **SARSA (On-policy):**\n",
    "    - **Purpose:** Demonstrates on-policy learning where both the behavior and target policies are epsilon-greedy and always match.\n",
    "    - **Salient Point:** The Q update uses the value of the exact next action chosen by the behavior policy, making sampling and updating fully aligned.\n",
    "3. **Q-learning with Experience Replay Buffer:**\n",
    "    - **Purpose:** Shows how experience transitions are stored in a replay buffer and reused for updates.\n",
    "    - **Salient Point:** Illustrates how off-policy Q-learning leverages exploratory samples and batch updates from the buffer to improve sample efficiency and stabilize learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sources of Variance in RL\n",
    "\n",
    "We introduced Variance as a way of comparing TD and MC for value-based methods.\n",
    "\n",
    "But variance is present in many RL methods and is considered \n",
    " a potential impediment to Learning via RL.\n",
    "- bigger updates in\n",
    "    - Value function for Value-based methods\n",
    "    - Parameters for Policy-based methods\n",
    "- can cause large changes in Policy\n",
    "- which can lead to unstable training\n",
    "\n",
    "We list the types of methods affected by each cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There multiple sources of Variance, which we summarize below\n",
    "- stochastic environment\n",
    "    - stochastic rewards and state transitions\n",
    "- stochastic policy\n",
    "- sparse rewards\n",
    "    - per-episode vs per-step rewards\n",
    "    - reward estimates become more noisy\n",
    "- bootstrapping    \n",
    "\n",
    "\n",
    "| Source of Variance             | Description                                                       | Methods Most Affected                |\n",
    "|:-------------------------------|:-------------------------------------------------------------------|:--------------------------------------|\n",
    "| Environmental Stochasticity   | Random rewards and transitions cause unpredictable outcomes        | Monte Carlo, TD, Policy Gradients    |\n",
    "| Policy Stochasticity          | Probabilistic (non-deterministic) action selection                | MC, On-policy, Policy Gradients      |\n",
    "| Long Trajectory Aggregation   | Returns summed over many steps compound the randomness             | Monte Carlo, n-step methods          |\n",
    "| Sparse or Delayed Rewards     | Few positive signals lead to noisy estimates                       | Monte Carlo, Value-based             |\n",
    "| Bootstrapping Error           | Using own predictions as targets introduces bias, but lowers variance | TD, Q-Learning                    |\n",
    "| Credit Assignment Difficulty  | Uncertainty in linking actions to future rewards increases variance | Policy Gradients, MC                 |\n",
    "| Sample Size & Exploration     | Insufficient samples or aggressive exploration cause wide fluctuations| All methods                       |\n",
    "| Model/Optimization Instability| Neural network or optimizer issues amplify variance                | Deep RL, Policy Gradient methods     |\n",
    "| Non-stationarity              | Changing environment or policy during training                     | All methods, esp. online learning    |\n",
    "\n",
    "**Key Points:**  \n",
    "- Variance comes from randomness in data, policy, training procedure, and modeling choices.  \n",
    "- Some sources can be controlled with design choices (e.g., baselines, bootstrapping, averaging), while others are intrinsic to RL problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we introduce new RL methods\n",
    "- one motivation if to reduce variance\n",
    "\n",
    "Some potential ways to reduce variance\n",
    "- $n \\gt 1$-step ahead Temporal Difference\n",
    "- estimating updates in mini-batches\n",
    "    - as in Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
