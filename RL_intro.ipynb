{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [HuggingFace Deep RL course](https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt)\n",
    "- [HuggingFace Deep RL course github](https://github.com/huggingface/deep-rl-class)\n",
    "- [Reinforcement Learning book: Sutton](http://incompleteideas.net/book/RLbook2020.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction: What is Reinforcement Learning (RL) ?\n",
    "\n",
    "We have previously learned a form of learning called *Supervised Learning*\n",
    "- learning a function from examples/demonstrations of the input/output relationship\n",
    "\n",
    "We will now consider another form of learning called *Reinforcement Learning*.\n",
    "\n",
    "Reinforcement Learning is the process whereby an *Agent* (the learner)\n",
    "- learns a function by trial and error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In contrast to Supervised Learning\n",
    "- where the learner learns from labeled examples\n",
    "    - mappings from input to output\n",
    "\n",
    "in Reinforcement Learning, the learner gathers information by interacting with the world\n",
    "- The Agent is able to partially observe information (the *State*) about the world\n",
    "\n",
    "- Given the State, the Agent has an available set of *Actions* that can be performed.\n",
    "\n",
    "- the Agent's function (the *Policy*) guides its behavior: mapping the current State to an Action to perform\n",
    "\n",
    "- the Agent performs the action \n",
    "\n",
    "- The *Environment* responds to the action\n",
    "    - with a *Reward*\n",
    "    - and a new State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is the Reward that serves as feedback for the Agent and guides it toward a solution\n",
    "- the Agent's goal is maximization of Reward\n",
    "- rather than minimization of Loss as in Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This interaction between Agent and Environment may continue for multiple steps\n",
    "- multi-step sequence of State/Action/Reward/New State is called an *episode* or *trajectory*\n",
    "\n",
    "The Agent's goal in formulating its Policy is maximization of reward received over the trajectory\n",
    "- *Return* is the cumulative Reward (received over the sequence of chosen actions)\n",
    "\n",
    "So Rewards are used by the Agent as a form of feedback\n",
    "- evaluating the chosen Action\n",
    "- guidance used to learn an optimal Policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each episode/trajectory in analogous to an example\n",
    "- deriving an optimal policy may require many episodes\n",
    "- until an episode yield Maximum Return is achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Reinforcement Learning: information flow</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/RL_agent_env_0.png\" width=80%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://mlvu.github.io/lecture13/71.ReinforcementLearning.key-stage-0003.svg\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Frozen Lake\n",
    "\n",
    "Here is an example: Frozen Lake\n",
    "- the goal is for the character who starts in the upper left corner of a grid\n",
    "- to learn a path to the gift in the lower right corner\n",
    "- while traversing a grid on a frozen lake\n",
    "- without falling into holes in the ice\n",
    "\n",
    "<img src=\"images/Frozen_Lake_env.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each element of the grid is a State.\n",
    "\n",
    "The Action space for the character is\n",
    "- Move: { Left, Right, Up, Down }\n",
    "\n",
    "The Environment responds to an Action\n",
    "- by moving the character to a new state\n",
    "- potentially giving a Reward\n",
    "    - large positive reward for reaching the state with the gift\n",
    "    - large negative reward for falling into a hole\n",
    "    - optional intermediate rewards\n",
    "        - for getting closer to the goal\n",
    "        \n",
    "The Agent plays the game for multiple episodes\n",
    "- until it learns how to safely navigate to the gift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison with Supervised Learning\n",
    "\n",
    "There is an important difference between Reinforcement Learning and Supervised Learning.\n",
    "\n",
    "In Supervised Learning, we have a *target*\n",
    "- Loss is minimized when the Agent reproduces the target **exactly**\n",
    "- it is a form of imitation\n",
    "\n",
    "In Reinforcement Learning\n",
    "- we can give a reward for the *quality* of an answer, not just its syntactic form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider Frozen Lake.\n",
    "\n",
    "Using SFT\n",
    "- we create examples\n",
    "    - each is a sequence of (State,Action) pairs that eventually lead to the gift\n",
    "- train a model to start from an empty sequence and predict the next state\n",
    "\n",
    "The Agent learns to imitate successful paths\n",
    "- without explicit guidance to the *principles* that lead to success\n",
    "    - avoid falling in holes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using Reinforcement Learning\n",
    "- we give a large negative reward for falling into a hole\n",
    "    - demonstrating the principle to avoid danger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Continuous versus discrete**\n",
    "\n",
    "Continuing the comparison\n",
    "- Supervised Learning's optimization is: Loss Minimization\n",
    "    - Loss is continuous, differentiable\n",
    "- Reinforcement Learning's optimization is: Return Maximization\n",
    "    - Rewards may be discrete, rather than continuous\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Reinforcement Learning to post-train an LLM to become an AI Assistant**\n",
    "\n",
    "Another example\n",
    "- reflecting the particular interest in this class to training AI Assistants.\n",
    "\n",
    "Consider how a model might learn to answer the question\n",
    "\n",
    "    How are you today ?\n",
    "\n",
    "In Supervised Learning, there is a single Target\n",
    "\n",
    "    I feel fine, thank you\n",
    "    \n",
    "But there may be an equally acceptable response\n",
    "\n",
    "    Great, thanks for asking\n",
    "\n",
    "Yet, this *semantically similar* response\n",
    "- results in a Loss\n",
    "\n",
    "as does a completely inappropriate response\n",
    "\n",
    "    The sky is blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Reinforcement Learning, we can acknowledge multiple answers and rank them (via reward)\n",
    "- the Return can be a measure of the *quality* of the response\n",
    "- rather than how *syntactically similar* it is to the target\n",
    "\n",
    "\n",
    "    Fine.\n",
    "    \n",
    "    Fine thank you.\n",
    "    \n",
    "    Fine, thank you, and how are you feeling ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Comment**\n",
    "\n",
    "Imitation feels \"shallow\", overly quantitative\n",
    "- emphasizing syntactic equivalence with the target\n",
    "- the \"what\" rather than the \"why\"\n",
    "\n",
    "Learning from experience feels \"deeper\", qualitative\n",
    "- emphasizing the result (semantics) rather than the exact path to the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Comparison: Reinforcement Learning vs Supervised Learning**\n",
    "\n",
    "| Aspect           | Reinforcement Learning (RL)                        | Supervised Learning (SL)                       |\n",
    "|:------------------------|:-------------------------------------------------|:------------------------------------------------|\n",
    "| Learning Signal   | Reward signal potentially delayed and sparse      | Direct feedback with labeled input-output pairs |\n",
    "| Data Acquisition  | Data collected through interaction with environment | Data typically fixed and pre-collected         |\n",
    "| Goal             | Learn a policy to maximize cumulative future reward| Learn a function mapping inputs to outputs     |\n",
    "| Feedback Type    | Scalar reward signal, often sparse and delayed     | Exact target labels for each input              |\n",
    "| Training Setup   | Trial-and-error interaction, sequential data        | Independent and identically distributed (i.i.d.) samples |\n",
    "| Exploration      | Critical to discover effective actions             | Usually not required, data is given              |\n",
    "| Optimization Target| Maximize expected cumulative reward (possibly stochastic) | Minimize empirical loss over dataset            |\n",
    "| Environment Model| May be unknown or partially known, learning from experience | Typically no environment dynamics involved      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Challenges of RL\n",
    "\n",
    "**State**\n",
    "- The \"full State\" (all relevant information) may not be visible\n",
    "    - The State available to the agent is *partially observable*\n",
    "\n",
    "For example, suppose you (the Agent) are playing a game against a computer (the Environment)\n",
    "- Chess/Go: full state is visible to Agent\n",
    "- Poker: opponent (Environment) cards are not visible to Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Rewards**\n",
    "\n",
    "*Sparse* rewards\n",
    "- rewards may *not be received at every step*\n",
    "- in the limit\n",
    "    - single reward at end of trajectory\n",
    "    - e.g., your assignment grade is received when you complete the assignment, not at every  partial solution\n",
    "\n",
    "Consider Frozen Lake\n",
    "- harder for Agent to learn\n",
    "- when the *only reward* is received in a terminal state\n",
    "    - fall into hole/receive gift\n",
    "- versus a per-step reward\n",
    "    - \"hot\"/\"cold\": closer/farther from gift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    \n",
    "**Environment**\n",
    "- state transition may be stochastic rather than deterministic\n",
    "\n",
    "In Frozen Lake\n",
    "- the surface of the lake is covered in ice\n",
    "- an Agent trying to move in one direction may slip\n",
    "    - and not wind up where expected\n",
    "    \n",
    "The Environment\n",
    "- with some non-zero probability\n",
    "- will move it to a state in the *opposite* direction\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Policy**\n",
    "- action choice may be stochastic\n",
    "- Policy is based only on State, not the trajectory\n",
    "    - *Partially Observable Markov Decision Process (POMDP)*\n",
    "    \n",
    "Given the possible stochastic nature of the trajectory\n",
    "- goal is maximization of *Expected Reward*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Challenges in finding an optimal policy**\n",
    "\n",
    "The Agent learns to update the policy through experience.\n",
    "\n",
    "The quality of the experience matters\n",
    "- you can't become an expert by only playing against weak opponents\n",
    "- the agent may never achieve true optimal policy\n",
    "    - for games where the optimal opponent strategy is not known (or is intractable computationally)\n",
    "    - the agent can become the best *current* player of the game of Go\n",
    "        - without a truly optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploration versus Exploitation\n",
    "\n",
    "The agent's policy evolves with experience.\n",
    "- accumulates more information about rewards and the environment through new episodes\n",
    "- in the interim, it only has partial knowledge\n",
    "\n",
    "Thus, if the agent\n",
    "- chooses the same action $\\act$ every time it visits state $\\state$\n",
    "    - when the policy has not changed since the last visit\n",
    "- it will never gain knowledge about other possible continuations of the trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example\n",
    "- choosing alternate action $\\act'$ may lead to a trajectory with higher return\n",
    "\n",
    "\n",
    "The dilemma is called *exploration versus exploitation*\n",
    "- *exploitation*: always choose the action with highest forward return\n",
    "    - based on current limited knowledge\n",
    "- *exploration*: take a new action, in order to explore alternate possible forward returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation\n",
    "\n",
    "Term &nbsp; | Definition\n",
    ":---|:---\n",
    "$\\States$   | **Set** of possible states\n",
    "$\\Actions$  | **Set** of possible actions\n",
    "$\\Rewards$   | **function** $\\States \\times \\Actions \\to \\Reals$\n",
    "&nbsp; | maps state and action to a reward\n",
    "$\\disc$      | discount factor for reward one step in future\n",
    "$\\stateseq_\\tt$  | The state at beginning of time step $\\tt$\n",
    "$\\actseq_\\tt$    | The action performed at time step $\\tt$\n",
    "$\\rewseq_{\\tt +1}$    | The reward resulting form the action performed at time step $\\tt$   \n",
    "$\\transp$     | **Function** $\\States \\times \\Actions \\to \\States \\times \\Rewards$\n",
    "&nbsp; | Transition probability: maps state and chosen action \n",
    "&nbsp; | to  new state and reward\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can begin to develop some notation to describe what is happening.\n",
    "\n",
    "An *episode* (or *trajectory*) is a sequence that records the events as agent\n",
    "follows its policy in making decisions.\n",
    "\n",
    "Here is a timeline of an episode\n",
    "- column labeled \"Agent\": actions chosen by the Agent\n",
    "- column labeled \"Environment\": the responses generated in reaction to the decision\n",
    "\n",
    "Step  &nbsp; &nbsp;| Agent &nbsp;&nbsp; | Environment | Notes\n",
    ":---|:---|:---|:---\n",
    "0 | | $\\stateseq_0$    | Environment chooses initial state\n",
    "  | | $\\pi(\\stateseq_0)$ | $\\rewseq_1, \\stateseq_1$ | Agent observes $\\stateseq_0$\n",
    "  | |  Chooses action $\\pi(\\stateseq_0)$ according to policy $\\pi$| Environment gives reward $\\rewseq_1$\n",
    "  | | | Environment updates state to $\\stateseq_1$ \n",
    "1 | $\\pi(\\stateseq_1)$ | $\\rewseq_2, \\stateseq_2$ | Agent continues to follow policy $\\pi$\n",
    "$\\vdots$\n",
    "$\\tt$ | $\\pi(\\stateseq_\\tt)$ | $\\rewseq_{\\tt+1}, \\stateseq_{\\tt+1}$| Agent follows policy to choose action $\\pi(\\stateseq_\\tt)$\n",
    "  | |  Chooses action $\\pi(\\stateseq_\\tt)$ according to policy $\\pi$| Environment gives reward $\\rewseq_{\\tt+1}$\n",
    "  | | | Environment updates state to $\\stateseq_{\\tt+1}$ \n",
    "$\\vdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Notes on episode notation**\n",
    "\n",
    "The triple of elements corresponding to experience number $\\tt$\n",
    "- is $$\\stateseq_\\tt, \\actseq_\\tt, \\rewseq_{\\tt+1}$$ \n",
    "- **not** $$\\stateseq_\\tt, \\actseq_\\tt, \\rewseq_\\tt$$\n",
    "\n",
    "following the \n",
    " notational standard of the [Sutton and Barto book](http://incompleteideas.net/book/RLbook2020.pdf).\n",
    " \n",
    "The latter notation is also used, but less common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We also develop notation to describe the behavior of\n",
    "- the Agent: policy\n",
    "- the Environment: transition probability to reward and successor state\n",
    "\n",
    "**Definitions relative to an episode**\n",
    "\n",
    "\\begin{array}[lll]\\\\\n",
    "\\stateseq_0, \\actseq_0, \\rewseq_1, \\ldots \\stateseq_\\tt, \\actseq_\\tt, \\rewseq_{\\tt+1}, \\ldots\n",
    " & \\textbf{sequence}: \\text{Episode/Trajectory} \\\\\n",
    "& \\text{sequence of states, action performed, reward received} \\\\\n",
    "\\transp({ \\state', \\rew | \\state, \\act }) & \\textbf{Transition probability} \\text{ (rules of the game)} \\\\\n",
    "& = \\transp({\\stateseq_\\tt = \\state', \\rewseq_\\tt = \\rew | \\stateseq_{\\tt-1} = \\state, \\actseq_{\\tt-1} = \\act }) \\\\\n",
    " & \\textbf{Markov Decision Process} \\text{: depends only on }  \\stateseq_{\\tt-1} \\\\\n",
    " & \\text{and not on history } \\stateseq_0, \\ldots, \\stateseq_{\\tt-1} \\\\\n",
    "\\pi(\\act | \\state) & \\text{Policy (decision process for agent)} \\\\\n",
    "                   & \\textbf{probability distribution over } \\Actions \\\\\n",
    "\\pi(\\state)        & \\text{the action } \\act \\text{ chosen by the policy from state } \\state \\\\\n",
    "                   & \\text{abuse of notation: } \\pi(\\state) \\text{ is a probability distribution over } \\Actions \\\\\n",
    "G_\\tt & \\text{return/return to go} \\\\\n",
    "                   & \\text{cumulative rewards (discounted by } \\gamma \\text{) in the episode starting from step }  \\tt \\\\\n",
    "                   & = \\sum_{k=0}^\\tt {  \\gamma^k * \\rewseq_{\\tt+k+1} } \\\\\n",
    "                   & = \\rew_{\\tt+1}  + \\gamma * G_{\\tt+1} \\\\\n",
    "                   & \\,\\,\\,\\, \\text{ where } \\gamma \\text{ is a factor for discounting future returns.}\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Reinforcement Learning: information flow (with labels)</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/RL_agent_env.png\" width=100%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: completeideas.net/book/RLbook2020.pdf#page=70\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding an optimal Policy: Solving an RL system\n",
    "\n",
    "A policy $\\pi$ is a function mapping a state to an action.\n",
    "\n",
    "It is the \"algorithm\" that guides the actions behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"solution\" to an RL system is the optimal policy $\\pi^*$ that maximizes *return* from the initial state\n",
    "- return is sum of discounted rewards accumulated by following the policy from a given state\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "G_\\tt & = & \\sum_{k=0}^\\tt {  \\gamma^k * \\rewseq_{\\tt+k+1} } \\\\\n",
    "      & = & \\rew_{\\tt+1}  + \\gamma * G_{\\tt+1} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\gamma \\le 1$ is a factor for discounting future returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note on the discount factor**\n",
    "\n",
    "What is the purpose of the discount factor ?\n",
    "\n",
    "Given two trajectories with *equal* return\n",
    "- we sometimes want to favor the *shorter* trajectory\n",
    "    - favor direct path over indirect path\n",
    "    - favor immediate rewards to deferred rewards\n",
    "- the discount factor is a way of expressing our preference\n",
    "\n",
    "For simplicity of presentation, we often assume $$\\gamma = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we find the optimal policy $\\pi$ ?\n",
    "\n",
    "\n",
    "\n",
    "The way we find the optimal policy is typically via an iterative process\n",
    "- We construct a sequence of improving policies \n",
    "$$\n",
    "\\pi_0, \\ldots, \\pi_p, \\ldots\n",
    "$$\n",
    "that hopefully converges to $\\pi^*$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For simplification, let us assume for the moment that\n",
    "- the sets of states, actions and rewards be *finite*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RL Methods Landscape\n",
    "\n",
    "<img src=\"images/rl_conceptual_map.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Graph Edge Explanations\n",
    "\n",
    "This graph illustrates the conceptual relationships and classifications within Reinforcement Learning methods.\n",
    "\n",
    "**General Arrows:**\n",
    "*   Arrows represent a hierarchical or conceptual relationship, indicating that a method or family falls under another category, or that one method incorporates another.\n",
    "\n",
    "**Colored Arrows from Family Nodes (Depth 1):**\n",
    "These arrows indicate the primary classification of the methods into different RL families:\n",
    "*   **Green (Solid, from 'Value-based'):** Methods primarily focused on estimating and optimizing value functions.\n",
    "*   **Red (Dashed, from 'Policy-based'):** Methods focused on directly learning and optimizing a policy.\n",
    "*   **Purple (Dotted, from 'Model-based'):** Methods that build or use a model of the environment.\n",
    "*   **Orange (Dashdot, from 'Model-free'):** Methods that learn directly from experience without needing an explicit model of the environment.\n",
    "\n",
    "**Specific Blue Arrow:**\n",
    "*   **Blue (Solid, from 'Actor-Critic' to 'TD(0)'):** This specific connection highlights that the 'Critic' component of many Actor-Critic algorithms typically uses Temporal Difference (TD) learning, often `TD(0)` or similar TD updates, to estimate the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model-Based vs Model-Free RL\n",
    "\n",
    "There are two main approaches to solving an RL system.\n",
    "\n",
    "A *model-based* approach uses a model of the environment in forming a solution.\n",
    "\n",
    "The model defines the response of the Environment to an action of the Agent\n",
    "- $\\transp({ \\state', \\rew | \\state, \\act })$\n",
    "\n",
    "The model can be either\n",
    "- given\n",
    "- learned\n",
    "\n",
    "The advantage of having a model is that \n",
    "- the Agent can explore the consequences of an action\n",
    "without having to perform an episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A *model-free* approach does not rely on a pre-defined model\n",
    "- it learns from interacting with the environment\n",
    "\n",
    "\n",
    "- Model-Based RL: Learns or uses the dynamics model \\$ P(s'|s,a) \\$.\n",
    "    - Example methods: Dynamic Programming, Dyna, Monte Carlo Tree Search.\n",
    "- Model-Free RL: Learns policies or value functions without modeling \\$ P \\$.\n",
    "    - Example methods: Q-Learning, SARSA, Policy Gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To illustrate with Frozen Lake:\n",
    "\n",
    "<img src=\"images/Frozen_Lake_env.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Model-based:\n",
    "\n",
    "The character has\n",
    "- has a global view of the grid\n",
    "- knowledge of the results of its actions\n",
    "\n",
    "It can find an optimal policy (route)\n",
    "- without \"playing the game\" (interacting with the Environment)\n",
    "    - optimization problem with no unknowns other than the route\n",
    "    - search by simulating the consequences of its action by calling the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Model-free:\n",
    "\n",
    "The character has **no** initial knowledge\n",
    "- of the layout of the grid, location of gift/holes\n",
    "- consequence of its actions (what is successor state)\n",
    "\n",
    "It must \"play the game\" in order to accumulate experience and formulate policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value-Based vs Policy-Based Methods\n",
    "\n",
    "There are several major approaches to solving an RL problem\n",
    "\n",
    "- Value-Based Methods\n",
    "    - Learn function to either\n",
    "        - map state $\\state$ to an expected return $\\statevalfun(\\state)$\n",
    "        - map state-action pair to an expected return $\\actvalfun_\\pi(\\state, \\act)$\n",
    "    - Derive policies indirectly from these functions\n",
    "    - Examples: Q-learning, DQN.\n",
    "    \n",
    "- Policy-Based Methods\n",
    "    - Learn the policy \\$ \\pi( \\act \\; | \\; \\state)$ directly.\n",
    "    - Examples: REINFORCE, Actor-Critic, PPO.\n",
    "- Actor-Critic\n",
    "    - Hybrid methods combining policy and value learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## On-Policy vs Off-Policy Methods\n",
    "\n",
    "Some methods involve two potentially distinct choices for actions.\n",
    "\n",
    "- the *behavior policy*: the one that choose an action *while learning*\n",
    "   \n",
    "- the *target policy*: the one that we are trying to learn; reflected in the update\n",
    "\n",
    "An *On-Policy* method\n",
    "- Behavior and Target policies are the same\n",
    "- Learn from the policy used to generate data (\\$ \\pi \\$).\n",
    "    - Examples: SARSA, REINFORCE.\n",
    "\n",
    "An *Off-policy* method\n",
    "- Behavior and Target policies are different\n",
    "- Learn about a target policy \\$ \\pi \\$ different from behavior policy \\$ \\mu \\$.\n",
    "    - Examples: Q-learning, DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## On-Line vs Off-Line RL\n",
    "\n",
    "The experience could be gained either\n",
    "- *On-line*: playing the game\n",
    "- *Off-line*: replaying the experiences of prior attempts   \n",
    "\n",
    "- On-Line RL: Updates and learns from interaction with environment in real time.\n",
    "- Off-Line RL: Learns from a fixed dataset without further interaction.\n",
    "    - Applications: healthcare, robotics.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "    <br>\n",
    "\n",
    "<table>\n",
    "    <center><strong>Reinforcement Learning: Online vs Offline</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/RL_online_offline.png\" width=100%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://www.ibm.com/think/topics/reinforcement-learning\n",
    "</table>\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Core RL Algorithms and Their Categories\n",
    "\n",
    "| Algorithm | Model-Based | Model-Free | Value-Based | Policy-Based | On-Policy | Off-Policy | On-Line | Off-Line |\n",
    "| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "| Value Iteration | ✓ |  | ✓ |  |  |  |  |  |\n",
    "| Policy Iteration | ✓ |  | ✓ | ✓ |  |  |  |  |\n",
    "| SARSA |  | ✓ | ✓ |  | ✓ |  | ✓ |  |\n",
    "| Q-learning |  | ✓ | ✓ |  |  | ✓ | ✓ |  |\n",
    "| REINFORCE |  | ✓ |  | ✓ | ✓ |  | ✓ |  |\n",
    "| Actor-Critic |  | ✓ | ✓ | ✓ | ✓ | Some | ✓ |  |\n",
    "| Deep Q-Networks (DQN) |  | ✓ | ✓ |  |  | ✓ | ✓ | ✓ |\n",
    "| PPO, A2C/A3C |  | ✓ | ✓ | ✓ | ✓ |  | ✓ |  |\n",
    "| Batch RL |  | ✓ | ✓ |  |  | ✓ |  | ✓ |\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Reinforcement Learning\n",
    "\n",
    "*Deep Reinforcement Learning* refers to the special case of Reinforcement Learning where\n",
    "- the *policy* is a parameterized (by $\\theta$) function mapping  states $\\stateseq$ to  (a probability distribution) actions\n",
    "    $$\n",
    "    \\pi_\\theta( \\actseq | \\stateseq )\n",
    "    $$\n",
    "- implemented as a Neural Network\n",
    "   \n",
    "Our initial presentation will be of fixed (non-parameterized) policies.\n",
    "- We will subsequently introduce parameterized Neural Networks to implement the functions we define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
