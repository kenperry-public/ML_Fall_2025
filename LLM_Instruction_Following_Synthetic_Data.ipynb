{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**References**\n",
    "- [SELF-Instruct paper](https://arxiv.org/pdf/2212.10560.pdf)\n",
    "- [Self-Alignment with Instruction Backtranslation](https://arxiv.org/pdf/2308.06259.pdf)\n",
    "- [Large Language Models can Self-Improve](https://arxiv.org/pdf/2210.11610.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using an LLM to generate Instruction Following examples\n",
    "\n",
    "In the module on [Instruction Following](LLM_Instruction_Following.ipynb)\n",
    "- we motivated the use of Fine-Tuning a LLM\n",
    "- to exhibit Instruction Following behavior\n",
    "\n",
    "Recall: an example of Instruction Following behavior is a triple\n",
    "\n",
    "$$\\langle \\text{Instruction}, \\text{Context}, \\text{Response} \\rangle $$\n",
    "\n",
    "for example\n",
    "- Instruction: \"Tell me the word that is the opposite of the word that I input\"\n",
    "- Context: \"Input: Stop\"\n",
    "- Response: \"Go\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Instruction describes the task to be accomplished\n",
    "- relationship between Input and Response\n",
    "- the Input/Response pair is an exemplar for this task\n",
    "\n",
    "In this module, we explore methods\n",
    "- to generate these fine-tuning examples\n",
    "- to improve examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using an LLM to generate Instruction Following examples\n",
    "\n",
    "**Reference**\n",
    "\n",
    "[SELF-Instruct paper](https://arxiv.org/pdf/2212.10560.pdf)\n",
    "\n",
    "Is there an alternative to the labor-intensity of constructing Instruction Following examples by human ?\n",
    "\n",
    "The answer is \"Yes\"\n",
    "- and involves the use of In-Context learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can imagine the process as\n",
    "- starting with a small number $k$ of human-constructed examples\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\langle \\text{Instruction}^{(1)}, \\text{Context}^{(1)}, \\text{Response}^{(1)} \\rangle \\\\\n",
    "\\vdots \\\\\n",
    "\\langle \\text{Instruction}^{(k)}, \\text{Context}^{(k)}, \\text{Response}^{(k)} \\rangle \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- which are used as exemplars in a *few-shot* learning prompt\n",
    "\n",
    "the hope is that the LLM will infer\n",
    "- given a non-exemplar prompt (one without a Response)\n",
    "\n",
    "$$\n",
    "\\langle \\text{Instruction}^{(k+1)}, \\text{Context}^{(k+1)}, \\rangle \n",
    "$$\n",
    "\n",
    "it's desired completion adds the Response\n",
    "\n",
    "$$\n",
    "\\langle \\text{Instruction}^{(k+1)}, \\text{Context}^{(k+1)}, \\text{Response}^{(k+1)} \\rangle \\\\\n",
    "$$\n",
    "\n",
    "The human and LLM generated examples can then be used to Fine Tune an LLM to better demonstrate Instruction Following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But\n",
    "- where does a wide variety of Instructions come from ?\n",
    "- given an Instruction: where do the Context and Response come from ?\n",
    "\n",
    "The actual process\n",
    "- is multi-step\n",
    "- using In-Context learning for each step.\n",
    "\n",
    "The initial data available\n",
    "- is a **small** set of \"seed\" examples\n",
    "- human generated, perhaps\n",
    "\n",
    "$$\\langle \\text{Instruction}, \\text{Context}, \\text{Response} \\rangle $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The seed data\n",
    "- is first used to augment the set of possible Instructions\n",
    "- and then used to generate the Context and Response\n",
    "\n",
    "resulting in new synthetic triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an overview of the method\n",
    "- we will reference this diagram in the following sub-sections\n",
    "\n",
    "<br>\n",
    "<img src=\"images/selfinstruct_process.png\">\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/2212.10560.pdf#page=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating the Instruction part of an Instruction-Output example\n",
    "\n",
    "The first step is to generate the  first Instruction part (i.e., the Instruction) of the triple\n",
    "\n",
    "$$\n",
    "\\langle \\textbf{Instruction}^{(k+1)}, \\text{Context}^{(k+1)}, \\text{Response}^{(k+1)} \\rangle \\\\\n",
    "$$\n",
    "\n",
    "using $k$ exemplars\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\langle \\text{Instruction}^{(1)} \\rangle \\\\\n",
    "\\vdots \\\\\n",
    "\\langle \\text{Instruction}^{(k)} \\rangle \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**See the box labeled \"Step 1\" in the illustration above**\n",
    "\n",
    "The exemplars are the \"seed\" data in the diagram.\n",
    "\n",
    "Here is the template for the exemplars used to generate Instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/selfinstruct_task_generation_prompts.png\" width=90%>\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/2212.10560.pdf#page=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With the above template, we expect the LLM\n",
    "- to generate a continuation of the prompt \n",
    "    - ending with `Task 9: `\n",
    "- which is the Instruction part of a new task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating the Context/Response (Input/Output) part, given an Instruction\n",
    "\n",
    "Once we have generated a the Instruction part\n",
    "$$\\text{Instruction}^{(k+1)}$$\n",
    "\n",
    "of the new synthetic example, we need to generate the Context and Response\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using the augmented set of Instructions\n",
    "- seed data + synthetic Instructions\n",
    "\n",
    "we use exemplars \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\langle \\text{Instruction}^{(1)}, \\text{Context}^{(1)}, \\text{Response}^{(1)} \\rangle \\\\\n",
    "\\vdots \\\\\n",
    "\\langle \\text{Instruction}^{(k)}, \\text{Context}^{(k)}, \\text{Response}^{(k)} \\rangle \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "and prompt\n",
    "$$\n",
    "\\text{Instruction}^{(k+1)}\n",
    "$$\n",
    "\n",
    "with the expectation that the LLM's continuation will be\n",
    "$$\n",
    "\\text{Context}^{(k+1)}, \\text{Response}^{(k+1)}\n",
    "$$\n",
    "\n",
    "\n",
    "**See the box labeled \"Step 3\" in the diagram above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate:\n",
    "\n",
    "For Classification tasks, the prompt might look like this\n",
    "\n",
    "    Task: Classify the sentiment of the sentence into positive, negative, or mixed\n",
    "    \n",
    "    Example 1\n",
    "    Sentence: I enjoy the flavor of the restaurant but their service is too slow.\n",
    "    Class Label: mixed\n",
    "    \n",
    "    Example 2\n",
    "    Sentence: I had a great day today. The weather was beautiful and I spent time with friends.\n",
    "    Class label: Positive\n",
    "    \n",
    "    \n",
    "    Task: Tell me if the following email is a promotion email or not.\n",
    "    \n",
    "    Email: Check out our amazing new sale! Weâ€™ve got discounts on all of your favorite products.\n",
    "    Class label: Promotion\n",
    "\n",
    "    Email: We hope you are doing well. Let us know if you need any help.\n",
    "    Class label: Not Promotion\n",
    "    \n",
    "    Task: {instruction for the target task}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The last line above contains a place holder for the Instruction of the Target Task\n",
    "\n",
    "$$\n",
    "\\text{Instruction}^{(k+1)}\n",
    "$$\n",
    "\n",
    "that we created in Step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an example of the template from the paper\n",
    "\n",
    "<img src=\"images/selfinstruct_generated_instances.png\">\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/2212.10560.pdf#page=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Difficulties in Generating the Input/Output part: Classification tasks\n",
    "\n",
    "Is Synthetic Data generation with an LLM really so easy ?\n",
    "\n",
    "Although the few-shot learning approach to generating an Input/Output given an Instruction \n",
    "- seems straightforward\n",
    "- the authors encountered difficulties when generating Input/Output for Classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the an Instruction Following example for a Classification task\n",
    "\n",
    "    Task: Classify the sentiment of the sentence into positive, negative, or mixed\n",
    "    \n",
    "    Example 1\n",
    "    Sentence: I enjoy the flavor of the restaurant but their service is too slow.\n",
    "    Class Label: mixed\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors found that the Response (i.e., `Class Label`) part generated by the LLM for Classification\n",
    "- were examples whose Class Label's \n",
    "- were *not well-distributed* among all possible labels \n",
    "    - examples with certain labels were either over or under represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This issue was traced\n",
    "- to the **format** of the exemplars\n",
    "\n",
    "using a deeper understanding of the Language Modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The original *format* of an exemplar used *Input-first* format\n",
    "$$\n",
    "\\langle \\text{Instruction}^{(i)}, \\text{Context}^{(i)}, \\text{Response}^{(i)} \\rangle\n",
    "$$\n",
    "\n",
    "That is: the Response part came **last**.\n",
    "\n",
    "By changing the format to \n",
    "\n",
    "$$\n",
    "\\langle \\text{Instruction}^{(i)},  \\text{Response}^{(i)}, \\text{Context}^{(i)} \\rangle\n",
    "$$\n",
    "\n",
    "that is: placing the Response in the middle\n",
    "- the distribution of Responses was improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "For example:\n",
    "\n",
    "     Task: Classify the sentiment of the sentence into positive, negative, or mixed\n",
    "\n",
    "     Example 1\n",
    "        Class Label: mixed\n",
    "        Sentence: I enjoy the flavor of the restaurant but their service is too slow.\n",
    "        \n",
    "\n",
    "        Example 2\n",
    "        Class label: Positive\n",
    "        Sentence: I had a great day today. The weather was beautiful and I spent time with friends.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is an example of Prompt Engineering\n",
    "- In-context learning seems very sensitive to the format of prompts\n",
    "- There is a skill of engineering a prompt to elicit the desired behavior\n",
    "\n",
    "This feels similar to the idea behind Chain of Thought prompting\n",
    "- by presenting `Class Label` first\n",
    "- the model seems better conditioned to generate a less biased distribution of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generating Instructions via  Backtranslation\n",
    "\n",
    "We now present an alternate method for generating the Instruction part of \n",
    "$$\n",
    "\\langle \\text{Instruction} \\rangle\n",
    "$$\n",
    "of an Instruction Following example\n",
    "$$\n",
    "\\langle \\text{Instruction}, \\text{Context}, \\text{Response} \\rangle\n",
    "$$\n",
    "\n",
    "The method is called *Back Translation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given an LLM $M_{xy}$\n",
    "- trained on \n",
    "$\\langle\\x, \\y \\rangle  = \\langle \\text{Instruction}, \\text{Response} \\rangle$\n",
    "pairs\n",
    "\n",
    "we will train an \"inverse\" LLM $M_{yz}$\n",
    "- trained on \n",
    "$\\langle \\y, \\x \\rangle  = \\langle  \\text{Response}, \\text{Instruction} \\rangle$\n",
    "pairs\n",
    "- obtained from the training data for $M_{xy}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This seems odd at first glance.\n",
    "\n",
    "**But** $M_{yz}$\n",
    "- can take a target $y$\n",
    "- and generate a *synthetic* feature vector $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the goal of creating synthetic Instruction Following examples:\n",
    "\n",
    "$M_{xy}$ is an LLM that generates\n",
    "$$\n",
    "\\text{Response}\n",
    "$$\n",
    "\n",
    "given input \n",
    "\n",
    "$$\n",
    "\\langle \\text{Instruction}, \\text{Context} \\rangle\n",
    "$$\n",
    "\n",
    "$M_{yz}$ will generate\n",
    "$$\n",
    "\\langle \\text{Instruction}, \\text{Context} \\rangle\n",
    "$$\n",
    "\n",
    "given input \n",
    "\n",
    "$$\n",
    "\\text{Response}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is similar in spirit to the Language Modeling task\n",
    "- we take an abundant source of unlabeled data\n",
    "    - documents for the LLM\n",
    "    - \"answers\" for the Synthetic Instruction Following task\n",
    "- and create targets/labels\n",
    "    - the continuation of a prefix for the LLM\n",
    "    - the $\\langle \\text{Instruction}, \\text{Context} \\rangle$ for the Synthetic Instruction Following task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The advantage of this approach is that\n",
    "- un-labeled data is plentiful\n",
    "    - almost any block of text\n",
    "- but labeled data ( `Response/Instruction` pairs) is scarce.\n",
    "\n",
    "So, starting with a plentiful resource, we create the scarce resource\n",
    "- i.e, Instruction Following example triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-Improvement\n",
    "\n",
    "We have seen [Self-Improvement](LLM_Self_Improvement.ipynb) before\n",
    "- Fine-tuning an LLM in stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Recall:\n",
    "\n",
    "- train the Target model $\\model$ in stages\n",
    "    - creating a sequence of fine-tuned Target models $\\model_{(0)}, \\model_{(1)}, \\dots$\n",
    "    - of  increasing power\n",
    "- base case\n",
    "    - fine-tune initial Target $\\model_{(0)}$\n",
    "    - using a mixture of strong (human-generated) and weak (LLM generated) fine-tuning  examples of the Target task\n",
    "    - resulting in weak Target model $\\model_{(1)}$\n",
    "- inductive case\n",
    "    - create improved Target $\\model_{(\\tt+1)}$\n",
    "    - by fine-tuning $\\model_\\tp$\n",
    "        - with the strong examples we already have\n",
    "        - augmented with examples created as outputs of Target model $\\model_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the purpose of generating Synthetic Instruction Following examples:\n",
    "\n",
    "With the newly extended set of seed Instruction/Response pairs\n",
    "- we have more exemplars\n",
    "- which we can use as a seed to another iteration of  $M_{yz}$\n",
    "    - the enlarged set of exemplars may result in *better* synthetic Instruction/Response pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can iterate on this process multiple times\n",
    "- using the Augmented set of Instruction/Response pairs from step $i$\n",
    "- as the \"seed\" for iteration $(i +1)$ of the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the workflow:\n",
    "\n",
    "<table>\n",
    "    <center><strong>Instruction Backtranslation</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/instruction_backtranslation.png\" width=70%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://arxiv.org/pdf/2308.06259.pdf#page=2\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting the best synthetic examples for augmentation\n",
    "\n",
    "The quality of the synthetic examples created at each step may not be uniformly high.\n",
    "\n",
    "It would be desirable \n",
    "- to select only the best examples to use\n",
    "- in augmenting the seed examples of each iterative Step.\n",
    "\n",
    "How can we rate the quality of a synthetic example ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ask the LLM to do it for you ! \n",
    "\n",
    "Using just the seed data\n",
    "- fine tune a \"first generation\" LLM\n",
    "    - denoted $M_0$\n",
    "- to create a quality score of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following prompt requests that the LLM evaluate the\n",
    "synthetic example using a rating scale of $1$ (low quality) to $5$ (high quality)\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <center><strong>Instruction Backtranslation Curation</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/instruction_backtranslation_curating.png\" width=70%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://arxiv.org/pdf/2308.06259.pdf#page=4\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use $M_0$ to\n",
    "- select the best first generation augmented examples (from the first iteration)\n",
    "\n",
    "The next generation augmented data set is\n",
    "- the prior generation \n",
    "- augmented with the best (highest quality scores) of the new generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we have\n",
    "- an augmented (high quality) \"generation $i$\" set of seed examples\n",
    "\n",
    "we continue our iterative process\n",
    "- creating a more powerful scoring LLM $M_i$\n",
    "- using exemplars\n",
    "    - instructions \n",
    "    - with scores from the generation $(i-1)$ scorer $M_{i-1}$\n",
    "    \n",
    "The scores of $M_i$ can then be used to create\n",
    "- an even higher quality scorer $M_{i+1}$\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This too is an example of [LLM Self-Improvement](LLM_Self_Improvement.ipynb).\n",
    "- improving the Scoring LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
